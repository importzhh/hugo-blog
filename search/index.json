[{"content":"为什么推荐这本书 《数据密集型应用系统设计》，英文名称是 《Designing Data-Intensive Application》 ，也被简称为 DDIA。\n面试八股文 分布式系统有什么特点 分布式系统的设计目标是，最大化整体资源利用率的同时，处理局部错误、保持对外可用性。 在构建分布式系统时，在逻辑上要注意以下这些方面：\n可扩展性：可扩展性是对分布式系统最本质的要求，即系统设计允许我们只通过增加机器来应对不断增长的外部需求。 容错性\\可用性：这是可扩展性所带来的一个副作用，即在系统规模不断变大之后，单个机器故障便会成为常态。系统需要自动处理这些故障，对外保持可用性。 并发性：由于没有全局时钟进行协调，分散的机器天然处在“平行宇宙”中。系统需要引导这些并发变为协作，以拆解并执行集群任务。 异构性（对内）：系统需要处理进群内部不同硬件、不同操作系统、不同中间件的差异性，并且能够容纳新的异构组件加入系统。 透明性（对外）：对外屏蔽系统复杂性，提供逻辑上的单一性。 有几种类型 在组织分布式系统时，在物理上可以有以下几种类型：\n主从架构（master-workers）：有一个负责指挥的机器，其他机器负责干活，如 Hadoop。好处是设计和实现相对容易，坏处是单点瓶颈和故障。 点对点架构（peer-to-peer）：所有机器逻辑等价。如亚马逊 Dynamo，好处是没有单点故障，坏处是机器协调不好做、一致性也不好保证。不过，如果系统是无状态的，则这种架构很合适。 多层架构（multi-tier）：这是一种复合架构，实际中也最常用，比如今年来常说存储计算分离。每一层可以根据不同特点（IO 密集型、计算密集型）进行设计，甚至可以复用现有组件（云原生）。 有哪些优劣 分布式系统是由于单机能力不匹配数据尺度的一种无奈之举。因此，在做系统设计时，优先考虑单机系统。毕竟，分布式系统的复杂度是指数上升的。\n优点 高可用、高吞吐、高可扩展性 高可用、容错：一部分机器坏掉，仍可以正常对外提供服务。 无限扩展：只要设计的好，可以通过线性的增加机器资源来应对不断增长的需求。 低延迟：多地部署，将用户请求按地理路由到最近机房处理。\n缺点 最大的问题是复杂性。\n数据的一致性。考虑到大量的机器故障：宕机、重启、关机，数据可能丢失、陈旧、出错，如何让系统容纳这些问题，对外保证数据的正确性，需要相当复杂的设计。 网络和通信故障。网络的不可靠，消息可能丢失、早到、迟到、Hang 住，这给机器间的协调带来了极大的复杂度。像 TCP 等网络基础协议，能解决部分问题，但更多的需要系统层面自己处理。更不用说，开放式网络上可能存在的消息伪造。 管理复杂度。机器数量到达一定数量级时，如何对他们进行有效监控、收集日志、负载均衡，都是很大挑战。 延迟。网络通信延迟要比机器内通信高出几个数量级，而组件越多、网络跳数越多，延迟便会更高，这些最终都会作用于系统对外服务质量上。 这本书能够起到什么帮助 作为一个分布式系统初学者，面对网上未加归类、浩如烟海的学习资料，很容易两眼抓瞎。 这本书能够帮助你对数据库和分布式系统相关的论文有一定了解, 帮助你在脑海中构建一套数据系统体系. 帮助你在数据系统设计的时候,知道你选择的组件有哪些合适的场景,而不至于两眼一抹黑掉到坑里. 读完此书并不会马上给你带来什么显然易见的收获，也不会直接帮助你写代码。但是，时间久了，这本书带给你的思考的提升，最终会让你收获颇丰。\n如何阅读一本书 阅读四问:\n整体来说，这本书到底在谈些什么？ • 书的主题是什么？ • 这本书在谈论什么东西？\n作者细部说了什么，怎么说的？ • 这本书是如何讲述这个主题的？ • 它的核心章节，或者是观点是什么？\n这本书说得有道理吗？是全部有道理，还是部分有道理？ • 这本书讲的有道理吗？ • 这个章节讲的有道理吗？ • 他说的理论有对应的开源实现吗？\n这本书和我有什么关系？ • 反问自己一句，读完这本书收获了多少？ • 合上眼，能对着目录讲出这个章节大概内容吗？ • 能把这本书用自己的话讲给别人听吗？\n什么是「数据密集型应用系统」？\n当数据（数据量、数据复杂度、数据变化速度）是一个应用的主要挑战，那么可以把这个应用称为数据密集型的。与之相对的是计算密集型——处理器速度是主要瓶颈。\n其实我们平时遇到的大部分系统都是数据密集型的——应用代码访问内存、硬盘、数据库、消息队列中的数据，经过业务逻辑处理，再返回给用户。 很多应用都是在解决不同场景下的数据存储和检索问题——MySQL，Redis，ElasticSearch，Tidb, HBase，Neo4j，…… 还有很多技术是围绕着数据展开——kv存储(B+ tree/lsm tree)，编码（JSON, XML, Thrift, ProtoBuffer），行列存储, 文档存储, 对象存储 …… 当数据在分布式处理时，要考虑——数据复制，分区，事务…… 大数据场景下，我们会使用——MapReduce，Spark，Flink 等批处理、流处理框架。 这本书既有理论也有实践，基本没有公式，图很多，阅读起来很流畅，比较容易理解。\n这本书分为了三部分：\n第一部分：数据系统的基石，包括数据模型与查询语言、存储与检索、数据编码与演化； 第二部分：分布式数据，包括复制、分片、事务、一致性与共识； 第三部分：衍生数据，包括批处理、流处理、数据系统的未来。 第一，全书分三个部分。分别是单机，多机，衍生。从单机开始聊数据系统，可以摒除分布式庞杂理论的影响，专注在数据系统本身相关理论； 到第二个部分放开单机限制，着重讲将数据系统扩展到多机所面临的问题和一般解决方案； 最后一部分笔锋一转，着眼数据处理，以数据系统视角看，无非是一个数据集的变换，也即数据的派生。 三个部分，层层递进，相互正交。这种行文思路，正是大型工业代码组织思路：将复杂度拆解到几个正交、但又相互连结的模块， 从而使每个部分都相对内聚而简洁。\n第二，具体到第一部分，开篇就给了三个总纲式的“心法”：可靠性、可伸缩性、可维护性。 然后，从上到下，由离用户最近的数据模型（比如关系模型）和查询语言（比如 SQL）， 到稍微底层一点的存储引擎（比如 B+ tree 和 lsm tree）和查询引擎， 再到最底层的编码（数据结构的降维）和演化，层层下探，零碎知识，至此百川入海，万法归一。\n第三，具体到每一章，也是节节递进，读起来无比丝滑。比如第三章，在讲存储引擎时，从一个仅由两个 shell 函数组成的“kv 引擎”起， 到一个简单的日志结构的存储（Bitcask），再到经典的 LSM-Tree。从一个最简单的数据库开始，越讲越深入，把常见数据库的存储与检索方式都讲完了。 这又是工程中惯用思路：从一个最小可用原型开始，不断增加需求、解决瓶颈，最终得到一个工业可用的存储引擎。\n阅读建议 这本书比较厚，概念也比较多，而且很少有代码，基本都是在讲逻辑，因此有些「太干了」。 这本书的内容大而多，每一章都可以成为一本单独的书。如果你时间精力有限，不妨阅读你感兴趣的重点章节。\n如果你对数据的编码、存储感兴趣，可以阅读第 1~4 章； 这些是比较基础的内容，阐述了数据库的原理，对工作比较有帮助 如果你对分布式系统感兴趣（如分布式复制、分区、事务等），可以阅读第 5~9 章； 这部分内容比较偏向于概念和思维，挺抽象的，属于进阶内容，帮助拓展思维（和准备面试）。 如果你对大数据计算（批处理、流处理）感兴趣，可以阅读第 10~12 章。 这部分内容工程实践比较强，如果你工作中用到了 Spark, Flink 等，可以在这些章节了解它们的原理 阅读资源 推荐系列阅读资源： 《数据密集型应用系统设计》开源翻译仓库（9.3K star）：https://github.com/Vonng/ddia 开源版本在线阅读：https://vonng.gitbooks.io/ddia-cn/content/ ddia-references: https://github.com/ept/ddia-references DDIA 逐章精读: https://ddia.qtmuniao.com/#/preface 分布式系统学习资料汇总: https://zhuanlan.zhihu.com/p/372646991 DDIA 逐章精读 Book Review 这里有个很不错的 Book Review，是一个小哥讲了《DDIA》每一章的概述，作者很用心。 全英文的，在油管可以看到。地址是： https://www.youtube.com/watch?v=PdtlXdse7pw\u0026amp;list=PL4KdJM8LzAMecwInbBK5GJ3Anz-ts75RQ\nMIT6.824 看这本书的时候，你可以结合 MIT6.824 分布式系统课程。油管上这门课程的视频版本，在 B 站有这个课程的中文字幕翻译合集地址： 这门课程的中文文字版，地址：https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/ DDIA作者Martin Kleppmann分布式系统 课件pdf：https://www.cl.cam.ac.uk/teaching/2122/ConcDisSys/dist-sys-notes.pdf 课程主页：https://www.cl.cam.ac.uk/teaching/2122/ConcDisSys/ 视频合集：https://www.youtube.com/playlist?list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB 作者主页：https://martin.kleppmann.com/ ","date":"2023-10-15T12:44:47+08:00","permalink":"https://blog.importzhh.me/p/i-ddia%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B01/","title":"I: DDIA阅读笔记1"},{"content":"软件版本 kafka_2.12-2.4.0.tgz（带zookeeper）\n服务端部署 修改kafka配置文件 server.properties vim /data/kafka/config/server.properties:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 # Kafka broker的唯一标识符，每个broker在集群中必须有个唯一的ID。 broker.id=0 # 监听相关参数 1steners SASLPLAINTEXT://192.168.1.131:9092 advertised.1fsteners=SASL_PLAINTEXT://192.168.1.131:9092 # 主题相关参数 # 禁止自动创建主题 auto.create.topics.enable=false # 启用删除topic功能 delete.topic.enable=true # 默认值1。当该副本所在的brker机，consumer_offsets 只有一份副本，该分区岩机。使用该分区存储消费分组offset位置的消费者均会收到影响，offset无法提交，从而导致生产者可以发送消息但消费者不可用。所以需要设置该字段的值大于1 offsets.topic.replication.factor=3 # 事务主题的复制因子(设置更高一确保可用性) 设置为节点数 内部主题创建将失败，直到集群大小满足此复制因素要求 transaction.state.log.replication.factor=3 # 覆盖事务主题的min.insync.replicas配置。设置为(节点数+1)/2 transaction.state.log.min.isr=2 # Kafka ISR 列表中最小同步副本数。设置为(节点数+1)/2 min.insync.replicas=2 # 事务最大超时时间默认60000ms transaction.max.timeout.ms=900000 # 默认副本因子 default.replication.factor=3 # 线程相关参数 # 后台处理任务的线程数，如果不出问题，默认不修改默认10 background.threads=10 # 处理磁盘IO的线程数量 默认8 num.io.threads=8 # 处理网络请求的线程数量 默认3 num.network.threads= # 在启动时用于日志恢复和在关闭时刷新的每个数据目录的线程数默认1 num.recovery.threads.per.data.dir=1 # 用于复制来自源代理的消息线程的数量 默认1 增加该值可以增加 follower broker 中的 I/O 平行度的程度。 queued.max.requests=500 # 默认值500 用于控制生产者在发送请求的时候可以排队等待的最大请求数量 如果生产者发送请求的速度超过了服务器处理请求的速度，未发送的请求将积压在生产者的队列中 queued.max.requests=500 # 发送套接字的缓冲区大小 socket.send.buffer.bytes=104857600 # 接受套接字的缓冲区大小 socket.receive.buffer.bytes=104857600 # 请求套接字的缓冲区大小 socket.request.max.bytes=104857600 #压缩相关参数 默认producer 意为和producer端的压缩格式保持一致 compression.type=producer # zookeeper相关参数 zookeeper.connection.timeout.ms=6000 zockeeper.sync.time.ms=2000 zookeeper.connect=192.168.1.112:2181,192.168.1.111:2181,192.168.1.110:2181/kafka # 重平衡相关参数 # Unclean Leader Election指在Kafka集群leader节点失败时,新选举出来的leader节点可能还没有获得全部partition的最新offset数据,这时候新leader就是一个“脏”leader。 # 正常情况下,Kafka会等待新leader从其他副本那里同步完毕数据后,才会开始作为leader提供服务。 # 但开启脏选举功能后,即使数据同步未完成,新选举出来的leader也会直接开始提供服务,这可能会导致一部分数据丢失。 unclean.leader.election.enable=false # 禁止自动重平衡 auto.leader.rebalance.enable=false # 检查各个分区是否平衡的频率 默认300s leader.imbalance.check.interval.seconds=30 # 触发重平衡的阈值百分比 默认为10 group.initial.rebalance.delay.ms=10000 # 日志刷写相关参数 暂不更改 log.dirs=/applog/kafka/logs # segment文件保备的最长时间，超时将被删除 默认7天168 log.retention.hours=168 # 日志滚动切片相关参数 # 日志滚动的最大时长，默认7天 log.roll.hours=48 # 给日志段的切分加一个扰动值，默认为0 避免大量日志段在同一时间进行切分操作 如果发现kafka有周期性的磁盘I/O打满情况，建议设置此值。 log.roll.jitter.hours=2 # 元数据相关参数 # 副本相关参数暂不更改 # 默认值500 follower副本发出的每个获取器请求的最大等待时间。该值在任何时候都应该小于replica.lag.time.max.ms，以防止低吞吐量主题的ISR频繁收缩 replica.fetch.wait.max.ms=500 # 默认值5000 将高水位保存到磁盘的频率 replica.high.watermark.checkpoint.interval.ms=5000 # 默认值30000 如果follower没有发送任何fetch请求，或者没有消耗leader将从ISR中删除follower replica.log.time.max.ms=30000 # 默认值65536 用于网络请求的套接字接收缓冲区 replica.socket.receive.buffer.bvtes=65536 # 网络请求的套接字超时。它的值至少应该是 replica.fetch.wait.max.ms replica.socket.timeout.ms=30000 # 定义副本节点与领导者leader节点之间的最大复制滞后时间表示在副本节点从领导者节点获取数据之前可以容忍的最大时间延迟 repiica.1aq.time.max.ms=10000 # 消息相关参数 message.max.bytes=10485760 replica.fetch.max.bvtes=10485760 log.message.timestamp.type=CreateTime # 默认1000 这个参数暂定 较小的值可以提高请求处理效率但增大清理操作的开销 fetch.purgatory.purge.interval.requests=100 # 权限配置 allow.everyone.if.no.acl.found=true authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer security.inter.broker.protocol=SASL_PLAINTEXT sasl.mechanism.inter.broker.protocol=SCRAM-SHA-256 sasl.enabled.mechanisms=SCRAM-SHA-256 super.users=User:admin 添加SASL配置文件 在配置目录下创建一个 kafka_server_jaas.conf 文件\n** vim kafka_server_jaas.conf: **\n1 2 3 4 5 6 7 8 KafkaServer { org.apache.kafka.common.security.scram.ScramLoginModule required username=\u0026#34;admin\u0026#34; password=\u0026#34;admin\u0026#34; user_admin=\u0026#34;admin\u0026#34; user_producer=\u0026#34;pwd1\u0026#34; user_consumer=\u0026#34;pwd2\u0026#34;; }; 基本语法注意事项： 第一行：固定名称指定 KafkaServer 端的配置。 第二行：安全认证类名为 ScramLoginModule，与 Kafka 属性文件中的协议类型一致。 第三、四行：服务端使用的帐号和密码。 第五行，超管帐号的密码。 后面都是预设普通帐号认证信息，语法为 user_真正的用户名=\u0026lsquo;\u0026lsquo;密码\u0026quot; 最后一行用户名密码定义后，必须以分号 ; 结尾。\n2.4 修改启动脚本 vim bin/kafka-server-start.sh ，找到 export KAFKA_HEAP_OPTS , 添加jvm 文件：\n1 export KAFKA_HEAP_OPTS=\u0026#34;-Xmx6G -Xms6G -Djava.security.auth.login.config=/approot/kafka/config/kafka_server_jaas.conf\u0026#34; 2.5 启动kafka服务 向 Zookeeper 注册 kafka_server_jaas.conf 文件中定义的各个用户的密码\n1 bin/kafka-configs.sh --zookeeper localhost:2181 --alter --add-config \u0026#39;SCRAM-SHA-256=[password=admin]\u0026#39; --entity-type users --entity-name admin 这一步必须配置，否则启动 Kafka 的时候会报错：\n1 /approot/kafka/bin/kafka-server-start.sh -daemon /approot/kafka/config/server.properties kafka客户端配置登录认证 创建客户端认证配置文件 在配置目录下创建一个 producer.conf 文件：\n1 2 3 security.protocol=SASL_PLAINTEXT sasl.mechanism=SCRAM-SHA-256 sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\u0026#34;producer\u0026#34; password=\u0026#34;pwd1\u0026#34;; 测试命令\n1 bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test --producer.config /approot/kafka/config/producer.conf 在配置目录下创建一个 consumer.conf 文件：\n1 2 3 security.protocol=SASL_PLAINTEXT sasl.mechanism=SCRAM-SHA-256 sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\u0026#34;consumer\u0026#34; password=\u0026#34;pwd2\u0026#34;; ","date":"2023-09-03T16:09:47+08:00","permalink":"https://blog.importzhh.me/p/kafka-%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE/","title":"kafka 基础配置"},{"content":"\r这有一整套方法论 他想教会你这些怎么分析思考 如何做决策 可以说我们遇到的所有问题类型里面 都有答案和解题思路\n理论与实践 比如作者在实践论中反对的 盲动主义和本本主义 理论实践缺失 哪一边的效果都接近零 二者之间重要性没有高下伯仲 但是在次序上有先后说法 是不是听起来感觉很浅 要深入加力度了 理论指导实践 实践完善理论 二者相互促进协同发展\n什么叫理论 我们看看辞海的解释是这样 由实践概括出来 关于自然界和社会知识有系统的结论 受不了吧 我也受不了翻译翻译 吃饱了不饿 这是因果原理 所以我饿了就去吃饭 这是根据原理得出来的行动指导 在我的定义里面 理论是原理加行动指导 这两样找到因果关系 然后干就完了 就是这么简单吗 不是这么简单 在原理层面需要做个选择题 这个我一般叫做判断框架\n这个原理的因果关系例如\n一因一果 或者 多因一果 或者多因多果 或者 多因一果\n吃饱了不饿是一因一果 有且只有一个条件能解决饱腹 吃饱喝足 有钱花才能爽 这叫合因异果 有人需求低 吃饱喝足 有钱花满足一个就能爽 这叫多因一果 条条大路通罗马 杠精导致四面树敌 孤立无援 思维封闭 行为滑稽运气变差 晚景凄凉 吐完最后一口狗血 变成怨气小土包这是一因多果\n无关因果是啥呢 有人说我的视频素材太少啊 有人说我的面具很难看我知道呀 为什么难看 还有几个人看 主要是内容所以有限的时间全投入内容 只在优势战场集中优势兵力 画面好看 作为锦上添花 暂时不做呗 戴面具是还没想好 促进不出镜 为啥不做成剪辑素材的 因为不想拿时间找素材出来 给你看个会动的玩意儿 有点互动感就行了 这是去除无关条件\n好搞清楚的框架是哪种 有什么用\n作者强调没有调查 没有发言权 想要搞清楚达成目的需要哪些条件 就有了纲要 调查直奔纲要明确方向啊 否则盲目的调查能看出什么东西来 作者走访一个多月 关于经济人口地理交通产业结构囊括全行业 这得走访多少人才能拿到数据 详尽的吓人 这些走访是有明确指向性的 有的放矢 不是大海捞针 盲目的调查也是盲动主义 这看起来似乎是个再简单不过的朴素道理 现实是如果观察 你会发现身边无数人搞错了顺序 或者在无关条件上下付出代价。 你要先有理论的同时 不要掉入另一个陷阱 理论的大忌是假装完美主义的逃避主义 光计划不动手 不如不计划。骗自己干嘛 调查搞个五六分方向没问题就行了 计划在开始阶段不可能有完善版本 给理论打补丁更新来弥合理论和现实的距离 最终最大程度上能切合现实世界 严丝合缝就成了知行相互指导 矫正接近现实的过程。\n你可以叫做认知升级 就这点事 首先根据目的找出框架 得到相关因素 根据相关因素的现状针对性调查 然后结合当前具体情况进行对比判断 以我目前的条件能不能干 要不要干 能干要干 制定计划行动 小步快频的犯错 再实践 搞定错误 不需要气馁 这是阶梯啊 兄弟可能有人觉得这点东西谁不懂 为什么一直重复懂和细化层步骤是不一样的 一旦确信正确重复1万次不够 再来1万次融入血肉为止\n千万不要混淆听懂和下意识本能的界限 蜻蜓点水的听两三句就我知道了\n在知识的广度和深度上面 有很多有道理的谚语 好像矛盾是谚语吗 呃我也不知道是什么道理吧 比如不谋全局者不足谋一域 说的是广度 百鸟在林不如一鸟在手 东一榔头西一锄头打不出水来 得聚焦深挖 这些偏向深度都有道理 但不能丢了应用场景 恭喜发财是句好话 别人家办白事的时候说不合适吧\n道理这东西失去了时间 空间 情境 范围 不完整 就会造成误读\n深度与广度 人生是一场赌博 分秒都在用时间下注 只是不觉得 有多少人真的知道自己下注下的是什么\n比如大学毕业了要选一个行业开始混 这是以几年时间为赌注的 怎么选呢 选错的后果是错误的时间进入了夕阳产业\n只有深度没有广度是什么样子呢 专精一行 细分再细分 聚焦再聚焦 以失去广度为代价 成了解决某些特定问题的高级工具 被物化了啊\n虽然我们的教育是分科的 但物理世界是不分学科的 它浑然一体缺乏某一块的常识 缺少一个视角带来的偏差都可能很大\n下面这句话很重要 推动行业前景变化的因素 在行业之外 有点抽象哦\n一个东西的价值不取决于它是否卓越 而是世界对它的需求\n未来世界的需求走向和它的价值变化的趋势是什么 没有几个人去想 能判断的就更少了\n每个人似乎都知道自己的目标，但又似乎身处迷茫中。 每个人似乎都有选择，但又似乎依旧身陷囹圄。 能看清事情发展规律的人已经很少了，能改变事情走向的人更少。\n人生总有高有低，读过书和没读书的人，人生的维度是不一样的，度过高峰期、熬过低谷期的方式都不一样。\n太早进入管理领域的话，多少要牺牲技术，尤其现在新技术、新材料层出不穷，你错过了这个阶段，很容易失去对新技术、新潮流的感知力。\n我个人的收获是相信常识 相信朴素的道理 相信大概率而不是奇迹\n积小胜 慢慢形成优势 善战者求之于势 不责于人 才能择人任势\n常识这张地图拼得越完整 找到的定位和坐标就越清晰 靠长期积累的 要求是不要局限就行 不要觉得哪一块儿就是没用啊 都有用的 都有一些常识 要靠谱 最好是来自专业的书籍或者专业人士\n当你知道很多东西的内在逻辑啊 隐隐能触摸到世界演进的轨迹的时候 你就无所畏惧 能坦然接受各种事情 不急不躁 有自己的节奏速度 你是知道的 自然也不会茫然了 到那时候好好坏都觉得正常\n深度这一块关系到60%以上的时间下注\n这世界无奇不破 唯有团结不破 说到补短板这一块 就说到书里另一个收获了 我们叫团结组织能力 借用汉弗莱说的名言格式啊 历史是无情对无脑的胜历史 赞同 但补充一点 历史也是团结对分散的胜历史\n教员在开篇仅次于矛盾实践论的重要篇章 就是团结\n革命不是请客吃饭，不是做文章，不是绘画绣花，不能那样雅致，那样从容不迫，文质彬彬，那样温良恭俭让。革命是暴动，是一个阶级推翻一个阶级的暴烈的行动。\n革命未能取得胜利的原因在于 没有搞清楚谁是真正的朋友 谁是真正的敌人 没有明确的联合真正的朋友以打击真正的敌人\n人在多大程度上可以调动身外之力 他就有多强\n调动身外之力有且只有三个要素\n权力的游戏当中有个桥段 国王和教皇指挥一名武士杀死对方 国王给出的条件是荣华富贵 教皇给出的条件是天堂 交换的武士技能是暴力 就是这三种要素\n钱信仰暴力 好想一下哪一种要素性价比高 是信仰\n注意信仰不是情谊 生产力改变生产关系，生产力正在逐渐结构生产关系 重构社会关系。 （进一步补充解释，农耕社会，以农耕为主，人多，密度大，灾难多导致竞争度高，人和人的关系就很紧张，因为种地抢水源打架什么的这些事情年年都有。 为了竞争中保持优势，拼的是人口数量和组织形态的严密性。为了管理就要用好孝和顺这条基本规则。 欧洲环地中海的地缘环境是以商贸为主流的谋生方式，很多人到港口市集城邦做买卖，人的社会关系是流动的，基本都是陌生人。 为了和谐不要互相坑，大家需要宗教和契约来提供秩序。 这两种家庭关系的差别是两种搞钱的生存方式不一样造成的。\n在社会关系中一个特性很重要，就是确定性，比如兰尼斯特有债必偿，就是打造确定性，能让人信任就是靠这个，有确定性的人，每个人都知道该怎么和你合作，该怎么利用你。 所以需要构建秩序，构建和谐的关键在于公平，但是公平是很灵活的，富有弹性，世界上没有事实上的公平，只有人思想上的公平，而人不过是思想的产物。 思想是块橡皮泥，形状是可以改变的，教育和媒体就是捏这块橡皮泥的。 这是思想的工业，是不是事实不重要，重要的是要一群人具备同一套有助于和谐的思想。 组织的本质是一套分配方案，统一认识当前方案就是最公平最佳的分配方案， 在这个方面来说宗教和信仰的力量发挥的威力是很恐怖的 ）\n了解这些有什么用呢 搭班子搞合作 这些是人类种群中最有用的知识 上者劳人 中者劳脑 下者劳力 懂得这些规则 要借用很多专业技能 信仰之力\n光荣荣在于平淡，艰巨在于漫长 很多实在人 因为有需求(信仰 物质等) 所以要坚持要干活堆出来的\n凝聚意愿，共同目标，信息通畅，这才是一个组织一支队伍的基本配置。 做老大的主要工作是什么呢？搭架构，协调组织，传递信念。 做好这三个的前提是要把自己从实际干活中抽出来，这样才能在旁观者的角度看整个的系统问题， 怎么分配资源，怎么分钱分工。这是搭建架构不断的调整修改，极力接近公平， 没有一步到位就刀枪入库，马放南山这种好事，这是懒汉的幻想。\n语言的品格 教员的文字上能感觉到魄力和对自己判断力的自信 唯独感觉不到一点颓丧 他为什么能这么稳定地 保持积极主动的人生态度 我也不太明白 要知道很多篇章是在几次起落 被孤立的那几年写的 文字中 完全感觉不到情绪有什么异样 不只是自己恒常 还能鼓舞士气 带给别人信心和力量\n他总是用接地气的语言 要知道梨子的滋味就尝一尝 要调查 不许瞎说 星星点火可以燎原 凡真理都不装样子吓人，它只是老老实实地说下去和做下去\n几乎都用大白话这种文笔 我愿称之为子弹 一针见血 精炼简洁 既干净又有穿透力 传播力和亲和力 这让我想起了以前的我装神弄鬼 喜欢用很厉害的词句 比如对现实的抽象就是对现实的毁灭 思维不感性就空了 直观不概念就瞎了\n这些话的含义 我以前懂不懂的不太懂 因为逼格高就要说 反正别人也不懂 假装深度\n这种语言习惯有毒 语言的最高品格就是精确的表述思想\n朴素中能传递锋利的信息 为了达到这样的平衡啊 要下很大功夫的\n为了向这个方向靠拢 第一步就是精确基本单元 我们的思想是建构在语言上的最小单元是词\n较真的话 我们会发现日常啊瞎扯蒙 事的多了去了 这不公平那个不公平 什么是公平 他的对立面是什么 它的内涵和外延是什么不知道的 反正就是用 日常的交谈多数是凭着模糊的感觉 在懂我意思吧 自己都未必懂 这种现象的原因就是随性\n每一个词 每一块砖头的含义都存在细微的模糊边界 用因为模糊导致轻微变形的砖头 组合起来的建筑偏差就相当大了\n指望靠这套语言系统跟别人形成共识 凝聚思想 对内分析思考 这能想明白什么玩意儿 靠运气罢了 这清晰度和800度近视差不多\n说到这儿就明白了 历史上的很多大佬 他们在雕字琢句中练习语言的把握能力 和这些人说话 比如某领导秘书 人文书功底过硬 一交锋就感觉自己像个傻子 赶紧闭嘴 假装自己不是个弱智\n都看过老罗和王自如直播pk吧 这就是训练过和没训练过的差别 不是说他们的观点一定正确 而是他们的观点在他们的语言系统当中 表达的相得益彰\n而脑子有劲儿 缺乏语言能力 用不出来的 除了风中凌乱 应付两句 OKOK还能咋办\n做视频的好处就是一方面在锤炼这个能力 一方面也算个营生 两者兼得挺不错的 还有一个自己写 没压力 视频乱来就有人骂你有监督作用 所以如果你不做视频啊 那建议哪怕是朋友圈说说百来个字 也经常写写 别光在舔妹子的时候写 写多了脑袋瓜就好使了\n这和开车一样 没有什么神秘 就是个熟练工\n写东西有一条红线 如果做不到高深 对某些词句的含义不能精确把握 那就换个朴素点的 不要硬来假装高深的代价是一旦被看穿 拙劣的表演会让人觉得恶心\n最后要升华一下啊 流行受欢迎的多数是立竿见影的技巧 却很少人愿意相信朴素的常识\n自我人格·死亡·意义 荣格 注意不可证伪的 具有传播力的是否是谣言 亦或洗脑。 在地球的任何地方，只要不是大草原，你能听到的只有主流思想的声音，真相是需要自己去找的，而不同出身的人命运在这就分开了， 有些家庭是几代人的智慧积累，这些孩子在20多岁就解开思想钢印了 没有这个条件的人，普遍得在35岁以后才行，这种思想的差距比财富的差距更让人绝望。 人类这样的信息环境已经存在了几千年，批判它没意义，最好的策略是，面对主流声音要保持缄默 有个另外收获，如果能给大家推荐一本书的话就是金岳霖的《形式逻辑》。 以前我们总是被告知你好好想想，想是如何想想， 没人告诉我们，形式逻辑是思维的基石。\n我们经常说 你对自己没点逼数吗？搞清楚自我这事儿可不容易 很多人误认为很了解自己 可以理性的控制好自己，这种错觉的危害是 拔掉了心理世界的警戒线和危险标识 然后傻头傻脑的在雷区溜达 一旦踩到地雷很难搞哦。 思想的动摇并非正确与错误之间左右不定，而是一种理智与非理智之间徘徊。你没有觉察到的事情，就会变成你的「命运」。\n","date":"2023-09-01T22:05:47+08:00","permalink":"https://blog.importzhh.me/p/b%E7%AB%99up-%E5%BD%B1%E5%AD%90%E7%8C%AA%E8%AF%BB%E6%AF%9B%E9%80%89-%E7%AC%94%E8%AE%B0/","title":"b站up 影子猪读毛选 笔记"},{"content":"https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/\n文档加载 文档加载器的介绍 文档加载器的作用，是将不同格式和来源的数据加载到标准的文档对象中，包括内容本身以及关联的元数据。\nLangChain提供了多种类型的文档加载器，用于处理非结构化数据，根据数据来源的不同大致可分为：\n公共数据源加载器，如YouTube、Twitter；\n专有数据源加载器，如Figma、Notion。\n文档加载器也可以加载结构化数据，比如基于表格中包含的文本数据，对问题进行回答或语义搜索。\n这种技术我们称之为检索增强生成（RAG，Retrieval-Augmented Generation）。\n在RAG中，LLM会从外部数据集中检索上下文文档，作为其执行的一部分，这对于询问特定文档的问题非常有用。\n下面我们来实际使用其中的一些文档加载器。\n加载PDF文档 1 2 3 4 5 6 # 导入PyPDFLoader文档加载器 from langchain.document_loaders import PyPDFLoader # 将位于特定路径下的PDF文档放入到加载器中 loader = PyPDFLoader(\u0026#34;docs/cs229_lectures/MachineLearning-Lecture01.pdf\u0026#34;) # 加载PDF文档 pages = loader.load() 默认情况下，这将加载一系列的文档。以页面为单位，每个页面都是一个独立的文档。\n1 2 3 # PDF的总页数 len(pages) # 22 每个文档都包含「页面内容」和「与文档关联的元数据」。\n页面内容：\n1 2 # 仅打印前500个字符 print(page.page_content[0:500]) MachineLearning-Lecture01\nInstructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine learning class. So what I wanna do today is ju st spend a little time going over the logistics of the class, and then we\u0026rsquo;ll start to talk a bit about machine learning.\nBy way of introduction, my name\u0026rsquo;s Andrew Ng and I\u0026rsquo;ll be instru ctor for this class. And so I personally work in machine learning, and I\u0026rsquo; ve worked on it for about 15 years now, and I actually think that machine learning i\n元数据：\n1 2 3 4 page.metadata # source: 源信息，这里对应PDF的文件名 # page：页码信息，这里对应PDF的页码 # {\u0026#39;source\u0026#39;: \u0026#39;docs/cs229_lectures/MachineLearning-Lecture01.pdf\u0026#39;, \u0026#39;page\u0026#39;: 0} 加载YouTube视频 步骤1：导入几个关键部分，包括 YouTube音频加载器：从YouTube视频加载音频文件\nOpenAI Whisper解析器：使用OpenAI的Whisper模型(一个语音转文本的模型)，将YouTube音频转换为我们可以处理的文本格式\n1 2 3 from langchain.document_loaders.generic import GenericLoader from langchain.document_loaders.parsers import OpenAIWhisperParser from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader 步骤2：指定URL及保存音频文件的目录，创建组合了步骤1两个关键部分的通用加载器并执行加载 1 2 3 4 5 6 7 url=\u0026#34;https://www.youtube.com/watch?v=jGwO_UgTS7I\u0026#34; save_dir=\u0026#34;docs/youtube/\u0026#34; loader = GenericLoader( YoutubeAudioLoader([url],save_dir), OpenAIWhisperParser() ) docs = loader.load() 步骤3：查看加载完成的视频文稿 1 docs[0].page_content[0:500] 加载网络URL 1 2 3 4 5 6 7 from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader(\u0026#34;https://github.com/basecamp/handbook/blob/master/37signals-is-you.md\u0026#34;) docs = loader.load() print(docs[0].page_content[:500]) 文档分割 在上一节中，我们将不同格式和来源的数据加载到了标准的文档对象中。但是，这些文档经过转换后依然很大，而我们通常只需要检索文档中与主题最相关的内容，可能只是几个段落或句子，而不需要整个文档。\n因此，在这一节中，我们将使用LangChain的文本分割器，把大型的文档分割成更小的块。\n文档分割的重要性 文档分割发生在数据加载之后，放入向量存储之前。\n如果简单地按字符长度来分割文档，可能会造成句子的断裂，导致语义的丢失或混乱。这样的分割方式，无法为我们正确地回答问题。\n合理的做法，是尽量保持语义的连贯性和完整性，分隔出有意义的块。\n文档分割的方式 在LangChain中，所有的文本分割器都遵循同一个原理，就是根据「块大小(chunk_size)」和「两个块之间的重叠大小(chunk_overlap)」进行分割。\nchunk_size指的是每个块包含的字符或Token（如单词、句子等）的数量。\nchunk_overlap指的是两个块之间共享的字符或Token的数量。chunk_overlap可以帮助保持上下文的连贯性，避免因为分割而丢失重要的信息。\nLangChain提供了多种类型的分割器，主要差别在于如何确定块的边界、块由哪些字符或Token组成、以及如何测量块的大小（按字符还是按Token）。\n元数据（Metadata）是块分割的另一个重要部分，我们需要在所有块中保持元数据的一致性，同时在需要的时候添加新的元数据。\n基于字符的分割 如何分割块通常取决于我们正在处理的文档类型。\n比如，处理代码的分割器拥有许多不同编程语言的分隔符，如Python、Ruby、C等。当分割代码文档时，它会考虑到不同编程语言之间的差异。\n步骤1：导入文本分割器 1 2 3 # RecursiveCharacterTextSplitter-递归字符文本分割器 # CharacterTextSplitter-字符文本分割器 from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter 步骤2：设定块大小和块重叠大小 1 2 chunk_size =26 chunk_overlap = 4 步骤3：初始化文本分割器 1 2 3 4 5 6 7 8 r_splitter = RecursiveCharacterTextSplitter( chunk_size=chunk_size, chunk_overlap=chunk_overlap ) c_splitter = CharacterTextSplitter( chunk_size=chunk_size, chunk_overlap=chunk_overlap ) 步骤4：使用不同的分割器对字符串进行分割 递归字符文本分割器 1 2 3 text2 = \u0026#39;abcdefghijklmnopqrstuvwxyzabcdefg\u0026#39; r_splitter.split_text(text2) # [\u0026#39;abcdefghijklmnopqrstuvwxyz\u0026#39;, \u0026#39;wxyzabcdefg\u0026#39;] 可以看到，第二个块是从「wxyz」开始的，刚好是我们设定的块重叠大小。\n1 2 3 text3 = \u0026#34;a b c d e f g h i j k l m n o p q r s t u v w x y z\u0026#34; r_splitter.split_text(text3) # [\u0026#39;a b c d e f g h i j k l m\u0026#39;, \u0026#39;l m n o p q r s t u v w x\u0026#39;, \u0026#39;w x y z\u0026#39;] 字符文本分割器 1 2 c_splitter.split_text(text3) # [\u0026#39;a b c d e f g h i j k l m n o p q r s t u v w x y z\u0026#39;] 可以看到，字符文本分割器实际并没有分割这个字符串，这是因为字符文本分割器默认是以换行符为分隔符的，为此，我们需要将分隔符设置为空格。\n1 2 3 4 5 6 7 c_splitter = CharacterTextSplitter( chunk_size=chunk_size, chunk_overlap=chunk_overlap, separator = \u0026#39; \u0026#39; ) c_splitter.split_text(text3) # [\u0026#39;a b c d e f g h i j k l m\u0026#39;, \u0026#39;l m n o p q r s t u v w x\u0026#39;, \u0026#39;w x y z\u0026#39;] 步骤5：递归分割长段落 1 2 3 4 5 6 7 8 9 10 11 12 13 14 some_text = \u0026#34;\u0026#34;\u0026#34;When writing documents, writers will use document structure to group content. \\ This can convey to the reader, which idea\u0026#39;s are related. For example, closely related ideas \\ are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n \\ Paragraphs are often delimited with a carriage return or two carriage returns. \\ Carriage returns are the \u0026#34;backslash n\u0026#34; you see embedded in this string. \\ Sentences have a period at the end, but also, have a space.\\ and words are separated by space.\u0026#34;\u0026#34;\u0026#34; r_splitter = RecursiveCharacterTextSplitter( chunk_size=150, chunk_overlap=0, separators=[\u0026#34;\\n\\n\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;, \u0026#34;\u0026#34;] ) r_splitter.split_text(some_text) 这里，我们传入一个分隔符列表，依次为双换行符、单换行符、空格和一个空字符。\n这就意味着，当你分割一段文本时，它会首先采用双换行符来尝试初步分割，并视情况依次使用其他的分隔符来进一步分割。\n最终分割结果如下：\n[\u0026ldquo;When writing documents, writers will use document structure to group content. This can convey to the reader, which idea\u0026rsquo;s are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\u0026rdquo;,\n\u0026lsquo;Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \u0026ldquo;backslash n\u0026rdquo; you see embedded in this string. Sentences have a period at the end, but also, have a space.and words are separated by space.\u0026rsquo;]\n如果需要按照句子进行分隔，则还要用正则表达式添加一个句号分隔符：\n1 2 3 4 5 6 r_splitter = RecursiveCharacterTextSplitter( chunk_size=150, chunk_overlap=0, separators=[\u0026#34;\\n\\n\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34;(?\u0026lt;=\\. )\u0026#34;, \u0026#34; \u0026#34;, \u0026#34;\u0026#34;] ) r_splitter.split_text(some_text) [\u0026ldquo;When writing documents, writers will use document structure to group content. This can convey to the reader, which idea\u0026rsquo;s are related.\u0026rdquo;,\n\u0026lsquo;For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\u0026rsquo;,\n\u0026lsquo;Paragraphs are often delimited with a carriage return or two carriage returns.\u0026rsquo;,\n\u0026lsquo;Carriage returns are the \u0026ldquo;backslash n\u0026rdquo; you see embedded in this string.\u0026rsquo;,\n\u0026lsquo;Sentences have a period at the end, but also, have a space.and words are separated by space.\u0026rsquo;]\n这就是递归字符文本分割器名字中“递归”的含义，总的来说，我们更建议在通用文本中使用递归字符文本分割器。\n基于Token的分割 很多LLM的上下文窗口长度限制是按照Token来计数的。因此，以LLM的视角，按照Token对文本进行分隔，通常可以得到更好的结果。\n为了理解基于字符分割和基于Token分割的区别，我们可以用一个简单的例子来说明。\n1 2 3 4 from langchain.text_splitter import TokenTextSplitter text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0) text1 = \u0026#34;foo bar bazzyfoo\u0026#34; text_splitter.split_text(text1) 这里，我们创建了一个Token文本分割器，将块大小设为1，块重叠大小设为0，相当于将任意字符串分割成了单个Token组成的列表，每个Token的内容如下：\n1 [\u0026#39;foo\u0026#39;, \u0026#39; bar\u0026#39;, \u0026#39; b\u0026#39;, \u0026#39;az\u0026#39;, \u0026#39;zy\u0026#39;, \u0026#39;foo\u0026#39;] 因此，Token的长度和字符长度是不一样的，Token通常为4个字符。\n分割Markdown文档 分块的目的旨在将具有共同上下文的文本放在一起。\n通常，我们可以通过使用指定分隔符来进行分隔，但有些类型的文档（例如 Markdown）本身就具有可用于分割的结构（如标题）。\nMarkdown标题文本分割器会根据标题或子标题来分割一个Markdown文档，并将标题作为元数据添加到每个块中。\n步骤1：定义一个Markdown文档 1 2 3 4 5 6 7 8 9 10 from langchain.document_loaders import NotionDirectoryLoader from langchain.text_splitter import MarkdownHeaderTextSplitter markdown_document = \u0026#34;\u0026#34;\u0026#34;# Title\\n\\n \\ ## Chapter 1\\n\\n \\ Hi this is Jim\\n\\n Hi this is Joe\\n\\n \\ ### Section \\n\\n \\ Hi this is Lance \\n\\n ## Chapter 2\\n\\n \\ Hi this is Molly\u0026#34;\u0026#34;\u0026#34; 步骤2：定义想要分割的标题列表和名称 1 2 3 4 5 headers_to_split_on = [ (\u0026#34;#\u0026#34;, \u0026#34;Header 1\u0026#34;), (\u0026#34;##\u0026#34;, \u0026#34;Header 2\u0026#34;), (\u0026#34;###\u0026#34;, \u0026#34;Header 3\u0026#34;), ] 步骤3：初始化Markdown标题文本切分器，分割Markdown文档 1 2 3 4 5 6 7 8 9 10 markdown_splitter = MarkdownHeaderTextSplitter( headers_to_split_on=headers_to_split_on ) md_header_splits = markdown_splitter.split_text(markdown_document) md_header_splits[0] # Document(page_content=\u0026#39;Hi this is Jim \\nHi this is Joe\u0026#39;, metadata={\u0026#39;Header 1\u0026#39;: \u0026#39;Title\u0026#39;, \u0026#39;Header 2\u0026#39;: \u0026#39;Chapter 1\u0026#39;}) md_header_splits[1] # Document(page_content=\u0026#39;Hi this is Lance\u0026#39;, metadata={\u0026#39;Header 1\u0026#39;: \u0026#39;Title\u0026#39;, \u0026#39;Header 2\u0026#39;: \u0026#39;Chapter 1\u0026#39;, \u0026#39;Header 3\u0026#39;: \u0026#39;Section\u0026#39;}) 可以看到，每个块都包含了页面内容和元数据，元数据中记录了该块所属的标题和子标题。\n我们已经了解了如何将长文档分割为语义相关的块，并且包含正确的元数据。接下来要做的就是将这些块放入到一个索引中，这样当我们要回答某个数据集相关的问题时，就能轻松地检索到对应的块。\n要实现这一目标，我们需要用到两个技术：嵌入(Embedding)和向量存储(Vector Store) 。\n向量存储和嵌入 嵌入是将一段文本转化为数值形式。具有相似内容的文本在数值空间中会有相似的向量，这就意味着我们可以通过比较这些向量，来找出相似的文本片段。\n而向量存储是一种数据库，它用来存储分割后的文档片段以及它们对应的嵌入，方便我们后续根据问题查找相关的文档。\n整个过程如下：\n提出一个问题，并为它生成一个嵌入； 将它跟向量存储里的所有不同的向量进行比较； 选出最相似的前n个片段； 将选出的片段和问题一起输入到LLM里，得到一个答案。 为了帮助理解，我们先看一个简单的例子：\n步骤1：提供一些例句，其中前两句非常相似，第三句则与前两句关联不大 1 2 3 sentence1 = \u0026#34;i like dogs\u0026#34; sentence2 = \u0026#34;i like canines\u0026#34; sentence3 = \u0026#34;the weather is ugly outside\u0026#34; 步骤2：使用Embbeding类为每个句子生成一个嵌入 1 2 3 4 5 6 from langchain.embeddings.openai import OpenAIEmbeddings embedding = OpenAIEmbeddings() embedding1 = embedding.embed_query(sentence1) embedding2 = embedding.embed_query(sentence2) embedding3 = embedding.embed_query(sentence3) 步骤3：用点积(dot product)来计算两两之间的嵌入相似度 1 2 3 4 5 6 7 8 9 10 import numpy as np np.dot(embedding1, embedding2) # 0.9631853877103518 np.dot(embedding1, embedding3) # 0.7709997651294672 np.dot(embedding2, embedding3) # 0.7596334120325523 点积的值越大，代表相似度就越高。\n再来看一个实际的例子：\n目标是为提供的所有PDF文档生成嵌入，并把它们存储在一个向量存储里。\n步骤1：加载PDF文档 1 2 3 4 5 6 7 8 9 10 11 12 13 from langchain.document_loaders import PyPDFLoader # 加载 PDF loaders = [ # 重复加载第一个文档，模拟一些脏数据 PyPDFLoader(\u0026#34;docs/cs229_lectures/MachineLearning-Lecture01.pdf\u0026#34;), PyPDFLoader(\u0026#34;docs/cs229_lectures/MachineLearning-Lecture01.pdf\u0026#34;), PyPDFLoader(\u0026#34;docs/cs229_lectures/MachineLearning-Lecture02.pdf\u0026#34;), PyPDFLoader(\u0026#34;docs/cs229_lectures/MachineLearning-Lecture03.pdf\u0026#34;) ] docs = [] for loader in loaders: docs.extend(loader.load()) 步骤2：用递归字符文本分割器来把文档分成块 1 2 3 4 5 6 7 8 # 分割 from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter( chunk_size = 1500, chunk_overlap = 150 ) splits = text_splitter.split_documents(docs) 步骤3：为每个块生成嵌入，并创建Chroma向量存储 这里用到的向量存储是Chroma。Chroma是一种轻量级、基于内存的向量存储，使用起来很方便。\n1 2 3 4 5 6 7 8 9 10 11 from langchain.vectorstores import Chroma # 可先用[rm -rf ./docs/chroma]移除可能存在的旧数据库数据 persist_directory = \u0026#39;docs/chroma/\u0026#39; # 传入之前创建的分割和嵌入，以及持久化目录 vectordb = Chroma.from_documents( documents=splits, embedding=embedding, persist_directory=persist_directory ) 步骤4：用相似性搜索方法来查找文档 1 2 3 question = \u0026#34;is there an email i can ask for help\u0026#34; # K=3用于指定返回的文档数量 docs = vectordb.similarity_search(question,k=3) 可以打印文档的长度和内容来检查：\n1 2 3 4 len(docs) # 3 docs[0].page_content 步骤5：持久化向量数据库，以便以后使用 1 vectordb.persist() 接下来我们将讨论一些边缘案例，展示几种可能出现失败情况：\n失败情况1：重复的块导致重复的冗余信息 1 2 3 4 5 question = \u0026#34;what did they say about matlab?\u0026#34; docs = vectordb.similarity_search(question,k=5) docs[0] docs[1] 其中，docs[0] 和 docs[1] 得到的结果是相同的，这是因为我们在一开始就有意重复加载了第一个文档。\n这样做的结果是，我们把两个内容相同的分块都传给了语言模型。而第二个分块是没有价值的，如果换成一个内容不同的分块会更好，这样至少语言模型可以从中获取更多信息。\n在下一课中，我们将讨论如何在保证检索到相关的块的同时，也能保证每个块都是唯一的。\n失败情况2：无法完整捕捉到问题中的关键信息 比如下面这个问题，“第三堂课里他们讲了什么关于回归的内容？”\n1 2 question = \u0026#34;what did they say about regression in the third lecture?\u0026#34; docs = vectordb.similarity_search(question,k=5) 一般来说，我们应该能看出，问题的提问者是想要从第三堂课里找到答案的。\n但实际上，当我们遍历所有文档，并打印出元数据后会发现，结果里实际上混合了多个文档的内容。\n1 2 3 4 5 6 7 8 for doc in docs: print(doc.metadata) # {\u0026#39;source\u0026#39;: \u0026#39;docs/cs229_lectures/MachineLearning-Lecture03.pdf\u0026#39;, \u0026#39;page\u0026#39;: 0} # {\u0026#39;source\u0026#39;: \u0026#39;docs/cs229_lectures/MachineLearning-Lecture03.pdf\u0026#39;, \u0026#39;page\u0026#39;: 14} # {\u0026#39;source\u0026#39;: \u0026#39;docs/cs229_lectures/MachineLearning-Lecture02.pdf\u0026#39;, \u0026#39;page\u0026#39;: 0} # {\u0026#39;source\u0026#39;: \u0026#39;docs/cs229_lectures/MachineLearning-Lecture03.pdf\u0026#39;, \u0026#39;page\u0026#39;: 6} # {\u0026#39;source\u0026#39;: \u0026#39;docs/cs229_lectures/MachineLearning-Lecture01.pdf\u0026#39;, \u0026#39;page\u0026#39;: 8} 这是因为，我们只是基于嵌入做了一个语义搜索，它为整个句子生成了一个嵌入，并且可能会更关注于“回归”这个词。\n当我们查看第五个文档时，就会发现它确实提到了“回归”这个词。\n失败情况3：随着检索文档数量的增加，相关性逐渐降低 当我们尝试改变k值，也就是检索的文档数量时，我们会得到更多的文档，但结果列表后面的文档可能没有前面的那些相关性强。\n检索 在这一课中，我们将深入探讨「检索」技术，并介绍一些更先进的方法来克服上一课的边缘情况。\n检索是检索增强生成（RAG）流程的核心。\n解决多样性：最大边缘相关性 最大边缘相关性(MMR, Maximum Marginal Relevance)背后的理念是，如果我们总是选择与查询在嵌入空间中最相似的文档，我们可能会错过一些多元化的信息。\nMMR可以帮助我们选择一个更多样化的文档集合。\nMMR在保持查询相关性的同时，尽量增加结果之间的多样性，它的做法是：\n首先发送一个查询，得到一组回答； 用\u0026quot;fetch_k\u0026quot;参数指定我们想要获取的响应数量，这完全基于语义相似性； 然后，针对这个较小的文档集合，从多样性方面进行优化； 最后从这组文档中，选择\u0026quot;k\u0026quot;个响应返回给用户。 我们用一个简单的例子来帮助理解：\n目标是查询有指定特征的蘑菇信息。\n步骤1：创建Chroma向量存储 1 2 3 4 5 6 7 8 9 from langchain.vectorstores import Chroma from langchain.embeddings.openai import OpenAIEmbeddings persist_directory = \u0026#39;docs/chroma/\u0026#39; embedding = OpenAIEmbeddings() vectordb = Chroma( persist_directory=persist_directory, embedding_function=embedding ) 步骤2：用少量信息创建一个小型数据库 1 2 3 4 5 6 7 8 9 10 # 鹅膏菌有一个巨大而雄伟的子实体(地上部分)。 # 有大子实体的蘑菇是鹅膏菌。有些品种是全白色的。 # 鹅膏菌，又叫死亡帽，是所有已知蘑菇中毒性最强的一种。 texts = [ \u0026#34;\u0026#34;\u0026#34;The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\u0026#34;\u0026#34;\u0026#34;, \u0026#34;\u0026#34;\u0026#34;A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\u0026#34;\u0026#34;\u0026#34;, \u0026#34;\u0026#34;\u0026#34;A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\u0026#34;\u0026#34;\u0026#34;, ] smalldb = Chroma.from_texts(texts, embedding=embedding) 步骤3：进行相似性搜索 1 2 3 4 5 6 # 告诉我有关带有大子实体的全白蘑菇的信息 question = \u0026#34;Tell me about all-white mushrooms with large fruiting bodies\u0026#34; smalldb.similarity_search(question, k=2) # [Document(page_content=\u0026#39;A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\u0026#39;, metadata={}), # Document(page_content=\u0026#39;The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\u0026#39;, metadata={})] 可以看到，它根据k值返回了两个最相关的文档，但没有提到它们有毒的事实。\n步骤4：进行MMR搜索 1 2 3 smalldb.max_marginal_relevance_search(question,k=2, fetch_k=3) # [Document(page_content=\u0026#39;A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\u0026#39;, metadata={}), # Document(page_content=\u0026#39;A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\u0026#39;, metadata={})] 这里我们传入了\u0026quot;k=2\u0026quot;，表示仍然想返回两个文档，但我们设置了\u0026quot;fetch_k=3\u0026quot;，表示想获取三个文档。然后我们就可以看到，返回的文档中包含了它们有毒的事实。\n现在我们试着用这个方法来处理上一节课中的失败情况1：\n1 2 3 4 5 6 7 8 question = \u0026#34;what did they say about matlab?\u0026#34; docs_mmr = vectordb.max_marginal_relevance_search(question,k=3) docs_mmr[0].page_content[:100] # \u0026#39;those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people \u0026#39; docs_mmr[1].page_content[:100] # \u0026#39;algorithm then? So what’s different? How come I was making all that noise earlier about \\nleast squa\u0026#39; 可以看到，第一个文档跟之前一样，因为它最相关。而第二个文档这次就不同了，这说明MMR让回答中增加了一些多样性。\n解决特殊性：使用自查询检索器处理元数据 自查询使用语言模型将原始问题分割为两个独立的部分，一个过滤器和一个搜索项。\n搜索项就是我们在语义上想要查找的问题内容。\n过滤器则是包含我们想要过滤的元数据。\n比如，“1980年制作的关于外星人的电影有哪些”，语义部分就是“关于外星人的电影”，元数据部分则是“电影年份应为1980年”。\n我们先手动指定一个元数据过滤器来验证它的效果。\n目标是处理上一节课的失败情况2：\n1 2 3 4 5 6 7 8 9 10 11 12 13 question = \u0026#34;what did they say about regression in the third lecture?\u0026#34; # 指定源为第三堂课的PDF文档 docs = vectordb.similarity_search( question, k=3, filter={\u0026#34;source\u0026#34;:\u0026#34;docs/cs229_lectures/MachineLearning-Lecture03.pdf\u0026#34;} ) for d in docs: print(d.metadata) # {\u0026#39;source\u0026#39;: \u0026#39;docs/cs229_lectures/MachineLearning-Lecture03.pdf\u0026#39;, \u0026#39;page\u0026#39;: 0} # {\u0026#39;source\u0026#39;: \u0026#39;docs/cs229_lectures/MachineLearning-Lecture03.pdf\u0026#39;, \u0026#39;page\u0026#39;: 14} # {\u0026#39;source\u0026#39;: \u0026#39;docs/cs229_lectures/MachineLearning-Lecture03.pdf\u0026#39;, \u0026#39;page\u0026#39;: 4} 可以看到，现在检索到的文档都来自那一堂课了。\n我们还可以使用SelfQueryRetriever，从问题本身推断出元数据。\n步骤1：提供元数据字段信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from langchain.llms import OpenAI from langchain.retrievers.self_query.base import SelfQueryRetriever from langchain.chains.query_constructor.base import AttributeInfo metadata_field_info = [ AttributeInfo( name=\u0026#34;source\u0026#34;, description=\u0026#34;The lecture the chunk is from, should be one of `docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `docs/cs229_lectures/MachineLearning-Lecture03.pdf`\u0026#34;, type=\u0026#34;string\u0026#34;, ), AttributeInfo( name=\u0026#34;page\u0026#34;, description=\u0026#34;The page from the lecture\u0026#34;, type=\u0026#34;integer\u0026#34;, ), ] 这个例子中的元数据只有两个字段，源（source）和页（page）。我们需要填写每个字段的名称、描述和类型。这些信息会被传给语言模型，所以需要尽可能描述得清楚。\n步骤2：初始化自查询检索器 1 2 3 4 5 6 7 8 9 10 # 指定文档实际内容的信息 document_content_description = \u0026#34;Lecture notes\u0026#34; llm = OpenAI(temperature=0) retriever = SelfQueryRetriever.from_llm( llm, vectordb, document_content_description, metadata_field_info, verbose=True ) 步骤3：运行自查询检索器搜索问题 1 2 3 4 5 6 7 8 9 question = \u0026#34;what did they say about regression in the third lecture?\u0026#34; docs = retriever.get_relevant_documents(question) for d in docs: print(d.metadata) # {\u0026#39;source\u0026#39;: \u0026#39;docs/cs229_lectures/MachineLearning-Lecture03.pdf\u0026#39;, \u0026#39;page\u0026#39;: 0} # {\u0026#39;source\u0026#39;: \u0026#39;docs/cs229_lectures/MachineLearning-Lecture03.pdf\u0026#39;, \u0026#39;page\u0026#39;: 14} # {\u0026#39;source\u0026#39;: \u0026#39;docs/cs229_lectures/MachineLearning-Lecture03.pdf\u0026#39;, \u0026#39;page\u0026#39;: 4} 可以看到，语义部分表明这是一个关于回归的查询。过滤器部分表明我们只想看那些source值为指定值的文档。\n而从打印出的元数据看，它们都来自指定的那一堂课，说明自查询检索器确实可以用来精确地进行元数据过滤。\n解决相关性：使用上下文压缩提取出与查询最相关的部分 提高检索文档质量的另一种方法是压缩。\n当你提出一个问题时，你会得到整个存储的文档，但可能只有其中一小部分是跟问题相关的。\n也就是说，与查询最相关的信息可能被隐藏在包含大量无关文本的文档里。\n上下文压缩就是为了解决这个问题的。\n通过压缩，你可以先让语言模型提取出最相关的片段，然后只把最相关的片段传给最终的语言模型调用。\n这会增加语言模型调用的成本，但也会让最终的答案更集中在最重要的内容上，这需要我们自己权衡。\n步骤1：创建上下文压缩检索器 1 2 3 4 5 6 7 8 9 10 11 from langchain.retrievers import ContextualCompressionRetriever from langchain.retrievers.document_compressors import LLMChainExtractor # 包装我们的向量存储 llm = OpenAI(temperature=0) compressor = LLMChainExtractor.from_llm(llm) compression_retriever = ContextualCompressionRetriever( base_compressor=compressor, base_retriever=vectordb.as_retriever() ) 步骤2：提出问题，检索压缩后的相关文档 1 2 3 question = \u0026#34;what did they say about matlab?\u0026#34; compressed_docs = compression_retriever.get_relevant_documents(question) pretty_print_docs(compressed_docs) 从压缩后的文档我们可以看到， 一，它们比普通的文档短得多； 二，仍然有重复的内容，这是因为底层我们还是用的语义搜索算法。\n为了解决内容重复的问题，我们可以在创建检索器时，结合前面的内容，把搜索类型设置为MMR。\n1 2 3 4 5 6 7 8 compression_retriever = ContextualCompressionRetriever( base_compressor=compressor, base_retriever=vectordb.as_retriever(search_type = \u0026#34;mmr\u0026#34;) ) question = \u0026#34;what did they say about matlab?\u0026#34; compressed_docs = compression_retriever.get_relevant_documents(question) pretty_print_docs(compressed_docs) 重新运行之后，我们得到的就是一个没有任何重复信息的过滤后的结果集了。\n下一步，我们将讨论如何使用这些检索到的文档来回答用户的问题。\n问答 在这节课中，我们将学习如何利用检索到的文档来回答用户的问题。\n整个过程可以拆分为以下几个步骤：\n用户输入一个问题（Question） 从向量存储（Store）中检索出与问题相关的文档分块（Relavant splits） 将这些分块连同系统提示（System:Prompt）和用户问题（Human:Question）一起作为输入传给语言模型（LLM） 语言模型根据输入生成答案（Answer） 默认使用的是stuff方法，其特点如下：\n特点 优点 缺点 将所有检索到的分块放入同一个上下文窗口中，只需要对语言模型进行一次调用。 简单、廉价且效果不错。 当检索到的文档过多时，由于上下文窗口长度有限，可能无法将所有分块都传入。 为了解决上下文窗口长度限制的问题，我们可以使用Map-reduce、Refine和Map-rerank三种方法，这些方法我们在之前的课程中已经简要介绍过了，今天我们将进一步深入了解。\nstuff 步骤1：加载之前保存的向量数据库 1 2 3 4 5 from langchain.vectorstores import Chroma from langchain.embeddings.openai import OpenAIEmbeddings persist_directory = \u0026#39;docs/chroma/\u0026#39; embedding = OpenAIEmbeddings() vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding) 步骤2：初始化将用于回答问题的语言模型 1 2 3 llm_name = \u0026#34;gpt-3.5-turbo\u0026#34; from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model_name=llm_name, temperature=0) temperature参数设置为0，可以帮助我们得到更准确的答案，因为它降低了语言模型的可变性，通常能给我们最高置信度、最可靠的答案。\n步骤3：导入、创建、调用检索问答链，输入问题，并获取答案 1 2 3 4 5 6 7 8 9 question = \u0026#34;What are major topics for this class?\u0026#34; from langchain.chains import RetrievalQA qa_chain = RetrievalQA.from_chain_type( llm, retriever=vectordb.as_retriever() ) result = qa_chain({\u0026#34;query\u0026#34;: question}) result[\u0026#34;result\u0026#34;] \u0026lsquo;The major topic for this class is machine learning. Additionally, the class may cover statistics and algebra as refreshers in the discussion sections. Later in the quarter, the discussion sections will also cover extensions for the material taught in the main lectures.\u0026rsquo;\n步骤4：使用提示模板优化输出结果 提示模板是一种可以帮助语言模型生成更符合要求的输出结果的技巧，这里我们使用的提示模板主要是为了让输出结果更简洁、更少编造、更礼貌。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from langchain.prompts import PromptTemplate # 构建提示词 # {context}：上下文占位符，用于放置文档内容 # {question}：问题占位符，放置要查询的问题 template = \u0026#34;\u0026#34;\u0026#34;Use the following pieces of context to answer the question at the end. If you don\u0026#39;t know the answer, just say that you don\u0026#39;t know, don\u0026#39;t try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \u0026#34;thanks for asking!\u0026#34; at the end of the answer. {context} Question: {question} Helpful Answer:\u0026#34;\u0026#34;\u0026#34; QA_CHAIN_PROMPT = PromptTemplate.from_template(template) # 运行链 # return_source_documents=True用于支持查看检索到的文档 qa_chain = RetrievalQA.from_chain_type( llm, retriever=vectordb.as_retriever(), return_source_documents=True, chain_type_kwargs={\u0026#34;prompt\u0026#34;: QA_CHAIN_PROMPT} ) question = \u0026#34;Is probability a class topic?\u0026#34; result = qa_chain({\u0026#34;query\u0026#34;: question}) result[\u0026#34;result\u0026#34;] \u0026lsquo;Yes, probability is assumed to be a prerequisite for this class. The instructor assumes familiarity with basic probability and statistics, and will go over some of the prerequisites in the discussion sections as a refresher course. Thanks for asking!\u0026rsquo;\n步骤5：查看返回的源文档，理解其从哪里获取数据 1 result[\u0026#34;source_documents\u0026#34;][0] Document(page_content=\u0026ldquo;of this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I\u0026rsquo;ll say a bit more about that later. \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I\u0026rsquo;m gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it\u0026rsquo;s been a while \\nsince you\u0026rsquo;ve seen some of this material. At some of the discussion sections, we\u0026rsquo;ll actually \\ngo over some of the prerequisites, sort of as a refresher course under prerequisite class. \\nI\u0026rsquo;ll say a bit more about that later as well. \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you\u0026rsquo;ve taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I\u0026rsquo;m \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that\u0026rsquo;d be even better. \\nBut if you don\u0026rsquo;t quite know or if you\u0026rsquo;re not qu ite sure, that\u0026rsquo;s fine, too. We\u0026rsquo;ll go over it in \\nthe review sections.\u0026rdquo;, metadata={\u0026lsquo;source\u0026rsquo;: \u0026lsquo;docs/cs229_lectures/MachineLearning-Lecture01.pdf\u0026rsquo;, \u0026lsquo;page\u0026rsquo;: 4})\nMap-reduce Map-reduce方法的特点如下：\n特点 优点 缺点 1.将每个文档单独发送到语言模型中，根据单个文档生成答案； 可以处理任意数量的文档。 1.涉及到对语言模型的多次调用，速度较慢； 2.将所有这些答案组合在一起，再调用语言模型生成最终答案。 2.信息可能分散在不同的文档中，无法基于同一个上下文获取信息，结果可能不准确。 1 2 3 4 5 6 7 8 qa_chain_mr = RetrievalQA.from_chain_type( llm, retriever=vectordb.as_retriever(), chain_type=\u0026#34;map_reduce\u0026#34; ) result = qa_chain_mr({\u0026#34;query\u0026#34;: question}) result[\u0026#34;result\u0026#34;] \u0026lsquo;There is no clear answer to this question based on the given portion of the document. The document mentions familiarity with basic probability and statistics as a prerequisite for the class, and there is a brief mention of probability in the text, but it is not clear if it is a main topic of the class.\u0026rsquo;\n使用LangSmith平台了解这些链内部的调用情况 LangSmith 是一个用于构建生产级 LLM 应用程序的平台。\n它可以让您轻松地调试、测试、评估和监控基于任何 LLM 框架构建的链和智能代理，并与使用 LLM 构建的开源框架 LangChain 完美集成。\n要体验这个平台的功能，你需要：\n前往LangSmith平台注册（可能需要排队） 创建 API 密钥 在以下代码中使用这个 API 密钥 取消注释以下代码，并重新运行MapReduce链 1 2 3 4 import os os.environ[\u0026#34;LANGCHAIN_TRACING_V2\u0026#34;] = \u0026#34;true\u0026#34; os.environ[\u0026#34;LANGCHAIN_ENDPOINT\u0026#34;] = \u0026#34;https://api.langchain.plus\u0026#34; os.environ[\u0026#34;LANGCHAIN_API_KEY\u0026#34;] = \u0026#34;...\u0026#34; # 替换...为你的 API 密钥 之后，就可以切换到LangSmith平台，找到刚刚运行的RetrievalQA，查看输入、输出以及调用链了：\n可以看到，MapReduceDocumentChain中涉及到了对语言模型的四次独立调用，点击其中一个，就可以看到每个文档的具体输入和输出：\n并且，可以看到，它们在最后的链中被合并为了StuffDocumentChain，也即把所有结果放到了最终调用中。点击进入就可以看到，系统消息中包含了来自前面文档的四个摘要：\nRefine Refine方法的特点如下：\n特点 优点 缺点 迭代地处理多个文档，基于前一个文档的答案来构建一个更好的答案。 允许组合信息，更鼓励信息的传递 速度较慢 1 2 3 4 5 6 7 qa_chain_mr = RetrievalQA.from_chain_type( llm, retriever=vectordb.as_retriever(), chain_type=\u0026#34;refine\u0026#34; ) result = qa_chain_mr({\u0026#34;query\u0026#34;: question}) result[\u0026#34;result\u0026#34;] \u0026ldquo;The main topic of the class is machine learning algorithms, including linear regression and classification. Basic probability and statistics, as well as linear algebra, are prerequisites for the class, but the instructor will provide a refresher course on these topics in some of the discussion sections. Later in the quarter, the discussion sections will also cover extensions for the material taught in the main lectures. The instructor will focus on a few important extensions that there wasn\u0026rsquo;t enough time to cover in the main lectures.\u0026rdquo;\n现在还有一个问题，我们目前使用的链是没有“记忆”这个概念的，这就导致了它不会记住之前的问题或答案。为了解决这个问题，我们需要引入“记忆”功能，这就是我们下一节要讲的内容。\nChat 在这节课中，我们将构建一个完整的问答聊天机器人，它将结合我们之前讲过的所有组件，并引入“聊天历史”这个概念，让它在回答问题时能够考虑到之前的对话或信息，也就是说，它能记住你之前说过什么。\n步骤1：初始化用于保存大量文档内容的向量数据库 1 2 3 4 5 from langchain.vectorstores import Chroma from langchain.embeddings.openai import OpenAIEmbeddings persist_directory = \u0026#39;docs/chroma/\u0026#39; embedding = OpenAIEmbeddings() vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding) 步骤2：初始化将作为聊天机器人使用的语言模型 1 2 3 llm_name = \u0026#34;gpt-3.5-turbo\u0026#34; from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model_name=llm_name, temperature=0) 步骤3：初始化提示模板，让输出结果更简介、更少编造、更礼貌 1 2 3 4 5 6 7 # 构建提示 from langchain.prompts import PromptTemplate template = \u0026#34;\u0026#34;\u0026#34;Use the following pieces of context to answer the question at the end. If you don\u0026#39;t know the answer, just say that you don\u0026#39;t know, don\u0026#39;t try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \u0026#34;thanks for asking!\u0026#34; at the end of the answer. {context} Question: {question} Helpful Answer:\u0026#34;\u0026#34;\u0026#34; QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\u0026#34;context\u0026#34;, \u0026#34;question\u0026#34;],template=template,) 步骤4：创建检索QA链，用于合并检索到的文本片段并调用语言模型 1 2 3 4 5 6 # 运行链 from langchain.chains import RetrievalQA qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever(), return_source_documents=True, chain_type_kwargs={\u0026#34;prompt\u0026#34;: QA_CHAIN_PROMPT}) 步骤5：使用ConversationBufferMemory增加聊天机器人的记忆功能 1 2 3 4 5 from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory( memory_key=\u0026#34;chat_history\u0026#34;, return_messages=True ) ConversationBufferMemory提供了一个聊天消息历史的记忆缓冲区，并且每次都会将这部分历史消息传入聊天机器人。\nreturn_messages=True表示将返回一个列表类型的聊天历史记录，而不是一个字符串。\n步骤6：创建ConversationalRetrievalChain（对话检索链），传入语言模型、检索器和记忆系统 1 2 3 4 5 6 7 from langchain.chains import ConversationalRetrievalChain retriever=vectordb.as_retriever() qa = ConversationalRetrievalChain.from_llm( llm, retriever=retriever, memory=memory ) ConversationalRetrievalChain会在RetrievalQAChain的基础上，将聊天历史和新提的问题整合成一个新的独立问题，以传递给向量存储库，查找相关文档。\n步骤7：使用PyPDFLoader加载所要参考的文档 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from langchain.embeddings.openai import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter from langchain.vectorstores import DocArrayInMemorySearch from langchain.document_loaders import TextLoader from langchain.chains import RetrievalQA, ConversationalRetrievalChain from langchain.memory import ConversationBufferMemory from langchain.chat_models import ChatOpenAI from langchain.document_loaders import TextLoader from langchain.document_loaders import PyPDFLoader def load_db(file, chain_type, k): # 加载文档 loader = PyPDFLoader(file) documents = loader.load() ... 步骤8：分割文档，为每个分块创建嵌入，并存储到向量存储库中。 1 2 3 4 5 6 7 8 9 10 def load_db(file, chain_type, k): ... # 分隔文档 text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150) docs = text_splitter.split_documents(documents) # 定义嵌入 embeddings = OpenAIEmbeddings() # 基于文档数据创建向量数据库 db = DocArrayInMemorySearch.from_documents(docs, embeddings) ... 步骤9：从向量数据库创建一个基于“相似度”的检索器。 1 2 3 4 5 def load_db(file, chain_type, k): ... # 定义检索器 retriever = db.as_retriever(search_type=\u0026#34;similarity\u0026#34;, search_kwargs={\u0026#34;k\u0026#34;: k}) ... 步骤10：创建对话检索链，用于将聊天历史和新提的问题整合成一个新的独立问题 1 2 3 4 5 6 7 8 9 10 11 def load_db(file, chain_type, k): ... # create a chatbot chain. Memory is managed externally. qa = ConversationalRetrievalChain.from_llm( llm=ChatOpenAI(model_name=llm_name, temperature=0), chain_type=chain_type, retriever=retriever, return_source_documents=True, return_generated_question=True, ) return qa 需要注意的是，这里我们并没有传入记忆系统，而是将记忆管理交给了GUI，这意味着聊天历史需要在链之外进行维护。\n步骤11：提供一个与聊天机器人交互的用户界面 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 import panel as pn import param class cbfs(param.Parameterized): chat_history = param.List([]) answer = param.String(\u0026#34;\u0026#34;) db_query = param.String(\u0026#34;\u0026#34;) db_response = param.List([]) def __init__(self, **params): super(cbfs, self).__init__( **params) self.panels = [] self.loaded_file = \u0026#34;docs/cs229_lectures/MachineLearning-Lecture01.pdf\u0026#34; self.qa = load_db(self.loaded_file,\u0026#34;stuff\u0026#34;, 4) def call_load_db(self, count): if count == 0 or file_input.value is None: # init or no file specified : return pn.pane.Markdown(f\u0026#34;Loaded File: {self.loaded_file}\u0026#34;) else: file_input.save(\u0026#34;temp.pdf\u0026#34;) # local copy self.loaded_file = file_input.filename button_load.button_style=\u0026#34;outline\u0026#34; self.qa = load_db(\u0026#34;temp.pdf\u0026#34;, \u0026#34;stuff\u0026#34;, 4) button_load.button_style=\u0026#34;solid\u0026#34; self.clr_history() return pn.pane.Markdown(f\u0026#34;Loaded File: {self.loaded_file}\u0026#34;) def convchain(self, query): if not query: return pn.WidgetBox(pn.Row(\u0026#39;User:\u0026#39;, pn.pane.Markdown(\u0026#34;\u0026#34;, width=600)), scroll=True) result = self.qa({\u0026#34;question\u0026#34;: query, \u0026#34;chat_history\u0026#34;: self.chat_history}) self.chat_history.extend([(query, result[\u0026#34;answer\u0026#34;])]) self.db_query = result[\u0026#34;generated_question\u0026#34;] self.db_response = result[\u0026#34;source_documents\u0026#34;] self.answer = result[\u0026#39;answer\u0026#39;] self.panels.extend([ pn.Row(\u0026#39;User:\u0026#39;, pn.pane.Markdown(query, width=600)), pn.Row(\u0026#39;ChatBot:\u0026#39;, pn.pane.Markdown(self.answer, width=600, style={\u0026#39;background-color\u0026#39;: \u0026#39;#F6F6F6\u0026#39;})) ]) inp.value = \u0026#39;\u0026#39; #clears loading indicator when cleared return pn.WidgetBox(*self.panels,scroll=True) @param.depends(\u0026#39;db_query \u0026#39;, ) def get_lquest(self): if not self.db_query : return pn.Column( pn.Row(pn.pane.Markdown(f\u0026#34;Last question to DB:\u0026#34;, styles={\u0026#39;background-color\u0026#39;: \u0026#39;#F6F6F6\u0026#39;})), pn.Row(pn.pane.Str(\u0026#34;no DB accesses so far\u0026#34;)) ) return pn.Column( pn.Row(pn.pane.Markdown(f\u0026#34;DB query:\u0026#34;, styles={\u0026#39;background-color\u0026#39;: \u0026#39;#F6F6F6\u0026#39;})), pn.pane.Str(self.db_query ) ) @param.depends(\u0026#39;db_response\u0026#39;, ) def get_sources(self): if not self.db_response: return rlist=[pn.Row(pn.pane.Markdown(f\u0026#34;Result of DB lookup:\u0026#34;, styles={\u0026#39;background-color\u0026#39;: \u0026#39;#F6F6F6\u0026#39;}))] for doc in self.db_response: rlist.append(pn.Row(pn.pane.Str(doc))) return pn.WidgetBox(*rlist, width=600, scroll=True) @param.depends(\u0026#39;convchain\u0026#39;, \u0026#39;clr_history\u0026#39;) def get_chats(self): if not self.chat_history: return pn.WidgetBox(pn.Row(pn.pane.Str(\u0026#34;No History Yet\u0026#34;)), width=600, scroll=True) rlist=[pn.Row(pn.pane.Markdown(f\u0026#34;Current Chat History variable\u0026#34;, styles={\u0026#39;background-color\u0026#39;: \u0026#39;#F6F6F6\u0026#39;}))] for exchange in self.chat_history: rlist.append(pn.Row(pn.pane.Str(exchange))) return pn.WidgetBox(*rlist, width=600, scroll=True) def clr_history(self,count=0): self.chat_history = [] return 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 cb = cbfs() file_input = pn.widgets.FileInput(accept=\u0026#39;.pdf\u0026#39;) button_load = pn.widgets.Button(name=\u0026#34;Load DB\u0026#34;, button_type=\u0026#39;primary\u0026#39;) button_clearhistory = pn.widgets.Button(name=\u0026#34;Clear History\u0026#34;, button_type=\u0026#39;warning\u0026#39;) button_clearhistory.on_click(cb.clr_history) inp = pn.widgets.TextInput( placeholder=\u0026#39;Enter text here…\u0026#39;) bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks) conversation = pn.bind(cb.convchain, inp) jpg_pane = pn.pane.Image( \u0026#39;./img/convchain.jpg\u0026#39;) tab1 = pn.Column( pn.Row(inp), pn.layout.Divider(), pn.panel(conversation, loading_indicator=True, height=300), pn.layout.Divider(), ) tab2= pn.Column( pn.panel(cb.get_lquest), pn.layout.Divider(), pn.panel(cb.get_sources ), ) tab3= pn.Column( pn.panel(cb.get_chats), pn.layout.Divider(), ) tab4=pn.Column( pn.Row( file_input, button_load, bound_button_load), pn.Row( button_clearhistory, pn.pane.Markdown(\u0026#34;Clears chat history. Can use to start a new topic\u0026#34; )), pn.layout.Divider(), pn.Row(jpg_pane.clone(width=400)) ) dashboard = pn.Column( pn.Row(pn.pane.Markdown(\u0026#39;# ChatWithYourData_Bot\u0026#39;)), pn.Tabs((\u0026#39;Conversation\u0026#39;, tab1), (\u0026#39;Database\u0026#39;, tab2), (\u0026#39;Chat History\u0026#39;, tab3),(\u0026#39;Configure\u0026#39;, tab4)) ) dashboard 步骤12：在运行起来的用户界面上进行实际的问答对话。 ","date":"2023-08-30T12:44:47+08:00","permalink":"https://blog.importzhh.me/p/iii-%E5%9F%BA%E4%BA%8E%E7%A7%81%E6%9C%89%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8langchain%E6%9E%84%E5%BB%BA%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/","title":"III: 基于私有数据使用LangChain构建聊天机器人"},{"content":"基于文档的问答 LLM可以根据从PDF文件、网页或公司内部文档中提取的文本来回答问题，这使得LLM能够与未经训练的数据结合起来，从而更灵活地适配不同应用场景。\n要构建一个基于文档的问答系统，需要引入 LangChain 的更多关键组件，例如嵌入（Embedding）模型和向量存储（Vector Stores）。\n简单实现，以轻松完成文档问答功能 首先，需要导入一些辅助构建链的工具：\n1 2 3 4 5 from langchain.chains import RetrievalQA from langchain.chat_models import ChatOpenAI from langchain.document_loaders import CSVLoader from langchain.vectorstores import DocArrayInMemorySearch from IPython.display import display, Markdown 工具 用途 RetrievalQA 检索文档 CSVLoader 加载专用数据(如CSV)，用来与模型结合使用 DocArrayInMemorySearch 内存方式的向量存储，不需要连接到任何类型的外部数据库 步骤1：初始化CSV加载器，为其指定一个CSV文件的路径 1 2 file = \u0026#39;OutdoorClothingCatalog_1000.csv\u0026#39; loader = CSVLoader(file_path=file) 步骤2：创建一个内存方式的向量存储，传入上一步创建的️️加载器。 1 2 3 4 5 from langchain.indexes import VectorstoreIndexCreator index = VectorstoreIndexCreator( vectorstore_cls=DocArrayInMemorySearch ).from_loaders([loader]) 步骤3：定义查询内容，并使用index.query生成响应 1 2 3 4 query =\u0026#34;Please list all your shirts with sun protection \\ in a table in markdown and summarize each one.\u0026#34; response = index.query(query) 步骤4：展示查询结果的表格以及摘要 1 display(Markdown(response)) 文档问答功能的底层原理 要想让语言模型与大量文档相结合，有一个关键问题必须解决。\n那就是，语言模型每次只能处理几千个单词，如何才能让它对一个大型文档的全部内容进行问答呢？\n这里就要用到嵌入（Embedding）和向量存储（Vector Stores）这两个技术。\n嵌入（Embedding） 嵌入是一种将文本片段转换为数字表示的方法，这些数字能够反映文本的语义信息。\n语义相近的文本会有相近的向量，这样我们就可以在向量空间中对文本进行比较。\n比如，\n两个同样描述宠物的句子的向量，其相似度会非常高。 而与一个描述汽车的句子的向量相比较，其相似度则会很低。 通过向量相似度，我们可以轻松地找出文本片段之间的语义关系。\n利用这个特性，我们可以从文档中检索出与问题语义最匹配的文本片段，然后将它们和问题一起交给语言模型来生成答案。\n向量数据库（Vector Database） 向量数据库是用于保存嵌入向量表示的数据库。\n我们可以将大型文档拆分成较小的块，为每个块生成一个嵌入，并将其和原始块一起存储到数据库中。\n这就是我们创建索引的过程。\n索引创建好后，我们就可以用它来查询与问题最相关的文本片段：\n当一个问题进来后，为问题生成嵌入； 将其与向量存储中的所有向量进行比较，选择最相似的n个文本片段； 将这些文本片段和问题一起传递给语言模型； 让语言模型根据检索到的文档内容生成最佳答案。 详细实现，以查看每一步的执行过程 步骤1：初始化CSV加载器，为其指定一个CSV文件的路径 1 2 loader = CSVLoader(file_path=file) docs = loader.load() 步骤2：为加载的所有文本片段生成嵌入，并存储在一个向量存储器中 1 2 3 4 5 6 7 from langchain.embeddings import OpenAIEmbeddings embeddings = OpenAIEmbeddings() db = DocArrayInMemorySearch.from_documents( docs, embeddings ) 我们可以用一段特定文本来查看生成的嵌入内容形式：\n1 2 3 4 5 6 7 8 9 embed = embeddings.embed_query(\u0026#34;Hi my name is Harrison\u0026#34;) # 打印嵌入的维度 print(len(embed)) # 1536 # 打印前5个数字 print(embed[:5]) # [-0.021930990740656853, 0.006712669972330332, -0.018181458115577698, -0.039156194776296616, -0.014079621061682701] 从打印结果可知，这个嵌入是一个1536维的向量，以及向量中的数字是如何表示的。\n我们也可以直接输入一段查询内容，查看通过向量存储器检索到的与查询内容相似的文本片段：\n1 2 3 4 5 6 7 8 9 10 query = \u0026#34;Please suggest a shirt with sunblocking\u0026#34; docs = db.similarity_search(query) # 打印检索到的文本片段数 len(docs) # 4 # 打印第一个文本片段内容 docs[0] # Document(page_content=\u0026#39;: 255\\nname: Sun Shield Shirt by\\ndescription: \u0026#34;Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\n\\nSize \u0026amp; Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\n\\nFabric \u0026amp; Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\n\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\n\\nSun Protection That Won\\\u0026#39;t Wear Off\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\\u0026#39;s harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.\u0026#39;, metadata={\u0026#39;source\u0026#39;: \u0026#39;OutdoorClothingCatalog_1000.csv\u0026#39;, \u0026#39;row\u0026#39;: 255}) 从打印结果可知，检索到了4个相似的文本片段，而返回的第一个文本片段也确实与查询内容相关。\n步骤3：使用RetrievalQA链对查询进行检索，然后在检索的文档上进行问答 1 2 3 4 5 6 7 8 9 10 retriever = db.as_retriever() llm = ChatOpenAI(temperature = 0.0) # stuff表示将文本片段合并成一段文本 qa_stuff = RetrievalQA.from_chain_type( llm=llm, chain_type=\u0026#34;stuff\u0026#34;, retriever=retriever, verbose=True ) RetrievalQA链其实就是把合并文本片段和调用语言模型这两步骤封装起来，如果没有RetrievalQA链，我们需要这样子实现：\n1.将检索出来的文本片段合并成一段文本\n1 qdocs = \u0026#34;\u0026#34;.join([docs[i].page_content for i in range(len(docs))]) 2.将合并后的文本和问题一起传给LLM\n1 2 response = llm.call_as_llm(f\u0026#34;{qdocs} Question: Please list all your \\ shirts with sun protection in a table in markdown and summarize each one.\u0026#34;) 步骤4：创建一个查询，并把查询的内容传入链并运行 1 response = qa_stuff.run(query) 步骤5：展示查询结果的表格以及摘要 1 display(Markdown(response)) 可选的链类型(chain_type) stuff 将所有内容放入一个提示中，发送给语言模型并获取一个回答。这种方法简单、廉价且效果不错。\n当文档数量较少且文档长度较短的情况下，这种方法是可行的。\nMap_reduce 将每个内容和问题一起传递给语言模型，并将所有单独的回答汇总成最终答案。\n这种方法可以处理任意数量的文档，并且可以并行处理各个问题。\nRefine 迭代地处理多个文档，它会基于前一个文档的答案来构建答案。\n这种方法适合组合信息和逐步构建答案，但速度较慢。\nMap_rerank 每个文档进行单独的语言模型调用，并要求返回一个分数，然后选择最高分数的答案。\n这种方法需要告诉语言模型如何评分，并且需要针对这部分指令进行优化。\n评估 评估有两个目的：\n检验LLM应用是否达到了验收标准 分析改动对于LLM应用性能的影响 基本的思路就是利用语言模型本身和链本身，来辅助评估其他的语言模型、链和应用程序。\n我们还是以上一节课的文档问答应用为例。\n评估过程需要用到评估数据集，我们可以直接硬编码一些示例数据集，比如：\n1 2 3 4 5 6 7 8 9 10 11 12 examples = [ { \u0026#34;query\u0026#34;: \u0026#34;Do the Cozy Comfort Pullover Set\\ have side pockets?\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;Yes\u0026#34; }, { \u0026#34;query\u0026#34;: \u0026#34;What collection is the Ultra-Lofty \\ 850 Stretch Down Hooded Jacket from?\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;The DownTek collection\u0026#34; } ] 但这种方式不太方便扩展，也比较耗时，所以，我们可以——\n步骤1：使用语言模型自动生成评估数据集 QAGenerateChain链用于接收文档，并借助语言模型为每个文档生成一个问答对。\n1 2 3 4 5 6 7 from langchain.evaluation.qa import QAGenerateChain example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI()) new_examples = example_gen_chain.apply_and_parse( [{\u0026#34;doc\u0026#34;: t} for t in data[:5]] ) 我们可以打印看下其返回的内容：\n1 2 3 4 print(new_examples[0]) {\u0026#39;query\u0026#39;: \u0026#34;What is the weight of each pair of Women\u0026#39;s Campside Oxfords?\u0026#34;, \u0026#39;answer\u0026#39;: \u0026#34;The approximate weight of each pair of Women\u0026#39;s Campside Oxfords is 1 lb. 1 oz.\u0026#34;} 步骤2：将生成的问答对添加到已有的评估数据集中 1 examples += new_examples 步骤3：为所有不同的示例生成实际答案 1 2 # 这里的qa对应的是上一节课的RetrievalQA链 predictions = qa.apply(examples) 步骤4：对LLM应用的输出进行评估 1 2 3 4 5 from langchain.evaluation.qa import QAEvalChain llm = ChatOpenAI(temperature=0) eval_chain = QAEvalChain.from_llm(llm) # 传入示例列表和实际答案列表 graded_outputs = eval_chain.evaluate(examples, predictions) 步骤5：打印问题、标准答案、实际答案和评分 1 2 3 4 5 6 7 for i, eg in enumerate(examples): print(f\u0026#34;Example {i}:\u0026#34;) print(\u0026#34;Question: \u0026#34; + predictions[i][\u0026#39;query\u0026#39;]) print(\u0026#34;Real Answer: \u0026#34; + predictions[i][\u0026#39;answer\u0026#39;]) print(\u0026#34;Predicted Answer: \u0026#34; + predictions[i][\u0026#39;result\u0026#39;]) print(\u0026#34;Predicted Grade: \u0026#34; + graded_outputs[i][\u0026#39;text\u0026#39;]) print() 列举其中的前3个评估结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Example 0: Question: Do the Cozy Comfort Pullover Set have side pockets? Real Answer: Yes Predicted Answer: The Cozy Comfort Pullover Set, Stripe does have side pockets. Predicted Grade: CORRECT Example 1: Question: What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from? Real Answer: The DownTek collection Predicted Answer: The Ultra-Lofty 850 Stretch Down Hooded Jacket is from the DownTek collection. Predicted Grade: CORRECT Example 2: Question: What is the weight of each pair of Women\u0026#39;s Campside Oxfords? Real Answer: The approximate weight of each pair of Women\u0026#39;s Campside Oxfords is 1 lb. 1 oz. Predicted Answer: The weight of each pair of Women\u0026#39;s Campside Oxfords is approximately 1 lb. 1 oz. Predicted Grade: CORRECT ... 对比第一个评估结果的两个答案可以看到，其标准答案较为简洁，而实际答案则较为详细，但表达的意思都是正确的，语言模型也能识别，因此才把它标记为正确的。\n虽然这两个字符串完全不同，以致于使用传统的正则表达式等手段是无法对它们进行比较的。\n这里就体现了使用语言模型进行评估的优势——同一个问题的答案可能有很多不同的变体，只要意思相通，就应该被认为是相似的。\n使用LangChain评估平台 以上操作都可以在LangChain评估平台上以可视化UI界面的形式展示，并对数据进行持久化，包括：\n查看和跟踪评估过程中的输入和输出 可视化链中每个步骤输出的信息 将示例添加到数据集中，以便持久化和进一步评估 代理 大型语言模型可以作为一个推理引擎，只要给它提供文本或其他信息源，它就会利用互联网上学习到的背景知识或你提供的新信息，来回答问题、推理内容或决定下一步的操作。\n这就是LangChain的代理框架能够帮我们实现的事情，而代理也正是LangChain最强大的功能之一。\n使用内置于 LangChain 的工具 步骤1：初始化语言模型 1 2 3 4 5 6 7 8 from langchain.agents.agent_toolkits import create_python_agent from langchain.agents import load_tools, initialize_agent from langchain.agents import AgentType from langchain.tools.python.tool import PythonREPLTool from langchain.python import PythonREPL from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(temperature=0) temperature参数 语言模型作为代理的推理引擎，会连接到其他数据和计算资源，我们会希望这个推理引擎尽可能地好用且精确，因此需要把temperature参数设为0。\n步骤2：加载工具 1 2 3 # llm-math：解决数学问题 # wikipedia：查询维基百科 tools = load_tools([\u0026#34;llm-math\u0026#34;,\u0026#34;wikipedia\u0026#34;], llm=llm) 步骤3：初始化代理 1 2 3 4 5 6 agent= initialize_agent( tools, llm, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, handle_parsing_errors=True, verbose = True) agent参数 agent参数 CHAT_ZERO_SHOT_REACT_DESCRIPTION中的CHAT部分，表示这是一个专门为Chat模型优化的代理。REACT部分表示一种组织Prompt的技术，能够最大化语言模型的推理能力。\nhandle_parsing_errors true表示当内容无法被正常解析时，会将错误内容传回语言模型，让它自行纠正。\n步骤4：向代理提问 数学问题：\n1 agent(\u0026#34;What is the 25% of 300?\u0026#34;) 百科问题：\n1 2 3 4 question = \u0026#34;Tom M. Mitchell is an American computer scientist \\ and the Founders University Professor at Carnegie Mellon University (CMU)\\ what book did he write?\u0026#34; result = agent(question) 从打印出来的中间步骤详细记录中，我们可以看到几个关键词，其表示的含义分别是：\n关键词 表示含义 Thought LLM在思考的内容 Action 执行特定的动作 Observation 从这个动作中观察到了什么 使用 Python 代理工具 类似于ChatGPT的代码解释器，Python 代理工具可以让语言模型编写并执行Python代码，然后将执行的结果返回给代理，让它决定下一步的操作。\n我们的任务目标是对一组客户名单按照姓氏和名字进行排序。\n步骤1：创建Python代理 1 2 3 4 5 agent = create_python_agent( llm, tool=PythonREPLTool(), verbose=True ) 步骤2：要求代理编写排序代码，并打印输出结果 1 2 3 4 5 6 7 8 9 10 11 12 customer_list = [[\u0026#34;Harrison\u0026#34;, \u0026#34;Chase\u0026#34;], [\u0026#34;Lang\u0026#34;, \u0026#34;Chain\u0026#34;], [\u0026#34;Dolly\u0026#34;, \u0026#34;Too\u0026#34;], [\u0026#34;Elle\u0026#34;, \u0026#34;Elem\u0026#34;], [\u0026#34;Geoff\u0026#34;,\u0026#34;Fusion\u0026#34;], [\u0026#34;Trance\u0026#34;,\u0026#34;Former\u0026#34;], [\u0026#34;Jen\u0026#34;,\u0026#34;Ayai\u0026#34;] ] agent.run(f\u0026#34;\u0026#34;\u0026#34;Sort these customers by \\ last name and then first name \\ and print the output: {customer_list}\u0026#34;\u0026#34;\u0026#34;) 使用自定义的工具 代理的一个优势就是你可以将其连接到你自己的信息来源、API、数据。\n步骤1：定义一个工具，用于获取当前日期 1 2 3 4 5 6 7 8 9 10 11 12 from langchain.agents import tool from datetime import date @tool def time(text: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Returns todays date, use this for any \\ questions related to knowing todays date. \\ The input should always be an empty string, \\ and this function will always return todays \\ date - any date mathmatics should occur \\ outside this function.\u0026#34;\u0026#34;\u0026#34; return str(date.today()) 除了函数名称，这里还写了一份详细的注释说明，代理会根据注释中的信息来判断何时应该调用、以及应该如何调用这个工具。\n步骤2：初始化代理，将自定义工具加到现有工具列表里 1 2 3 4 5 6 agent= initialize_agent( tools + [time], llm, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, handle_parsing_errors=True, verbose = True) 步骤3：调用代理，获取当前日期 1 2 3 4 try: result = agent(\u0026#34;whats the date today?\u0026#34;) except: print(\u0026#34;exception on external access\u0026#34;) ","date":"2023-08-29T20:44:47+08:00","permalink":"https://blog.importzhh.me/p/ii-%E5%9F%BA%E4%BA%8Elangchain%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E4%B8%8B/","title":"II: 基于LangChain的大语言模型应用开发(下)"},{"content":"继《面向开发者的ChatGPT提示工程》一课爆火之后，时隔一个月，吴恩达教授再次推出了另外三门免费的AI课程，今天要讲的就是其中联合了 LangChain 一起授课的——《基于LangChain的大语言模型应用开发》。\n这门课程将系统地介绍 LangChain 的常见组件及其应用，包括：\n模型，提示和输出解析（第1章节） 记忆（第2章节） 链（第3章节） 基于文档的问答（第4章节） 评估（第5章节） 代理（第6章节） 在线观看链接：https://www.deeplearning.ai/\n介绍 LangChain是什么？ 一个开源的、用于构建LLM应用的开发框架。\nLangChain做了什么工作？ 虽然通过提示（Prompt），可以加速开发LLM应用的进程，但过程中可能会产生很多胶水代码。\nLangChain所做的，就是把其中一些公共的部分抽象出来。\nLangChain有什么特点？ LangChain注重组合和模块化。\nLangChain拥有许多独立组件，可以单独使用，也可以与其他组件组合使用。\n通过将模块化的组件链式组合，可以构建一个更完整的应用程序。\n模型、提示与输出解析 先理清几个概念：\n概念 解释 模型 语言模型，用于生成文本。 提示 用于向模型传递信息。 解析器 接收模型的输出，并将其解析成更结构化的格式。 LangChain提供了一套简单的抽象，用于简化原先需要对模型反复提示与解析的操作。\n直接调用API vs 使用LangChain 使用LangChain访问ChatGPT，与直接使用OpenAI API有什么区别？让我们用一个例子来对比一下。\n任务是将文本按指定风格翻译成目标语言。\n直接调用API 步骤1：定义辅助函数，用于调用OpenAI API 1 2 3 4 5 6 7 8 def get_completion(prompt, model=\u0026#34;gpt-3.5-turbo\u0026#34;): messages = [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: prompt}] response = openai.ChatCompletion.create( model=model, messages=messages, temperature=0, ) return response.choices[0].message[\u0026#34;content\u0026#34;] 步骤2：定义待翻译文本与翻译风格 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 文本 customer_email = \u0026#34;\u0026#34;\u0026#34; Arrr, I be fuming that me blender lid \\ flew off and splattered me kitchen walls \\ with smoothie! And to make matters worse,\\ the warranty don\u0026#39;t cover the cost of \\ cleaning up me kitchen. I need yer help \\ right now, matey! \u0026#34;\u0026#34;\u0026#34; # 风格 style = \u0026#34;\u0026#34;\u0026#34;American English \\ in a calm and respectful tone \u0026#34;\u0026#34;\u0026#34; 步骤3：定义提示，用于指定目标语言，并以字符串插值的形式插入文本与风格 1 2 3 4 5 prompt = f\u0026#34;\u0026#34;\u0026#34;Translate the text \\ that is delimited by triple backticks into a style that is {style}. text: ```{customer_email}``` \u0026#34;\u0026#34;\u0026#34; 步骤4：调用函数并打印结果 1 2 response = get_completion(prompt) print(response) 使用LangChain 步骤1：创建ChatOpenAI实例 1 2 3 4 # 导入ChatOpenAI，这是LangChain对ChatGPT API访问的抽象 from langchain.chat_models import ChatOpenAI # 要控制 LLM 生成文本的随机性和创造性，请使用 temperature = 0.0 chat = ChatOpenAI(temperature=0.0) 步骤2：创建提示模板实例 1 2 3 4 5 6 7 8 9 # 模板字符串，用于指定目标语言，拥有两个输入变量，\u0026#34;style\u0026#34;和\u0026#34;text\u0026#34; template_string = \u0026#34;\u0026#34;\u0026#34;Translate the text \\ that is delimited by triple backticks \\ into a style that is {style}. \\ text: ```{text}``` \u0026#34;\u0026#34;\u0026#34; # 构建一个ChatPromptTemplate实例，用于模板的重用 from langchain.prompts import ChatPromptTemplate prompt_template = ChatPromptTemplate.from_template(template_string) 步骤3：定义翻译风格与待翻译文本，作为输入变量传入提示模板 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 风格 customer_style = \u0026#34;\u0026#34;\u0026#34;American English \\ in a calm and respectful tone \u0026#34;\u0026#34;\u0026#34; # 文本 customer_email = \u0026#34;\u0026#34;\u0026#34; Arrr, I be fuming that me blender lid \\ flew off and splattered me kitchen walls \\ with smoothie! And to make matters worse, \\ the warranty don\u0026#39;t cover the cost of \\ cleaning up me kitchen. I need yer help \\ right now, matey! \u0026#34;\u0026#34;\u0026#34; # 将风格和文本作为输入变量传入提示模板 customer_messages = prompt_template.format_messages( style=customer_style, text=customer_email) 步骤4：调用LLM翻译成指定风格，并打印结果 1 2 customer_response = chat(customer_messages) print(customer_response.content) 可以看出，使用LangChain访问ChatGPT，与直接使用OpenAI API相比，主要区别是：\n对ChatGPT API的访问做了抽象，简化调用 改用提示模板，而不是\u0026quot;f\u0026quot;字符串，方便重用 为什么要使用提示模板？ 随着构建应用程序的复杂度增加时，提示可能会需要更长且更详细。\n提示模板作为一种抽象，可以让我们适时重用提示。\n此外，LangChain还为一些常见操作提供了提示，如摘要、回答问题、连接到SQL数据库或连接到不同的API。\n通过使用LangChain的内置提示，可以快速地使应用程序运行，而无需设计自己的提示。\nLangChain输出解析器的作用 输出解析器可以提取模型输出中的特定字段，解析为更易于处理的格式。\n比如，解析为 Python 字典：\n步骤1：指定返回的JSON的格式规范 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from langchain.output_parsers import ResponseSchema from langchain.output_parsers import StructuredOutputParser # 礼物规范 gift_schema = ResponseSchema(name=\u0026#34;gift\u0026#34;, description=\u0026#34;Was the item purchased\\ as a gift for someone else? \\ Answer True if yes,\\ False if not or unknown.\u0026#34;) # 送货日期规范 delivery_days_schema = ResponseSchema(name=\u0026#34;delivery_days\u0026#34;, description=\u0026#34;How many days\\ did it take for the product\\ to arrive? If this \\ information is not found,\\ output -1.\u0026#34;) # 价格值规范 price_value_schema = ResponseSchema(name=\u0026#34;price_value\u0026#34;, description=\u0026#34;Extract any\\ sentences about the value or \\ price, and output them as a \\ comma separated Python list.\u0026#34;) 步骤2：创建解析器实例，获取格式指令 1 2 3 4 5 6 7 8 9 10 # 将格式规范放到一个列表里 response_schemas = [gift_schema, delivery_days_schema, price_value_schema] # 构建一个StructuredOutputParser实例 output_parser = StructuredOutputParser.from_response_schemas(response_schemas) # 获取将发送给LLM的格式指令 format_instructions = output_parser.get_format_instructions() print(format_instructions) 格式指令用于让LLM生成指定的内容格式，以便解析器可以解析，打印得到其内容如下：\n1 2 3 4 5 6 7 8 9 The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \u0026#34;\\`\\`\\`json\u0026#34; and \u0026#34;\\`\\`\\`\u0026#34;: ```json { \u0026#34;gift\u0026#34;: string // Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown. \u0026#34;delivery_days\u0026#34;: string // How many days did it take for the product to arrive? If this information is not found, output -1. \u0026#34;price_value\u0026#34;: string // Extract any sentences about the value or price, and output them as a comma separated Python list. } \\``` 步骤3：创建提示模板实例，将文本和格式指令作为输入变量传入 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 提示 review_template_2 = \u0026#34;\u0026#34;\u0026#34;\\ For the following text, extract the following information: gift: Was the item purchased as a gift for someone else? \\ Answer True if yes, False if not or unknown. delivery_days: How many days did it take for the product\\ to arrive? If this information is not found, output -1. price_value: Extract any sentences about the value or price,\\ and output them as a comma separated Python list. text: {text} {format_instructions} \u0026#34;\u0026#34;\u0026#34; # 构建一个ChatPromptTemplate实例，用于模板的重用 prompt = ChatPromptTemplate.from_template(template=review_template_2) # 将文本和格式指令作为输入变量传入 messages = prompt.format_messages(text=customer_review, format_instructions=format_instructions) 步骤4：调用LLM解析文本，并打印结果 1 2 response = chat(messages) print(response.content) 打印结果如下：\n1 2 3 4 5 6 7 ```json { \u0026#34;gift\u0026#34;: true, \u0026#34;delivery_days\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;price_value\u0026#34;: [\u0026#34;It\u0026#39;s slightly more expensive than the other leaf blowers out there, but I think it\u0026#39;s worth it for the extra features.\u0026#34;] } \\``` 步骤5：将结果解析为字典类型，并提取与送货天数相关联的值 1 2 output_dict = output_parser.parse(response.content) print(output_dict.get(\u0026#39;delivery_days\u0026#39;)) 提取到的值如下：\n1 \u0026#39;2\u0026#39; 记忆(Memory) 当我们与模型互动时，由于模型本身是无状态的，因此它通常无法记住之前对话的历史消息。\n每个请求交互，每次调用API都是独立的，这对于构建流畅的对话应用是个问题。\n为此，LangChain提供了多种记忆存储管理策略。\n策略 特点 ConversationBufferMemory 存储完整的对话历史 ConversationBufferWindowMemory 只保留最后几轮对话 ConversationalTokenBufferMemory 限制存储的令牌数量 ConversationSummaryBufferMemory 使用摘要存储对话历史 ConversationBufferMemory ConversationBufferMemory可用于临时存储完整的对话历史。\n例子如下：\n步骤1：创建对话链实例 1 2 3 4 5 6 7 8 9 10 11 from langchain.chat_models import ChatOpenAI from langchain.chains import ConversationChain from langchain.memory import ConversationBufferMemory llm = ChatOpenAI(temperature=0.0) memory = ConversationBufferMemory() conversation = ConversationChain( llm=llm, memory = memory, verbose=True ) 步骤2：使用\u0026quot;conversation.predict\u0026quot;函数进行对话 1 2 3 conversation.predict(input=\u0026#34;Hi, my name is Andrew\u0026#34;) conversation.predict(input=\u0026#34;What is 1+1?\u0026#34;) conversation.predict(input=\u0026#34;What is my name?\u0026#34;) 也可使用\u0026quot;memory.save_context\u0026quot;直接往存储里添加新内容\n1 2 memory.save_context({\u0026#34;input\u0026#34;: \u0026#34;Not much, just hanging\u0026#34;}, {\u0026#34;output\u0026#34;: \u0026#34;Cool\u0026#34;}) 由于我们把\u0026quot;verbose\u0026quot;变量改成\u0026quot;True，因此可以看到LangChain运行时的更多细节：\n\u0026gt; Entering new ConversationChain chain\u0026hellip;\nPrompt after formatting:\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\nCurrent conversation:\nHuman: Hi, my name is Andrew\nAI: Hello Andrew, it\u0026rsquo;s nice to meet you. My name is AI. How can I assist you today?\nHuman: What is 1+1?\nAI: The answer to 1+1 is 2.\nHuman: What is my name?\nAI:\n\u0026gt; Finished chain.\n\u0026lsquo;Your name is Andrew, as you mentioned earlier.\u0026rsquo;\n可以看到，记忆存储包含了到目前为止的所有对话消息，并用作LLM的输入或额外上下文。\n这样，它在生成输出时，就可以基于之前所说过的会话内容，再生成新的会话，让你感觉它好像“记得”你说过的话。\n步骤3：打印当前对话存储的所有历史消息 1 print(memory.buffer) Human: Hi, my name is Andrew\nAI: Hello Andrew, it\u0026rsquo;s nice to meet you. My name is AI. How can I assist you today?\nHuman: What is 1+1?\nAI: The answer to 1+1 is 2.\nHuman: What is my name?\nAI: Your name is Andrew, as you mentioned earlier.\n或者，\n1 print(memory.load_memory_variables({})) {\u0026lsquo;history\u0026rsquo;: \u0026ldquo;Human: Hi, my name is Andrew\\nAI: Hello Andrew, it\u0026rsquo;s nice to meet you. My name is AI. How can I assist you today?\\nHuman: What is 1+1?\\nAI: The answer to 1+1 is 2.\\nHuman: What is my name?\\nAI: Your name is Andrew, as you mentioned earlier.\u0026rdquo;}\n但随着对话的进行，记忆存储的大小会增加，发送Token的成本也会增加，为此，LangChain提供了另外几种策略。\nConversationBufferWindowMemory ConversationBufferWindowMemory只保留窗口记忆，也即只保留最后几轮对话。它有一个变量k，表示想记住最后几轮对话。\n比如，当k等于1时，表示仅记住最后一轮对话。\n例子如下：\n1 2 3 4 5 6 7 8 9 from langchain.memory import ConversationBufferWindowMemory llm = ChatOpenAI(temperature=0.0) memory = ConversationBufferWindowMemory(k=1) conversation = ConversationChain( llm=llm, memory = memory, verbose=False ) 我们可以在进行几轮对话之后，尝试让其回顾之前的对话内容：\n1 2 3 4 5 6 7 8 conversation.predict(input=\u0026#34;Hi, my name is Andrew\u0026#34;) # \u0026#34;Hello Andrew, it\u0026#39;s nice to meet you. My name is AI. How can I assist you today?\u0026#34; conversation.predict(input=\u0026#34;What is 1+1?\u0026#34;) # \u0026#39;The answer to 1+1 is 2.\u0026#39; conversation.predict(input=\u0026#34;What is my name?\u0026#34;) # \u0026#34;I\u0026#39;m sorry, I don\u0026#39;t have access to that information. Could you please tell me your name?\u0026#34; 这时我们会发现，由于窗口记忆的限制，它会丢失了前面有关名字的交流，从而无法说出我的名字。\n这个功能可以防止记忆存储量随着对话的进行而无限增长。当然在实际使用时，k不会设为1，而是会通常设置为一个较大的数字。\nConversationalTokenBufferMemory 很多LLM定价是基于Token的，Token调用的数量直接反映了LLM调用的成本。\n使用ConversationalTokenBufferMemory，可以限制保存在记忆存储的令牌数量。\n例子如下：\n1 2 3 4 5 6 7 from langchain.memory import ConversationTokenBufferMemory from langchain.llms import OpenAI llm = ChatOpenAI(temperature=0.0) # 指定LLM和Token限制值 memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30) 在插入一些消息之后，我们可以打印其实际保存的历史消息。\n1 2 3 4 5 6 7 8 9 10 # 插入一些消息 memory.save_context({\u0026#34;input\u0026#34;: \u0026#34;AI is what?!\u0026#34;}, {\u0026#34;output\u0026#34;: \u0026#34;Amazing!\u0026#34;}) memory.save_context({\u0026#34;input\u0026#34;: \u0026#34;Backpropagation is what?\u0026#34;}, {\u0026#34;output\u0026#34;: \u0026#34;Beautiful!\u0026#34;}) memory.save_context({\u0026#34;input\u0026#34;: \u0026#34;Chatbots are what?\u0026#34;}, {\u0026#34;output\u0026#34;: \u0026#34;Charming!\u0026#34;}) # 打印历史消息 memory.load_memory_variables({}) 打印内容如下：\n{\u0026lsquo;history\u0026rsquo;: \u0026lsquo;AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!\u0026rsquo;}\n我们会发现，当把Token限制值调得比较高时，它几乎可以包含整个对话。\n而如果减少值，它会删掉对话最早的那部分消息，只保留最近对话的消息，并且保证总的消息内容长度不超过Token限制值。\n另外，之所以还要指定一个LLM参数，是因为不同的LLM使用不同的Token计算方式。\n这里是告诉它，使用ChatOpenAI LLM使用的计算Token的方法。\nConversationSummaryBufferMemory ConversationSummaryBufferMemory试图将消息的显性记忆，保持在我们设定的Token限制值之下，也即\n当Token限制值能覆盖文本长度时，会存储整个对话历史。 而当Token限制值小于文本长度时，则会为所有历史消息生成摘要，改在记忆中存储历史消息的摘要。 以情况2为例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from langchain.memory import ConversationSummaryBufferMemory # 创建一个长字符串 schedule = \u0026#34;There is a meeting at 8am with your product team. \\ You will need your powerpoint presentation prepared. \\ 9am-12pm have time to work on your LangChain \\ project which will go quickly because Langchain is such a powerful tool. \\ At Noon, lunch at the italian resturant with a customer who is driving \\ from over an hour away to meet you to understand the latest in AI. \\ Be sure to bring your laptop to show the latest LLM demo.\u0026#34; memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100) memory.save_context({\u0026#34;input\u0026#34;: \u0026#34;Hello\u0026#34;}, {\u0026#34;output\u0026#34;: \u0026#34;What\u0026#39;s up\u0026#34;}) memory.save_context({\u0026#34;input\u0026#34;: \u0026#34;Not much, just hanging\u0026#34;}, {\u0026#34;output\u0026#34;: \u0026#34;Cool\u0026#34;}) memory.save_context({\u0026#34;input\u0026#34;: \u0026#34;What is on the schedule today?\u0026#34;}, {\u0026#34;output\u0026#34;: f\u0026#34;{schedule}\u0026#34;}) conversation = ConversationChain( llm=llm, memory = memory, verbose=True ) conversation.predict(input=\u0026#34;What would be a good demo to show?\u0026#34;) 运行的细节如下：\n\u0026gt; Entering new ConversationChain chain\u0026hellip;\nPrompt after formatting:\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\nCurrent conversation:\nSystem: The human and AI engage in small talk before discussing the day\u0026rsquo;s schedule. The AI informs the human of a morning meeting with the product team, time to work on the LangChain project, and a lunch meeting with a customer interested in the latest AI developments.\nHuman: What would be a good demo to show?\nAI:\n\u0026gt; Finished chain.\n\u0026ldquo;Based on the customer\u0026rsquo;s interest in AI developments, I would suggest showcasing our latest natural language processing capabilities. We could demonstrate how our AI can accurately understand and respond to complex language queries, and even provide personalized recommendations based on the user\u0026rsquo;s preferences. Additionally, we could highlight our AI\u0026rsquo;s ability to learn and adapt over time, making it a valuable tool for businesses looking to improve their customer experience.\u0026rdquo;\n可以看到，由于超过了设定的Token限制值，它为历史会话的生成了一个摘要，并放在系统消息的提示词中。\n其他记忆存储管理策略 策略 特点 向量数据存储（VectorDataMemory） 存储嵌入向量，用于检索相关文本块 实体记忆存储（EntityMemories） 记住特定实体的详细信息，比如对话中某个重要人物的信息 除了这些记忆存储类型，也可将整个对话存储在传统数据库中，如键值存储（key-value store）或SQL数据库。\n这样就可以回顾整个对话，进行审计或进一步改进系统。\n链 链（Chain）是LangChain中最关键的构建模块。\n除了将 LLM 与提示结合在一起，还可以通过组合多个链，对文本或其他数据执行一系列的操作。\nLangChain提供了多种可用的链类型：\n类型 场景 LLM链（LLMChain） 将LLM和提示结合在一起 简单顺序链（SimpleSequentialChain） 只需要一个输入并且只返回一个输出 常规顺序链（SequentialChain） 有多个输入或多个输出 路由链（RouterChain） 根据输入的具体内容路由到不同的子链 下面让我们来逐一了解一下。\n首先，用 pandas DataFrame 加载一些稍后要用到的数据。\n1 2 3 4 import pandas as pd df = pd.read_csv(\u0026#39;Data.csv\u0026#39;) df.head() LLM链（LLMChain） LLM链是一个简单但非常强大的链，它是我们后面要讨论的其他链类型的基础，用于将LLM和提示结合在一起。\n例子如下：\n步骤1：初始化语言模型和提示 1 2 3 4 5 6 7 8 9 10 11 from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.chains import LLMChain # 用一个比较高的temperature值以获得一些更有意思的结果 llm = ChatOpenAI(temperature=0.9) # 接收一个名为“product”的变量，要求LLM生成描述生产该产品的公司的最佳名称 prompt = ChatPromptTemplate.from_template( \u0026#34;What is the best name to describe \\ a company that makes {product}?\u0026#34; ) 步骤2：将产品传入链中，并运行链 1 2 3 chain = LLMChain(llm=llm, prompt=prompt) product = \u0026#34;Queen Size Sheet Set\u0026#34; chain.run(product) 运行链后，它将在后台格式化提示词，然后将格式化后的完整提示词传递给LLM，然后得到结果：\n\u0026lsquo;Royal Beddings.\u0026rsquo;\n简单顺序链（SimpleSequentialChain） 当我们的子链只需要一个输入并且只返回一个输出时，简单顺序链很有效。\n例子如下：\n步骤1：初始化语言模型 1 2 3 from langchain.chains import SimpleSequentialChain llm = ChatOpenAI(temperature=0.9) 步骤2：创建第一个链 1 2 3 4 5 6 7 8 # 提示模板1：接受产品并返回描述该公司的最佳名称 first_prompt = ChatPromptTemplate.from_template( \u0026#34;What is the best name to describe \\ a company that makes {product}?\u0026#34; ) # 第一个链 chain_one = LLMChain(llm=llm, prompt=first_prompt) 步骤3：创建第二个链 1 2 3 4 5 6 7 # 提示模板2：获取公司名称，然后输出该公司的 20 字描述 second_prompt = ChatPromptTemplate.from_template( \u0026#34;Write a 20 words description for the following \\ company:{company_name}\u0026#34; ) # 第二个链 chain_two = LLMChain(llm=llm, prompt=second_prompt) 步骤4：创建简单顺序链实例，并运行链 1 2 3 4 5 # 第一个链的输出将传递到第二个链 overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True ) overall_simple_chain.run(product) 运行的细节如下：\n\u0026gt; Entering new SimpleSequentialChain chain\u0026hellip;\n\u0026ldquo;Royal Bedding Co.\u0026rdquo;\n\u0026ldquo;Royal Bedding Co. offers luxurious and comfortable bedding solutions for a restful and regal sleep experience fit for royalty.\u0026rdquo;\n\u0026gt; Finished chain.\n可以看到，它首先输出公司名称，然后将其传递到第二条链，并给出该公司可能的业务描述。\n常规顺序链（SequentialChain） 当有多个输入或多个输出时，可以使用常规顺序链。\n例子如下：\n步骤1：初始化语言模型 1 2 3 from langchain.chains import SequentialChain llm = ChatOpenAI(temperature=0.9) 步骤2：创建一堆将依次使用的链 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # 第一条链，将评论翻译成英语。 first_prompt = ChatPromptTemplate.from_template( \u0026#34;Translate the following review to english:\u0026#34; \u0026#34;\\n\\n{Review}\u0026#34; ) chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\u0026#34;English_Review\u0026#34; ) # 第二条链，用一句话总结该评论 second_prompt = ChatPromptTemplate.from_template( \u0026#34;Can you summarize the following review in 1 sentence:\u0026#34; \u0026#34;\\n\\n{English_Review}\u0026#34; ) chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\u0026#34;summary\u0026#34; ) # 第三条链，检测原始评论的语言 third_prompt = ChatPromptTemplate.from_template( \u0026#34;What language is the following review:\\n\\n{Review}\u0026#34; ) chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\u0026#34;language\u0026#34; ) # 第四条链，接收第二条链的摘要内容(\u0026#34;summary\u0026#34;变量)，以及第三条链的语言类别(\u0026#34;language\u0026#34;变量)，要求后续回复摘要内容时使用指定语言。 fourth_prompt = ChatPromptTemplate.from_template( \u0026#34;Write a follow up response to the following \u0026#34; \u0026#34;summary in the specified language:\u0026#34; \u0026#34;\\n\\nSummary: {summary}\\n\\nLanguage: {language}\u0026#34; ) chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key=\u0026#34;followup_message\u0026#34; ) 常规顺序链中的任何一个步骤，都可以接收来自上游的多个输入变量，特别当你有复杂的下游链需要和多个上游链组合时，这会非常有用。\n步骤3：将这些链组合在顺序链中，并指定输入与输出变量 1 2 3 4 5 6 overall_chain = SequentialChain( chains=[chain_one, chain_two, chain_three, chain_four], input_variables=[\u0026#34;Review\u0026#34;], output_variables=[\u0026#34;English_Review\u0026#34;, \u0026#34;summary\u0026#34;,\u0026#34;followup_message\u0026#34;], verbose=True ) 让这些变量名称准确排列非常重要，因为有很多不同的输入和输出。如果你遇到任何问题，请检查它们排列顺序是否正确。\n步骤4：选择一条评论并将其传递到整个链中 1 2 review = df.Review[5] overall_chain(review) 执行的细节如下：\n\u0026gt; Entering new SequentialChain chain\u0026hellip;\n\u0026gt; Finished chain.\n{\u0026lsquo;Review\u0026rsquo;: \u0026ldquo;Je trouve le goût médiocre. La mousse ne tient pas, c\u0026rsquo;est bizarre. J\u0026rsquo;achète les mêmes dans le commerce et le goût est bien meilleur\u0026hellip;\\nVieux lot ou contrefaçon !?\u0026rdquo;,\n\u0026lsquo;English_Review\u0026rsquo;: \u0026ldquo;I find the taste mediocre. The foam doesn\u0026rsquo;t hold up, it\u0026rsquo;s weird. I buy the same ones in stores and the taste is much better\u0026hellip; Old batch or counterfeit!?\u0026rdquo;,\n\u0026lsquo;summary\u0026rsquo;: \u0026lsquo;The reviewer expresses dissatisfaction with the taste and foam of the product, suspecting that it might be an old batch or counterfeit.\u0026rsquo;,\n\u0026lsquo;followup_message\u0026rsquo;: \u0026ldquo;Réponse : Le critique exprime sa déception quant au goût et à la mousse du produit, soupçonnant qu\u0026rsquo;il s\u0026rsquo;agit peut-être d\u0026rsquo;un lot périmé ou contrefait. Il est important que les fabricants prennent des mesures pour garantir la qualité de leurs produits afin de maintenir la confiance de leurs clients. Nous espérons que ce problème sera rapidement résolu pour que les consommateurs puissent profiter du produit tel qu\u0026rsquo;il est censé être.\u0026rdquo;}\n可以看到，其最终采用了检测到的原始语言——法语对摘要内容进行了回复。\n路由链（RouterChain） 如果你有多个子链，且每个子链专门负责处理某种特定类型的输入，这种情况就可以使用路由链。\n路由链会根据输入的具体内容路由到不同的子链。\n它会首先判断该使用哪个子链，然后将输入传递到相应的子链。\n例子如下：\n步骤1：提供多个特定类型的提示模板 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # 第一个提示，适合回答物理问题 physics_template = \u0026#34;\u0026#34;\u0026#34;You are a very smart physics professor. \\ You are great at answering questions about physics in a concise\\ and easy to understand manner. \\ When you don\u0026#39;t know the answer to a question you admit\\ that you don\u0026#39;t know. Here is a question: {input}\u0026#34;\u0026#34;\u0026#34; # 第二个提示，适合回答数学问题 math_template = \u0026#34;\u0026#34;\u0026#34;You are a very good mathematician. \\ You are great at answering math questions. \\ You are so good because you are able to break down \\ hard problems into their component parts, answer the component parts, and then put them together\\ to answer the broader question. Here is a question: {input}\u0026#34;\u0026#34;\u0026#34; # 第三个提示，适合回答历史问题 history_template = \u0026#34;\u0026#34;\u0026#34;You are a very good historian. \\ You have an excellent knowledge of and understanding of people,\\ events and contexts from a range of historical periods. \\ You have the ability to think, reflect, debate, discuss and \\ evaluate the past. You have a respect for historical evidence\\ and the ability to make use of it to support your explanations \\ and judgements. Here is a question: {input}\u0026#34;\u0026#34;\u0026#34; # 第四个提示,适合回答计算机科学问题。 computerscience_template = \u0026#34;\u0026#34;\u0026#34; You are a successful computer scientist.\\ You have a passion for creativity, collaboration,\\ forward-thinking, confidence, strong problem-solving capabilities,\\ understanding of theories and algorithms, and excellent communication \\ skills. You are great at answering coding questions. \\ You are so good because you know how to solve a problem by \\ describing the solution in imperative steps \\ that a machine can easily interpret and you know how to \\ choose a solution that has a good balance between \\ time complexity and space complexity. Here is a question: {input}\u0026#34;\u0026#34;\u0026#34; 步骤2：为每个提示模板提供更多相关信息 这些信息将传递给路由链，以帮助路由链决定何时使用哪条子链。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 prompt_infos = [ { \u0026#34;name\u0026#34;: \u0026#34;physics\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Good for answering questions about physics\u0026#34;, \u0026#34;prompt_template\u0026#34;: physics_template }, { \u0026#34;name\u0026#34;: \u0026#34;math\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Good for answering math questions\u0026#34;, \u0026#34;prompt_template\u0026#34;: math_template }, { \u0026#34;name\u0026#34;: \u0026#34;History\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Good for answering history questions\u0026#34;, \u0026#34;prompt_template\u0026#34;: history_template }, { \u0026#34;name\u0026#34;: \u0026#34;computer science\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Good for answering computer science questions\u0026#34;, \u0026#34;prompt_template\u0026#34;: computerscience_template } ] 步骤3：导入需要的链类型，定义使用的语言模型 MultiPromptChain是一种特定类型的链，用于在多个不同提示模板之间进行路由。\nLLMRouterChain会借助语言模型的帮助，让语言模型根据上面提供的名称和描述等信息，判断如何路由。\nRouterOutputParser将LLM输出解析成一个字典，根据字典内容确定下游使用哪条链，以及链的输入应该是什么。\n1 2 3 4 5 from langchain.chains.router import MultiPromptChain from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser from langchain.prompts import PromptTemplate llm = ChatOpenAI(temperature=0) 步骤4：创建目标链 路由链会根据输入内容调用这些目标链的其中一个。\n1 2 3 4 5 6 7 8 9 10 destination_chains = {} for p_info in prompt_infos: name = p_info[\u0026#34;name\u0026#34;] prompt_template = p_info[\u0026#34;prompt_template\u0026#34;] prompt = ChatPromptTemplate.from_template(template=prompt_template) chain = LLMChain(llm=llm, prompt=prompt) destination_chains[name] = chain destinations = [f\u0026#34;{p[\u0026#39;name\u0026#39;]}: {p[\u0026#39;description\u0026#39;]}\u0026#34; for p in prompt_infos] destinations_str = \u0026#34;\\n\u0026#34;.join(destinations) 步骤5：创建默认链 默认链是在路由找不到合适的子链调用时，用来备用的一条链路。\n1 2 default_prompt = ChatPromptTemplate.from_template(\u0026#34;{input}\u0026#34;) default_chain = LLMChain(llm=llm, prompt=default_prompt) 步骤6：定义一个路由提示模板 LLM 会根据提示词的内容在不同链之间路由。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 MULTI_PROMPT_ROUTER_TEMPLATE = \u0026#34;\u0026#34;\u0026#34;Given a raw text input to a \\ language model select the model prompt best suited for the input. \\ You will be given the names of the available prompts and a \\ description of what the prompt is best suited for. \\ You may also revise the original input if you think that revising\\ it will ultimately lead to a better response from the language model. \u0026lt;\u0026lt; FORMATTING \u0026gt;\u0026gt; Return a markdown code snippet with a JSON object formatted to look like: \\```json {{{{ \u0026#34;destination\u0026#34;: string \\ name of the prompt to use or \u0026#34;DEFAULT\u0026#34; \u0026#34;next_inputs\u0026#34;: string \\ a potentially modified version of the original input }}}} \\``` REMEMBER: \u0026#34;destination\u0026#34; MUST be one of the candidate prompt \\ names specified below OR it can be \u0026#34;DEFAULT\u0026#34; if the input is not\\ well suited for any of the candidate prompts. REMEMBER: \u0026#34;next_inputs\u0026#34; can just be the original input \\ if you don\u0026#39;t think any modifications are needed. \u0026lt;\u0026lt; CANDIDATE PROMPTS \u0026gt;\u0026gt; {destinations} \u0026lt;\u0026lt; INPUT \u0026gt;\u0026gt; {{input}} \u0026lt;\u0026lt; OUTPUT (remember to include the ```json)\u0026gt;\u0026gt;\u0026#34;\u0026#34;\u0026#34; 步骤7：组合语言模型、路由提示模板，构成路由链 1 2 3 4 5 6 7 8 9 10 router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format( destinations=destinations_str ) router_prompt = PromptTemplate( template=router_template, input_variables=[\u0026#34;input\u0026#34;], output_parser=RouterOutputParser(), ) router_chain = LLMRouterChain.from_llm(llm, router_prompt) 步骤8：组合路由链、目标链和默认链，创建整条链 1 2 3 4 chain = MultiPromptChain(router_chain=router_chain, destination_chains=destination_chains, default_chain=default_chain, verbose=True ) 步骤9：提问不同类型的问题 1 2 # 物理问题 chain.run(\u0026#34;What is black body radiation?\u0026#34;) 输出如下：\n\u0026gt; Entering new MultiPromptChain chain\u0026hellip;\nphysics: {\u0026lsquo;input\u0026rsquo;: \u0026lsquo;What is black body radiation?\u0026rsquo;}\n\u0026gt; Finished chain.\n\u0026ldquo;Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an object that absorbs all radiation that falls on it and emits radiation at all wavelengths. The radiation emitted by a black body depends only on its temperature and follows a specific distribution known as Planck\u0026rsquo;s law. This type of radiation is important in understanding the behavior of stars, as well as in the development of technologies such as incandescent light bulbs and infrared cameras.\u0026rdquo;\n1 2 # 数学问题 chain.run(\u0026#34;what is 2 + 2\u0026#34;) 输出如下：\n\u0026gt; Entering new MultiPromptChain chain\u0026hellip;\nmath: {\u0026lsquo;input\u0026rsquo;: \u0026lsquo;what is 2 + 2\u0026rsquo;}\n\u0026gt; Finished chain.\n\u0026lsquo;As an AI language model, I can answer this question easily. The answer to 2 + 2 is 4.\u0026rsquo;\n1 2 # 生物问题，无匹配，走默认链 chain.run(\u0026#34;Why does every cell in our body contain DNA?\u0026#34;) \u0026gt; Entering new MultiPromptChain chain\u0026hellip;\nNone: {\u0026lsquo;input\u0026rsquo;: \u0026lsquo;Why does every cell in our body contain DNA?\u0026rsquo;}\n\u0026gt; Finished chain.\n\u0026lsquo;Every cell in our body contains DNA because DNA carries the genetic information that determines the characteristics and functions of each cell. DNA contains the instructions for the synthesis of proteins, which are essential for the structure and function of cells. Additionally, DNA is responsible for the transmission of genetic information from one generation to the next. Therefore, every cell in our body needs DNA to carry out its specific functions and to maintain the integrity of the organism as a whole.\u0026rsquo;\n相关链接 langchain中文社区 https://www.langchain.cn/\nlangchain教程 https://python.langchain.com/docs/get_started/introduction.html\n","date":"2023-08-29T12:44:47+08:00","permalink":"https://blog.importzhh.me/p/i-%E5%9F%BA%E4%BA%8Elangchain%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E4%B8%8A/","title":"I: 基于LangChain的大语言模型应用开发(上)"},{"content":"前言 转载自b站建议结合原视频动画看，文字版主要节约再次观看时间 向量 这里有一些狗 熟悉犬类的朋友 应该能很快的区分出它们的品种\n我们之所以能做到这一点\n是因为我们会从不同的角度来观察它们的特征 比如体型的大小\n如果我们用一个坐标轴来表示这个特征 这些狗将落在不同的坐标\n然而单纯依靠体型这一个特征还不够 比如金毛和拉布拉多的体型就十分的接近\n所以需要继续观察其他的特征 比如毛发的长短 我们再建立一个坐标轴 这样就区分开了金毛和拉布拉多\n现在每只狗对应一个二维坐标点\n但这仍然不能够很好地区分 德国牧羊犬和挪威纳犬 因为无论是体型还是毛发的长短 他们都十分的接近\n所以我们需要继续从更多的角度来观察 比如鼻子的长短 再建立一个坐标轴 于是这两个全轴也被区分开来 现在每只狗对应一个三维的坐标点\n我们还可以有更多的角度 比如眼睛的大小\n虽然作为三维生物 我们不可能在四维空间作图 但在坐标点的数值上却很容易实现 这一点 继续向后追加就好\n现在每只狗对应一个四维的坐标点\n如果我们从更多的角度或者说维度 来观察一只狗的特征 比如腿的粗 毛发的卷曲 甚至是一些抽象的角度\n比如服从性 攻击性等等等等 我们使用的维度越多 对狗的区分能力就越强 同时坐标点所在的空间维度也就越高\n不仅是狗 实际上几乎所有的事情都可以被这样表达\n可以是具象的山河日月鸟兽鱼虫 也可以是抽象的喜怒哀乐 悲欢离合\n不同事物在不同的特征维度中有着不同的表现 或者说不同的数值\n所以最终都会在在一个高维的特征空间中 对应着一个坐标点 只不过在这些更大的范围内 我们可能需要更高的特征维度\n才能很好的对进行事物区分 可能几百几千乃至上万\n但由于我们无法做出超过三维的图 所以接下来我们都是用二维坐标来讲解 你会发现这种表达事物的方式 有着很多美妙的特性\n那些概念上更为接近的点 在空间中更为聚集\n而那些概念上非常不同的点则距离很远\n更进一步 如果以坐标原点为起点 这些坐标点为终点\n我们知道 这就是我们熟悉的带有方向和大小的向量\n而从向量的角度 这种表达方式甚至有了一定的推理能力\n比如警察的向量减去小偷的向量得到的结果向和猫的向量减去老鼠的结果向量十分的相似 这意味着猫和老鼠的关系 类似于警察和小偷的关系\n用向量化的数据来表达概念 不得不说是一个非常好的想法\n如果我们对图片进行向量化 那么就可以通过搜索相似的向量 实现图搜图的功能 如果对视频进行向量的话 就可以通过搜索相似的向量 实现相关视频的推荐\n如果对商品进行向量的话 就可以通过搜索相似的向量 针对用户当前浏览的商品进行相关推荐\n而如果对一个文本进行向量化 就能在一个智能问答系统 从根据用户当前的问题 找到一些已经解决过的相似的问题 以供参考\n词向量 说到文本类的数据 近些年来对词汇进行向量化 在自然语言处理领域得到了巨大意义\n也就是所谓的词向量\n实际上语言作为概念的符号\n你会发现一个训练恰当的词向量集合 将和其所指代的事物之间的向量集合 十分的接近\n这很利于发现自然语言中所蕴含的实际概念\n除了词向量以外\n最近随着以chatgpt为代表的大语言模型的如火如荼\n人们又发现了向量数据的一些妙用\n如果我们把chatgpt的对话内容进行向量化 便可以用当前的对话\n搜索到历史中最为相似的一些对话 也就是找到和当前对话最相关的记忆\n而把这些最相关的记忆提示给模型 将极大地提高其输出的效果\n向量数据库 最近几年 一种叫做向量数据库的产品 正趁着ai的热潮开始崭露头角\n向量数据库的倡导者和创业者们 正是基于这样的一种设想 开始了他们的视野\n伴随着大ai时代的到来 向量将成为一种重要的数据形式\n而传统的数据库并不适合用来存储和检索 向量数据\n因此我们需要一种专门设计的数据库来处理这些问题\n这或许会成为未来数据层面的基础设施\n和存储数据表 然后用查询语句进行精准搜索的传统数据库不同\n向量数据库存储的是向量数据 而查询过程则是从库中搜索出和查询向量最为相似的一些向量 具有一定的模糊性\n最近邻搜索算法 通过以上这些例子不难看出\n向量数据的一个主要应用场景就是给定一个查询向量 然后从众多的向量中找到最为相似的一些\n这就是所谓的最近邻问题\n而能实现这一点的则称之为最近邻搜索算法\n一种最容易想到的方法可能就是暴力搜索 就是一路直接平推过去 依次比较所有向量和查询向量的相似度 挑选出相似度最高的几个\n比较两个向量的相似度的具体方法有许多\n两个向量的夹角越小越相似 所以可以通过计算向量夹角的余弦值来判断\n再比如直接计算两个向量的欧式距离 距离越近越相似\n显然如果库中的向量过多 这种毫无技术含量的暴力方法 将导致极高的搜索时间\n但这种方法也有着其他方法 永远无法到达的一个好处 它的搜索质量是完美的\n因为它真的比较了每一个向量 所以如果库中数据规模较小 便你全部销量的时间可以接受\n这也不失为一种好的方法 然而实际应用中的数据规模往往都不会太小\n所以我们必须找出一些方法进行优化 有一种朴素的想法\n指出了优化的大致思路 我们可以用一个寻人的例子来说明\n假如已经知道照片上的这个人在a城市 现在请你想一个办法把他找出来\n就相当于去找a城市的每一个人逐一比较 这样一定能够找到\n但要花费海量的时间 但我们通过他胸前的红领巾 就知道他是一名小学生\n所以就可以把寻找的范围 缩小到a城市的所有小学 如此就有可能将千万级别的查找次数\n降到只有几万的级别 所以对于向量的搜索问题\n假如能够为查询向量先划定一个大致的范围再搜索岂不美哉\n有一种称之为聚类的算法可以实现这一点\n我们以最为流行的k-means聚类算法为例\n很简单我们先选定一个想要分类的数量 比如四类 然后随机生成四个点 称之为聚类中心点 然后这些向量和哪一个中心点 最近就被分为哪一类 再然后用当前被分为一类的向量 计算出一个平均的向量点 把对应的中心点的位置更新为这个平均点\n再判断每个向量点和哪一个中心点最近 重新分类 然后继续用同一类的向量点计算出一个平均点 把中心点更新过去 再次重新分类\n如此反复 这个不断迭代的过程就称之为训练\n最后这些中心点将趋于稳定或者说收敛 最终将这些向量分成了四类\n如此在搜索的时候 只需要找出和查询向量最近的那个聚类中心\n搜索这个分类中的向量即可 也就实现了缩小搜索范围的目的\n当然聚类的方法并不能保证 不会出现遗漏的问题\n比如查询向量在这里\n他和这个分类的中心最近 但和它最近的向量 其实是在这个分类中\n有一些缓解这个问题的办法 比如我们增加聚类的数量 同时指定搜索多个最近的区域 减少遗漏\n但只要是试图提高搜索的质量 基本上都会增加搜索的时间\n实际上速度和质量往往是一对难以调和的矛盾 而几乎所有的算法都是在这两个指标上\n结合实际情况衡量的结果 所以现实就是 除了暴力搜索一定能够找到最近邻的一些向量以外 其他任何方法都不能保证这一点 而只能得到一些近似的结果 所以其实这些算法一般也被称之为近似最近邻算法\n除了聚类以外 减少搜索范围的方式还有很多\n这里再介绍一种比较流行的基于哈希的方法 我们曾在md 的视频中介绍过哈希的概念 简单来说就是任何数据经过哈希函数计算之后 都会输出一个固定长度的哈希值 比如128位 而且由于输入是任意的数据 而输出是固定长度的数 以有穷对无穷\n根据鸽巢原理 必然会出现数据不同 但哈希值相同的情况 这也被称之为碰撞\n通常情况下 哈希函数的设计一般力求减少这种碰撞的发生 但这里所构建的哈希函数却反其道而行\n它力求增大发生碰撞的可能\n因为哈希碰撞正是分组的依据 哈希值一样的向量被分到同一组 这些分组也被称之为桶 除了容易发生碰撞以外 这个哈希函数还要具备这样的一个特性\n位置越近 或者说越相似的向量发生碰撞的概率越高 被分到同一个桶中的可能性越大\n如此在查询的时候 只需要计算一下查询向量的哈希值 找到其所在的桶\n再在这个桶中搜索就好了 因为和查询向量最相似的一些销量\n大概率都在这个图 我们把具有这种特性的哈希函数 称之为位置敏感的哈希函数\n这样的哈希函数怎么实现呢 我们来看一种常用的手法 我们以a b c 个向量为例 首先随机生成一条直线 而且这条线区分正反两侧 比如这边是正的一侧 这边是反的一侧 如果一个向量在这条线的正的一侧 那么他就是一 如果再反的一侧 那么就是零\n然后再随机生成一条直线\n同样正侧的向量是一 反侧的向量是零 如此这般我们随机的生成若干个这样的直线\n每次都根据所在的正反侧得到一或者零 如果一共使用四条随机的直线\n如此就为每个向量算出了四个零或者一 各自得到一个四位的二进制编码 现在我们来观察一下 这三个二进制编码的相似程度\n很明显 ac这两个更近的向量的编码更为相似\n只有第三位不同 相似度高达3/4 而这个较远的b和ac的相似度都很低 一个是1/4 一个是零 所以为什么向量越接近 得到的二进制编码就越相似呢\n我们直观的来定性分析一下 在计算这四个二进制编码的某一位的时候\n如果要让b和c的结果一样 那么这条线应该是这样的\n当然也可以是这样\n但不论如何 一定要从ac和ab之间穿过去 同样如果要让a和c的结果一样 这条线应该从bc和ab之间穿过 我们忽略共同穿越的AB 直观上来看 b和c之间的宽度 要远远大于a和c之间的宽度\n而我们的直线是随机生成的 所以从概率上来说 生成的直线从bc穿过的可能性\n要比从ac穿过的要更大 所以在生成二进制编码的过程中 c更有可能和a一样而不是b 同样如果要让a和b的编码值一样 直线要穿过ac和bc 而如果要让a和c的结果一样\n这条线应该从bc和ab之间穿过 忽略共同穿越的bc ac间的宽度也远小ab\n所以a也是更可能和c一样 而不是b\n所以这串二进制编码便可以作为向量的哈希值 而这个生成的过程 就是一种我们所找寻的哈希函数 因为它满足我们刚才说的两个要求 容易碰撞 而且越相似的向量越容易碰撞 以至于被分到同一个桶中\n当然这三个向量的哈希值并不完全相同 但如果我们再观察一个距离a更近的d\n你就会发现d最后的二进制编码 或者说哈希值和a一样发生的碰撞\n我们用四个向量 展示了这个方法的基本工作原理 而对于真实情况中的许多向量而言\n每个向量通过这个哈希函数后 都会得到一串二进制编码的哈希值 而那些非常接近的\n像那样的哈希值大概率是一样的 也就被分到了同一个桶中 对于更高维度的向量 道理也是一样的\n比如三维 便可以使用三维空间中的一个随机平面 来做哈希函数的计算 这个面也有正反正面的向量得到一 反面的向量得到零 若干个随机平面\n自然也就得到了一串二进制编码的哈希值 如果是更高的维度 虽然我们做不出图形\n但却可以理解 在这些更高维度中也存在着包围的超平面 同样也可以完成这样的哈希值生成\n所以我们把这种方法称之为 随机超平面或者随机投影 当然正如我们所言\n除了直接暴搜 任何试图减少搜索量的方法 方法都会在一定程度上降低搜索的质量\n比如还是这四个向量 现在我们用九条随机直线 每个向量生成长度为九的二进制哈希编码 可能是这样的过程 ad两个比较近的点的哈希值一致 被分到了同一个桶中 这是比较理想的结果 但因为直线是随机的 我们把握不住 所以这个过程也有可能是这样的 在生成第六个编码的时候 这个直线真的就随机到了 从ad之间穿过 导致a和d的哈希编码的第六位不同 虽然我们说概率比较小 但毕竟是有可能发生的 最后a和d无法进入同一个痛 所以一般会采用一种分段的措施 来改善这种情况 比如现在我们把这些二进制哈希值分成三段 然后独立的对这些片段进行分头 如此 虽然d的第二个片段被分到了不同的桶中 但第个片段却被分到了同一个桶 我们可以采用只要匹配一个片段\n就将其作为候选项的策略 合理的扩充更多的搜索范围 你会发现在这种分段的措施下 a和c的第二段也被分到了同一个桶中 两者由此成为了相似向量的候选项 因为他们比较接近 这是一种合理的扩充 而b则没有任何一个段和其他向量对应的段\n被分到同一个桶中 因为它们距离很远 这很合理\n在上集中，我们简单介绍了一些提升搜索速度的方法，通过减少搜索范围来实现。但是对于海量的向量数据来说，除了搜索速度，内存开销也是一个巨大的挑战。\n内存开销问题 举个例子，假设向量的维度是128，每个维度的值是一个32位浮点数，那么一个向量占用的内存就是512字节。如果数据库中有1000万个向量，总共占用的内存就是大约4.77GB。在实际应用中，上千维甚至上亿维的向量数量并不罕见，所以内存开销问题是非常严重的。\n有损压缩方法 乘积量化 Product Quantization 每个向量都对应着一个有用的记录，所以无法通过删除向量来节省内存。唯一的选择就是降低每个向量本身的大小。 一种方法是使用k-means聚类算法，将相邻且有一定聚集性的向量分为一类，并用聚类中心（也叫质心）来代替该类中的其他向量。这种方法实际上是一种有损压缩的方法。 图像压缩的例子 举个图片的例子来说明，图片由像素点组成，每个像素点有RGB三个数值，可以将一个像素点看作是一个三维空间中的向量点。 通过聚类算法，在像素空间中将所有像素点替换为所在类别的质心点，可以实现图片的压缩。虽然质量有所下降，但仍然保留了原图的样貌。\n向量量化 类似地，对于向量数据，可以使用向量量化的方法来降低内存开销。将每个向量用编码值来表示，然后将编码值和对应的质心记录下来，形成一个码本。 每次使用某个向量时，通过编码值从码本中找到对应的质心，再恢复出原始向量的具体值。虽然向量已经不再是原来的样子，但问题不大，因为量化过程仍然保留了一些原始信息。 量化的内存开销 量化后的向量占用的内存开销确实降低了，但是量化过程会产生一个额外的码本，这会增加内存开销。 随着数据量的增大，数据可能越来越稀疏，聚类的效果如果需要得到保证，可能需要的聚类质心也就越多。 在低维向量中，这个开销可能不明显，但在高维向量中，这个开销会变得非常大。为了保证量化质量，可能需要非常大的聚类数量，这被称为维度灾难问题。\n基量化算法 为了解决维度灾难问题，可以将高维向量分割成低维子向量，然后在子向量上进行独 立的量化。将子向量的量化编码值合并在一起，就得到了原始向量的最终量化编码值。 这种方法被称为基量化算法（PQ）。通过基量化，可以大大减少内存开销。 Product quantization for nearest neighbor searchPQ原始论文:基于(乘)积量化的近邻搜索\nNSW算法 德劳内三角剖分法 Delaunay triangulation algorithm NSW（Navigating Spreading-out Walk) 是一种基于图结构的近似最近邻算法。通过建立图结构，可以快速导航到目标节点，并通过精细化的搜索找到最相似的向量。 NSW算法的核心思想是先粗略搜索，再精细搜索。为了改善NSW算法的性能，可以使用HN-SW算法，它在NSW算法的基础上添加了多层结构，实现了由粗到细的搜索过程。\n向量所有的点都先拿出来 在开始建立点之间的关系的时候，由于数据量较少，因此很稀疏（很容易演化成点之间的长连接）， 随着数据的增加，点之间的联系会越来越密集。通过长连接快速到达点 再通过短连接细化到具体的点数据上。\nEfficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs 使用分层导航小世界的高效且健壮的近似最近邻搜索\n向量数据库的功能 向量数据库除了支持近似最近邻算法，还需要具备传统数据库的功能，如简单易用的访问接口、访问控制、权限管理、审计功能、数据备份等。 此外，向量数据库还需要具备多节点、容错性、监控和追踪功能，以满足大规模数据和访问量的需求。\n在大AI时代的开端，向量数据库是一个具有广阔前景的领域，但也面临着工程上的挑战。一个可用的向量数据库产品需要解决诸多细节问题，并综合各种算法和功能，以提供高效、可靠的服务。\n","date":"2023-08-21T22:05:47+08:00","permalink":"https://blog.importzhh.me/p/i-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8A%80%E6%9C%AF%E9%89%B4%E8%B5%8F/","title":"I: 向量数据库技术鉴赏"},{"content":"函数式编程思想 在函数式编程中，纯函数是指满足以下两个条件的函数：\n相同的输入始终产生相同的输出：纯函数的输出仅由输入决定，不受任何外部状态的影响。对于相同的输入，纯函数始终返回相同的输出。这种确定性的行为使得纯函数易于理解和推理。\n没有副作用：纯函数在执行过程中不会修改外部状态或产生其他可观察到的副作用。副作用包括对全局变量的修改、对文件系统的读写、网络请求等。纯函数仅通过输入和输出之间的映射来完成计算，不会对外部环境造成任何影响。\n纯函数的特性使得其具有很多优点，包括：\n可测试性：由于纯函数的输出仅由输入决定，因此很容易编写测试来验证其行为。测试可以针对不同的输入和边界情况进行，而无需考虑外部状态的变化。\n可重用性：纯函数可以被重复使用，因为它们的行为是确定性的，不会受到外部状态的影响。可以在不同的上下文中调用纯函数，而无需担心副作用对系统的影响。\n并发安全性：由于纯函数没有副作用，它们可以在并发环境中安全地执行。多个线程或进程可以同时调用纯函数，而不会发生竞争条件或数据损坏。\n需要注意的是，纯函数并不意味着所有函数都必须是纯函数。在实际的应用中，我们可能需要与外部环境进行交互或处理可变状态。然而，通过使用纯函数来封装和隔离这些副作用，可以提高代码的可维护性和可测试性。\n让我们通过一个具体的例子来说明这个概念。假设我们需要从文件中读取数据，并计算数据的平均值。\n如果我们不使用纯函数，可能会直接在函数中读取文件和计算平均值：\n1 2 3 4 def calculate_average(filename): with open(filename, \u0026#39;r\u0026#39;) as file: numbers = list(map(int, file.readlines())) return sum(numbers) / len(numbers) 上面的 calculate_average 函数是非纯函数，因为它有一个副作用：读取文件。这使得函数的行为依赖于外部状态（文件的内容），并且每次调用函数可能会得到不同的结果。\n现在，让我们试着将副作用封装在纯函数之外，将非纯函数转换为纯函数：\n1 2 3 4 5 6 7 8 9 def read_numbers(filename): with open(filename, \u0026#39;r\u0026#39;) as file: return list(map(int, file.readlines())) def calculate_average(numbers): return sum(numbers) / len(numbers) numbers = read_numbers(\u0026#39;data.txt\u0026#39;) average = calculate_average(numbers) 在这个例子中，我们将读取文件的副作用封装在 read_numbers 函数中，而 calculate_average 函数则是一个纯函数，它只负责计算平均值。这样，我们就将副作用隔离在纯函数之外，使得纯函数的行为更容易理解和测试。\n当然，函数式编程允许函数作为参数传递给其他函数，这被称为高阶函数。这是函数式编程的一个重要特性，可以帮助我们编写更灵活和可重用的代码。\n1 2 3 4 5 6 7 8 9 10 11 def apply_function(func, value): return func(value) def double(x): return x * 2 def square(x): return x ** 2 print(apply_function(double, 5)) # 输出：10 print(apply_function(square, 5)) # 输出：25 在这个例子中，apply_function 是一个高阶函数，它接受一个函数 func 和一个值 value 作为参数，然后返回 func(value) 的结果。\n我们可以将 double 函数或 square 函数作为参数传递给 apply_function。这样，我们就可以使用 apply_function 来应用任何我们想要的函数，使得代码更具灵活性和可重用性。\n这就是函数式编程中的高阶函数概念，通过传递函数作为参数，我们可以创建更强大、更灵活的抽象。在这个例子中，apply_function 的行为依赖于它的输入参数 func 和 value。\n当我们说函数不应该依赖于外部状态，我们是指函数的行为不应该依赖于不作为其输入参数的外部状态。例如，全局变量或者类的成员变量。\n在这个例子中，虽然 apply_function 的行为会根据传入的函数 func 而改变，但这并不意味着 apply_function 依赖于外部状态，因为 func 是 apply_function 的一个输入参数。\n如果我们总是用相同的函数和值调用 apply_function，那么它总会返回相同的结果。这也是纯函数的一个重要特性：给定相同的输入，总会产生相同的输出，而不受外部状态的影响。\n我们使用Python来编写一个简单的MapReduce示例。我们将计算一组数字的平均值，使用Map阶段将数字分割为键值对，然后使用Reduce阶段计算平均值。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 from collections import defaultdict # 输入数据 data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # Map阶段 def mapper(data): result = defaultdict(list) for num in data: key = num % 2 # 将奇数和偶数分为两组 result[key].append(num) return result.items() # Reduce阶段 def reducer(key, values): count = len(values) total = sum(values) average = total / count return key, average # 执行MapReduce mapped_data = mapper(data) reduced_data = [reducer(key, values) for key, values in mapped_data] # 打印结果 for key, average in reduced_data: print(f\u0026#34;Key: {key}, Average: {average}\u0026#34;) 1 2 3 4 5 6 7 8 这个例子中，我们将数字分为奇数和偶数两组，并计算每组的平均值。在Map阶段，我们使用取模运算符将数字分为两组，并将数字添加到对应的键中。在Reduce阶段，我们计算每组数字的总和和数量，然后计算平均值。 运行这段代码，你会看到输出结果类似于： Key: 0, Average: 6.0 Key: 1, Average: 5.5 其中，Key表示奇数或偶数组，Average表示对应组的平均值。这个简单的示例展示了MapReduce思想的基本原理，通过将数据分割为键值对，然后对相同键的值进行聚合处理，以实现数据处理和计算。\nhttps://codewords.recurse.com/issues/one/an-introduction-to-functional-programming\nLambda表达式 例子 例一 我们在创建线程并启动时可以使用匿名内部类的写法：\n1 2 3 4 5 6 new Thread(new Runnable() { @Override public void run() { System.out.println(\u0026#34;hello world \u0026#34;); } }).start(); 可以使用Lambda的格式对其进行修改。修改后如下：\n1 2 3 new Thread(()-\u0026gt;{ System.out.println(\u0026#34;hello world \u0026#34;); }).start(); 例二: 现有方法定义如下，其中IntBinaryOperator是一个接口。先使用匿名内部类的写法调用该方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public static int calculateNum(IntBinaryOperator operator){ int a = 10; int b = 20; return operator.applyAsInt(a, b); } public static void main(String[] args) { int i = calculateNum(new IntBinaryOperator() { @Override public int applyAsInt(int left, int right) { return left + right; } }); System.out.println(i); } Lambda写法：\n1 2 3 4 5 6 public static void main(String[] args) { int i = calculateNum((int left, int right)-\u0026gt;{ return left + right; }); System.out.println(i); } 例三： 现有方法定义如下，其中IntPredicate是一个接口。先使用匿名内部类的写法调用该方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public static void printNum(IntPredicate predicate){ int[] arr = {1,2,3,4,5,6,7,8,9,10}; for (int i : arr) { if(predicate.test(i)){ System.out.println(i); } } } public static void main(String[] args) { printNum(new IntPredicate() { @Override public boolean test(int value) { return value%2==0; } }); } Lambda写法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 public static void main(String[] args) { printNum((int value)-\u0026gt; { return value%2==0; }); } public static void printNum(IntPredicate predicate){ int[] arr = {1,2,3,4,5,6,7,8,9,10}; for (int i : arr) { if(predicate.test(i)){ System.out.println(i); } } } 例四： 现有方法定义如下，其中Function是一个接口。先使用匿名内部类的写法调用该方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public static \u0026lt;R\u0026gt; R typeConver(Function\u0026lt;String,R\u0026gt; function){ String str = \u0026#34;1235\u0026#34;; R result = function.apply(str); return result; } public static void main(String[] args) { Integer result = typeConver(new Function\u0026lt;String, Integer\u0026gt;() { @Override public Integer apply(String s) { return Integer.valueOf(s); } }); System.out.println(result); } Lambda写法：\n1 2 3 4 Integer result = typeConver((String s)-\u0026gt;{ return Integer.valueOf(s); }); System.out.println(result); 例五： 现有方法定义如下，其中IntConsumer是一个接口。先使用匿名内部类的写法调用该方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public static void foreachArr(IntConsumer consumer){ int[] arr = {1,2,3,4,5,6,7,8,9,10}; for (int i : arr) { consumer.accept(i); } } public static void main(String[] args) { foreachArr(new IntConsumer() { @Override public void accept(int value) { System.out.println(value); } }); } Lambda写法：\n1 2 3 4 5 public static void main(String[] args) { foreachArr((int value)-\u0026gt;{ System.out.println(value); }); } 省略规则 参数类型可以省略 方法体只有一句代码时大括号return和唯一一句代码的分号可以省略 方法只有一个参数时小括号可以省略 以上这些规则都记不住也可以省略不记 Stream流 概述 Java8的Stream使用的是函数式编程模式，如同它的名字一样，它可以被用来对集合或数组进行链状流式的操作。可以更方便的让我们对集合或数组操作。\n案例数据准备 1 2 3 4 5 6 7 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.16\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @Data @NoArgsConstructor @AllArgsConstructor @EqualsAndHashCode//用于后期的去重使用 public class Author { //id private Long id; //姓名 private String name; //年龄 private Integer age; //简介 private String intro; //作品 private List\u0026lt;Book\u0026gt; books; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @Data @AllArgsConstructor @NoArgsConstructor @EqualsAndHashCode//用于后期的去重使用 public class Book { //id private Long id; //书名 private String name; //分类 private String category; //评分 private Integer score; //简介 private String intro; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 private static List\u0026lt;Author\u0026gt; getAuthors() { //数据初始化 Author author = new Author(1L,\u0026#34;张三\u0026#34;,33,\u0026#34;一个勤劳的农民\u0026#34;,null); Author author2 = new Author(2L,\u0026#34;李四\u0026#34;,15,\u0026#34;聪明好学的青年\u0026#34;,null); Author author3 = new Author(3L,\u0026#34;王五\u0026#34;,14,\u0026#34;阳光开朗的少年\u0026#34;,null); Author author4 = new Author(3L,\u0026#34;王五\u0026#34;,14,\u0026#34;阳光开朗的少年\u0026#34;,null); //书籍列表 List\u0026lt;Book\u0026gt; books1 = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Book\u0026gt; books2 = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Book\u0026gt; books3 = new ArrayList\u0026lt;\u0026gt;(); books1.add(new Book(1L,\u0026#34;努力改变人生\u0026#34;,\u0026#34;自传,励志\u0026#34;,88,\u0026#34;不懈努力才能改变命运\u0026#34;)); books1.add(new Book(2L,\u0026#34;青春不留白\u0026#34;,\u0026#34;成长,励志\u0026#34;,99,\u0026#34;要充实青春,争取进步\u0026#34;)); books2.add(new Book(3L,\u0026#34;世界之大\u0026#34;,\u0026#34;哲学\u0026#34;,85,\u0026#34;要用思考来领悟世界\u0026#34;)); books2.add(new Book(3L,\u0026#34;世界之大\u0026#34;,\u0026#34;哲学\u0026#34;,85,\u0026#34;要用思考来领悟世界\u0026#34;)); books2.add(new Book(4L,\u0026#34;随心而动\u0026#34;,\u0026#34;爱情,自传\u0026#34;,56,\u0026#34;要学会顺其自然\u0026#34;)); books3.add(new Book(5L,\u0026#34;你我皆一体\u0026#34;,\u0026#34;爱情\u0026#34;,56,\u0026#34;要珍惜彼此\u0026#34;)); books3.add(new Book(6L,\u0026#34;心与行\u0026#34;,\u0026#34;自传\u0026#34;,100,\u0026#34;要言行一致\u0026#34;)); books3.add(new Book(6L,\u0026#34;心与行\u0026#34;,\u0026#34;自传\u0026#34;,100,\u0026#34;要言行一致\u0026#34;)); author.setBooks(books1); author2.setBooks(books2); author3.setBooks(books3); author4.setBooks(books3); List\u0026lt;Author\u0026gt; authorList = new ArrayList\u0026lt;\u0026gt;(Arrays.asList(author,author2,author3,author4)); return authorList; } 快速入门 需求 我们可以调用getAuthors方法获取到作家的集合。现在需要打印所有年龄小于18的作家的名字，并且要注意去重。\n实现 1 2 3 4 5 6 7 //打印所有年龄小于18的作家的名字，并且要注意去重 List\u0026lt;Author\u0026gt; authors = getAuthors(); authors. stream()//把集合转换成流 .distinct()//先去除重复的作家 .filter(author -\u0026gt; author.getAge()\u0026lt;18)//筛选年龄小于18的 .forEach(author -\u0026gt; System.out.println(author.getName()));//遍历打印名字 常用操作 创建流 单列集合： 集合对象.stream()\n1 2 List\u0026lt;Author\u0026gt; authors = getAuthors(); Stream\u0026lt;Author\u0026gt; stream = authors.stream(); 数组：Arrays.stream(数组) 或者使用Stream.of来创建\n1 2 3 Integer[] arr = {1,2,3,4,5}; Stream\u0026lt;Integer\u0026gt; stream = Arrays.stream(arr); Stream\u0026lt;Integer\u0026gt; stream2 = Stream.of(arr); 双列集合：转换成单列集合后再创建\n1 2 3 4 5 6 Map\u0026lt;String,Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); map.put(\u0026#34;蜡笔小新\u0026#34;,19); map.put(\u0026#34;黑子\u0026#34;,17); map.put(\u0026#34;日向翔阳\u0026#34;,16); Stream\u0026lt;Map.Entry\u0026lt;String, Integer\u0026gt;\u0026gt; stream = map.entrySet().stream(); 中间操作 filter 可以对流中的元素进行条件过滤，符合过滤条件的才能继续留在流中。\n例如：\n打印所有姓名长度大于1的作家的姓名\n1 2 3 4 List\u0026lt;Author\u0026gt; authors = getAuthors(); authors.stream() .filter(author -\u0026gt; author.getName().length()\u0026gt;1) .forEach(author -\u0026gt; System.out.println(author.getName())); map 可以把对流中的元素进行计算或转换。\n例如：\n打印所有作家的姓名\n1 2 3 4 5 6 List\u0026lt;Author\u0026gt; authors = getAuthors(); authors .stream() .map(author -\u0026gt; author.getName()) .forEach(name-\u0026gt;System.out.println(name)); 1 2 3 4 5 6 7 // 打印所有作家的姓名 List\u0026lt;Author\u0026gt; authors = getAuthors(); authors.stream() .map(author -\u0026gt; author.getAge()) .map(age-\u0026gt;age+10) .forEach(age-\u0026gt; System.out.println(age)); distinct 可以去除流中的重复元素。\n例如：\n打印所有作家的姓名，并且要求其中不能有重复元素。\n1 2 3 4 List\u0026lt;Author\u0026gt; authors = getAuthors(); authors.stream() .distinct() .forEach(author -\u0026gt; System.out.println(author.getName())); 注意：distinct方法是依赖Object的equals方法来判断是否是相同对象的。所以需要注意重写equals方法。\nsorted 可以对流中的元素进行排序。\n例如：\n对流中的元素按照年龄进行降序排序，并且要求不能有重复的元素。\n1 2 3 4 5 6 List\u0026lt;Author\u0026gt; authors = getAuthors(); // 对流中的元素按照年龄进行降序排序，并且要求不能有重复的元素。 authors.stream() .distinct() .sorted() .forEach(author -\u0026gt; System.out.println(author.getAge())); 1 2 3 4 5 6 List\u0026lt;Author\u0026gt; authors = getAuthors(); // 对流中的元素按照年龄进行降序排序，并且要求不能有重复的元素。 authors.stream() .distinct() .sorted((o1, o2) -\u0026gt; o2.getAge()-o1.getAge()) .forEach(author -\u0026gt; System.out.println(author.getAge())); 注意：如果调用空参的sorted()方法，需要流中的元素是实现了Comparable。\nlimit 可以设置流的最大长度，超出的部分将被抛弃。\n例如：\n对流中的元素按照年龄进行降序排序，并且要求不能有重复的元素,然后打印其中年龄最大的两个作家的姓名。\n1 2 3 4 5 6 List\u0026lt;Author\u0026gt; authors = getAuthors(); authors.stream() .distinct() .sorted() .limit(2) .forEach(author -\u0026gt; System.out.println(author.getName())); skip 跳过流中的前n个元素，返回剩下的元素\n例如：\n打印除了年龄最大的作家外的其他作家，要求不能有重复元素，并且按照年龄降序排序。\n1 2 3 4 5 6 7 // 打印除了年龄最大的作家外的其他作家，要求不能有重复元素，并且按照年龄降序排序。 List\u0026lt;Author\u0026gt; authors = getAuthors(); authors.stream() .distinct() .sorted() .skip(1) .forEach(author -\u0026gt; System.out.println(author.getName())); flatMap map只能把一个对象转换成另一个对象来作为流中的元素。而flatMap可以把一个对象转换成多个对象作为流中的元素。\n例一：\n打印所有书籍的名字。要求对重复的元素进行去重。\n1 2 3 4 5 6 7 // 打印所有书籍的名字。要求对重复的元素进行去重。 List\u0026lt;Author\u0026gt; authors = getAuthors(); authors.stream() .flatMap(author -\u0026gt; author.getBooks().stream()) .distinct() .forEach(book -\u0026gt; System.out.println(book.getName())); 例二：\n打印现有数据的所有分类。要求对分类进行去重。不能出现这种格式：哲学,爱情\n1 2 3 4 5 6 7 8 // 打印现有数据的所有分类。要求对分类进行去重。不能出现这种格式：哲学,爱情 爱情 List\u0026lt;Author\u0026gt; authors = getAuthors(); authors.stream() .flatMap(author -\u0026gt; author.getBooks().stream()) .distinct() .flatMap(book -\u0026gt; Arrays.stream(book.getCategory().split(\u0026#34;,\u0026#34;))) .distinct() .forEach(category-\u0026gt; System.out.println(category)); 终结操作 forEach 对流中的元素进行遍历操作，我们通过传入的参数去指定对遍历到的元素进行什么具体操作。\n例子：\n输出所有作家的名字\n1 2 3 4 5 6 7 // 输出所有作家的名字 List\u0026lt;Author\u0026gt; authors = getAuthors(); authors.stream() .map(author -\u0026gt; author.getName()) .distinct() .forEach(name-\u0026gt; System.out.println(name)); count 可以用来获取当前流中元素的个数。\n例子：\n打印这些作家的所出书籍的数目，注意删除重复元素。\n1 2 3 4 5 6 7 8 // 打印这些作家的所出书籍的数目，注意删除重复元素。 List\u0026lt;Author\u0026gt; authors = getAuthors(); long count = authors.stream() .flatMap(author -\u0026gt; author.getBooks().stream()) .distinct() .count(); System.out.println(count); max\u0026amp;min 可以用来或者流中的最值。\n例子：\n分别获取这些作家的所出书籍的最高分和最低分并打印。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 分别获取这些作家的所出书籍的最高分和最低分并打印。 //Stream\u0026lt;Author\u0026gt; -\u0026gt; Stream\u0026lt;Book\u0026gt; -\u0026gt;Stream\u0026lt;Integer\u0026gt; -\u0026gt;求值 List\u0026lt;Author\u0026gt; authors = getAuthors(); Optional\u0026lt;Integer\u0026gt; max = authors.stream() .flatMap(author -\u0026gt; author.getBooks().stream()) .map(book -\u0026gt; book.getScore()) .max((score1, score2) -\u0026gt; score1 - score2); Optional\u0026lt;Integer\u0026gt; min = authors.stream() .flatMap(author -\u0026gt; author.getBooks().stream()) .map(book -\u0026gt; book.getScore()) .min((score1, score2) -\u0026gt; score1 - score2); System.out.println(max.get()); System.out.println(min.get()); collect 把当前流转换成一个集合。\n例子：\n获取一个存放所有作者名字的List集合。\n1 2 3 4 5 6 // 获取一个存放所有作者名字的List集合。 List\u0026lt;Author\u0026gt; authors = getAuthors(); List\u0026lt;String\u0026gt; nameList = authors.stream() .map(author -\u0026gt; author.getName()) .collect(Collectors.toList()); System.out.println(nameList); 获取一个所有书名的Set集合。\n1 2 3 4 5 6 7 // 获取一个所有书名的Set集合。 List\u0026lt;Author\u0026gt; authors = getAuthors(); Set\u0026lt;Book\u0026gt; books = authors.stream() .flatMap(author -\u0026gt; author.getBooks().stream()) .collect(Collectors.toSet()); System.out.println(books); 获取一个Map集合，map的key为作者名，value为List\n1 2 3 4 5 6 7 8 // 获取一个Map集合，map的key为作者名，value为List\u0026lt;Book\u0026gt; List\u0026lt;Author\u0026gt; authors = getAuthors(); Map\u0026lt;String, List\u0026lt;Book\u0026gt;\u0026gt; map = authors.stream() .distinct() .collect(Collectors.toMap(author -\u0026gt; author.getName(), author -\u0026gt; author.getBooks())); System.out.println(map); 查找与匹配 anyMatch 可以用来判断是否有任意符合匹配条件的元素，结果为boolean类型。\n例子：\n判断是否有年龄在29以上的作家\n1 2 3 4 5 // 判断是否有年龄在29以上的作家 List\u0026lt;Author\u0026gt; authors = getAuthors(); boolean flag = authors.stream() .anyMatch(author -\u0026gt; author.getAge() \u0026gt; 29); System.out.println(flag); allMatch 可以用来判断是否都符合匹配条件，结果为boolean类型。如果都符合结果为true，否则结果为false。\n例子：\n判断是否所有的作家都是成年人\n1 2 3 4 5 // 判断是否所有的作家都是成年人 List\u0026lt;Author\u0026gt; authors = getAuthors(); boolean flag = authors.stream() .allMatch(author -\u0026gt; author.getAge() \u0026gt;= 18); System.out.println(flag); noneMatch 可以判断流中的元素是否都不符合匹配条件。如果都不符合结果为true，否则结果为false\n例子：\n判断作家是否都没有超过100岁的。\n1 2 3 4 5 6 7 // 判断作家是否都没有超过100岁的。 List\u0026lt;Author\u0026gt; authors = getAuthors(); boolean b = authors.stream() .noneMatch(author -\u0026gt; author.getAge() \u0026gt; 100); System.out.println(b); findAny 获取流中的任意一个元素。该方法没有办法保证获取的一定是流中的第一个元素。\n例子：\n获取任意一个年龄大于18的作家，如果存在就输出他的名字\n1 2 3 4 5 6 7 // 获取任意一个年龄大于18的作家，如果存在就输出他的名字 List\u0026lt;Author\u0026gt; authors = getAuthors(); Optional\u0026lt;Author\u0026gt; optionalAuthor = authors.stream() .filter(author -\u0026gt; author.getAge()\u0026gt;18) .findAny(); optionalAuthor.ifPresent(author -\u0026gt; System.out.println(author.getName())); findFirst 获取流中的第一个元素。\n例子：\n获取一个年龄最小的作家，并输出他的姓名。\n1 2 3 4 5 6 7 // 获取一个年龄最小的作家，并输出他的姓名。 List\u0026lt;Author\u0026gt; authors = getAuthors(); Optional\u0026lt;Author\u0026gt; first = authors.stream() .sorted((o1, o2) -\u0026gt; o1.getAge() - o2.getAge()) .findFirst(); first.ifPresent(author -\u0026gt; System.out.println(author.getName())); reduce归并 对流中的数据按照你指定的计算方式计算出一个结果。（缩减操作）\nreduce的作用是把stream中的元素给组合起来，我们可以传入一个初始值，它会按照我们的计算方式依次拿流中的元素和初始化值进行计算，计算结果再和后面的元素计算。\nreduce两个参数的重载形式内部的计算方式如下：\n1 2 3 4 T result = identity; for (T element : this stream) result = accumulator.apply(result, element) return result; 其中identity就是我们可以通过方法参数传入的初始值，accumulator的apply具体进行什么计算也是我们通过方法参数来确定的。\n例子：\n使用reduce求所有作者年龄的和\n1 2 3 4 5 6 7 // 使用reduce求所有作者年龄的和 List\u0026lt;Author\u0026gt; authors = getAuthors(); Integer sum = authors.stream() .distinct() .map(author -\u0026gt; author.getAge()) .reduce(0, (result, element) -\u0026gt; result + element); System.out.println(sum); 使用reduce求所有作者中年龄的最大值\n1 2 3 4 5 6 7 // 使用reduce求所有作者中年龄的最大值 List\u0026lt;Author\u0026gt; authors = getAuthors(); Integer max = authors.stream() .map(author -\u0026gt; author.getAge()) .reduce(Integer.MIN_VALUE, (result, element) -\u0026gt; result \u0026lt; element ? element : result); System.out.println(max); 使用reduce求所有作者中年龄的最小值\n1 2 3 4 5 6 // 使用reduce求所有作者中年龄的最小值 List\u0026lt;Author\u0026gt; authors = getAuthors(); Integer min = authors.stream() .map(author -\u0026gt; author.getAge()) .reduce(Integer.MAX_VALUE, (result, element) -\u0026gt; result \u0026gt; element ? element : result); System.out.println(min); reduce一个参数的重载形式内部的计算\n1 2 3 4 5 6 7 8 9 10 11 boolean foundAny = false; T result = null; for (T element : this stream) { if (!foundAny) { foundAny = true; result = element; } else result = accumulator.apply(result, element); } return foundAny ? Optional.of(result) : Optional.empty(); 如果用一个参数的重载方法去求最小值代码如下：\n1 2 3 4 5 6 // 使用reduce求所有作者中年龄的最小值 List\u0026lt;Author\u0026gt; authors = getAuthors(); Optional\u0026lt;Integer\u0026gt; minOptional = authors.stream() .map(author -\u0026gt; author.getAge()) .reduce((result, element) -\u0026gt; result \u0026gt; element ? element : result); minOptional.ifPresent(age-\u0026gt; System.out.println(age)); 注意事项 惰性求值（如果没有终结操作，没有中间操作是不会得到执行的） 流是一次性的（一旦一个流对象经过一个终结操作后。这个流就不能再被使用） 不会影响原数据（我们在流中可以多数据做很多处理。但是正常情况下是不会影响原来集合中的元素的。这往往也是我们期望的） Optional 概述 我们在编写代码的时候出现最多的就是空指针异常。所以在很多情况下我们需要做各种非空的判断。\n例如：\n1 2 3 4 Author author = getAuthor(); if(author!=null){ System.out.println(author.getName()); } 尤其是对象中的属性还是一个对象的情况下。这种判断会更多。\n而过多的判断语句会让我们的代码显得臃肿不堪。\n所以在JDK8中引入了Optional,养成使用Optional的习惯后你可以写出更优雅的代码来避免空指针异常。\n并且在很多函数式编程相关的API中也都用到了Optional，如果不会使用Optional也会对函数式编程的学习造成影响。\n使用 创建对象 Optional就好像是包装类，可以把我们的具体数据封装Optional对象内部。然后我们去使用Optional中封装好的方法操作封装进去的数据就可以非常优雅的避免空指针异常。\n我们一般使用Optional的静态方法ofNullable来把数据封装成一个Optional对象。无论传入的参数是否为null都不会出现问题。\n1 2 Author author = getAuthor(); Optional\u0026lt;Author\u0026gt; authorOptional = Optional.ofNullable(author); 你可能会觉得还要加一行代码来封装数据比较麻烦。但是如果改造下getAuthor方法，让其的返回值就是封装好的Optional的话，我们在使用时就会方便很多。\n而且在实际开发中我们的数据很多是从数据库获取的。Mybatis从3.5版本可以也已经支持Optional了。我们可以直接把dao方法的返回值类型定义成Optional类型，MyBastis会自己把数据封装成Optional对象返回。封装的过程也不需要我们自己操作。\n如果你确定一个对象不是空的则可以使用Optional的静态方法of来把数据封装成Optional对象。\n1 2 Author author = new Author(); Optional\u0026lt;Author\u0026gt; authorOptional = Optional.of(author); 但是一定要注意，如果使用of的时候传入的参数必须不为null。（尝试下传入null会出现什么结果）\n如果一个方法的返回值类型是Optional类型。而如果我们经判断发现某次计算得到的返回值为null，这个时候就需要把null封装成Optional对象返回。这时则可以使用Optional的静态方法empty来进行封装。\n1 Optional.empty() 所以最后你觉得哪种方式会更方便呢？ofNullable\n安全消费值 我们获取到一个Optional对象后肯定需要对其中的数据进行使用。这时候我们可以使用其ifPresent方法对来消费其中的值。\n这个方法会判断其内封装的数据是否为空，不为空时才会执行具体的消费代码。这样使用起来就更加安全了。\n例如,以下写法就优雅的避免了空指针异常。\n1 2 3 Optional\u0026lt;Author\u0026gt; authorOptional = Optional.ofNullable(getAuthor()); authorOptional.ifPresent(author -\u0026gt; System.out.println(author.getName())); 获取值 如果我们想获取值自己进行处理可以使用get方法获取，但是不推荐。因为当Optional内部的数据为空的时候会出现异常。\n安全获取值 如果我们期望安全的获取值。我们不推荐使用get方法，而是使用Optional提供的以下方法。\norElseGet\n获取数据并且设置数据为空时的默认值。如果数据不为空就能获取到该数据。如果为空则根据你传入的参数来创建对象作为默认值返回。\n1 2 Optional\u0026lt;Author\u0026gt; authorOptional = Optional.ofNullable(getAuthor()); Author author1 = authorOptional.orElseGet(() -\u0026gt; new Author()); orElseThrow\n获取数据，如果数据不为空就能获取到该数据。如果为空则根据你传入的参数来创建异常抛出。\n1 2 3 4 5 6 7 Optional\u0026lt;Author\u0026gt; authorOptional = Optional.ofNullable(getAuthor()); try { Author author = authorOptional.orElseThrow((Supplier\u0026lt;Throwable\u0026gt;) () -\u0026gt; new RuntimeException(\u0026#34;author为空\u0026#34;)); System.out.println(author.getName()); } catch (Throwable throwable) { throwable.printStackTrace(); } 过滤 我们可以使用filter方法对数据进行过滤。如果原本是有数据的，但是不符合判断，也会变成一个无数据的Optional对象。\n1 2 Optional\u0026lt;Author\u0026gt; authorOptional = Optional.ofNullable(getAuthor()); authorOptional.filter(author -\u0026gt; author.getAge()\u0026gt;100).ifPresent(author -\u0026gt; System.out.println(author.getName())); 判断 我们可以使用isPresent方法进行是否存在数据的判断。如果为空返回值为false,如果不为空，返回值为true。但是这种方式并不能体现Optional的好处，更推荐使用ifPresent方法。\n1 2 3 4 5 Optional\u0026lt;Author\u0026gt; authorOptional = Optional.ofNullable(getAuthor()); if (authorOptional.isPresent()) { System.out.println(authorOptional.get().getName()); } 数据转换 Optional还提供了map可以让我们的对数据进行转换，并且转换得到的数据也还是被Optional包装好的，保证了我们的使用安全。\n例如我们想获取作家的书籍集合。\n1 2 3 4 5 private static void testMap() { Optional\u0026lt;Author\u0026gt; authorOptional = getAuthorOptional(); Optional\u0026lt;List\u0026lt;Book\u0026gt;\u0026gt; optionalBooks = authorOptional.map(author -\u0026gt; author.getBooks()); optionalBooks.ifPresent(books -\u0026gt; System.out.println(books)); } 函数式接口 概述 只有一个抽象方法的接口我们称之为函数接口。\nJDK的函数式接口都加上了**@FunctionalInterface** 注解进行标识。但是无论是否加上该注解只要接口中只有一个抽象方法，都是函数式接口。\n常见函数式接口 Consumer 消费接口\n根据其中抽象方法的参数列表和返回值类型知道，我们可以在方法中对传入的参数进行消费。\nFunction 计算转换接口\n根据其中抽象方法的参数列表和返回值类型知道，我们可以在方法中对传入的参数计算或转换，把结果返回\nPredicate 判断接口\n根据其中抽象方法的参数列表和返回值类型知道，我们可以在方法中对传入的参数条件判断，返回判断结果\nSupplier 生产型接口\n根据其中抽象方法的参数列表和返回值类型知道，我们可以在方法中创建对象，把创建好的对象返回\n常用的默认方法 and\n我们在使用Predicate接口时候可能需要进行判断条件的拼接。而and方法相当于是使用\u0026amp;\u0026amp;来拼接两个判断条件\n例如：\n打印作家中年龄大于17并且姓名的长度大于1的作家。\nList authors = getAuthors(); Stream authorStream = authors.stream(); authorStream.filter(new Predicate() { @Override public boolean test(Author author) { return author.getAge()\u0026gt;17; } }.and(new Predicate() { @Override public boolean test(Author author) { return author.getName().length()\u0026gt;1; } })).forEach(author -\u0026gt; System.out.println(author));\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - or 我们在使用Predicate接口时候可能需要进行判断条件的拼接。而or方法相当于是使用||来拼接两个判断条件。 例如： 打印作家中年龄大于17或者姓名的长度小于2的作家。 ```java // 打印作家中年龄大于17或者姓名的长度小于2的作家。 List\u0026lt;Author\u0026gt; authors = getAuthors(); authors.stream() .filter(new Predicate\u0026lt;Author\u0026gt;() { @Override public boolean test(Author author) { return author.getAge()\u0026gt;17; } }.or(new Predicate\u0026lt;Author\u0026gt;() { @Override public boolean test(Author author) { return author.getName().length()\u0026lt;2; } })).forEach(author -\u0026gt; System.out.println(author.getName())); negate\nPredicate接口中的方法。negate方法相当于是在判断添加前面加了个! 表示取反\n例如：\n打印作家中年龄不大于17的作家。\n1 // 打印作家中年龄不大于17的作家。 List authors = getAuthors(); authors.stream() .filter(new Predicate() { @Override public boolean test(Author author) { return author.getAge()\u0026gt;17; } }.negate()).forEach(author -\u0026gt; System.out.println(author.getAge()));\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 ## 方法引用 我们在使用lambda时，如果方法体中只有一个方法的调用的话（包括构造方法）,我们可以用方法引用进一步简化代码。 ### 推荐用法 我们在使用lambda时不需要考虑什么时候用方法引用，用哪种方法引用，方法引用的格式是什么。我们只需要在写完lambda方法发现方法体只有一行代码，并且是方法的调用时使用快捷键尝试是否能够转换成方法引用即可。 当我们方法引用使用的多了慢慢的也可以直接写出方法引用。 ### 基本格式 类名或者对象名::方法名 ### 语法详解(了解) #### 引用类的静态方法 其实就是引用类的静态方法 ##### 格式 ```java 类名::方法名 使用前提 如果我们在重写方法的时候，方法体中只有一行代码，并且这行代码是调用了某个类的静态方法，并且我们把要重写的抽象方法中所有的参数都按照顺序传入了这个静态方法中，这个时候我们就可以引用类的静态方法。\n例如：\n如下代码就可以用方法引用进行简化\n1 2 3 4 5 6 List\u0026lt;Author\u0026gt; authors = getAuthors(); Stream\u0026lt;Author\u0026gt; authorStream = authors.stream(); authorStream.map(author -\u0026gt; author.getAge()) .map(age-\u0026gt;String.valueOf(age)); 注意，如果我们所重写的方法是没有参数的，调用的方法也是没有参数的也相当于符合以上规则。\n优化后如下：\n1 2 3 4 5 6 List\u0026lt;Author\u0026gt; authors = getAuthors(); Stream\u0026lt;Author\u0026gt; authorStream = authors.stream(); authorStream.map(author -\u0026gt; author.getAge()) .map(String::valueOf); 引用对象的实例方法 格式 1 对象名::方法名 使用前提 如果我们在重写方法的时候，方法体中只有一行代码，并且这行代码是调用了某个对象的成员方法，并且我们把要重写的抽象方法中所有的参数都按照顺序传入了这个成员方法中，这个时候我们就可以引用对象的实例方法\n例如：\n1 2 3 4 5 6 List\u0026lt;Author\u0026gt; authors = getAuthors(); Stream\u0026lt;Author\u0026gt; authorStream = authors.stream(); StringBuilder sb = new StringBuilder(); authorStream.map(author -\u0026gt; author.getName()) .forEach(name-\u0026gt;sb.append(name)); 优化后：\n1 2 3 4 5 6 List\u0026lt;Author\u0026gt; authors = getAuthors(); Stream\u0026lt;Author\u0026gt; authorStream = authors.stream(); StringBuilder sb = new StringBuilder(); authorStream.map(author -\u0026gt; author.getName()) .forEach(sb::append); 引用类的实例方法 格式 1 类名::方法名 使用前提 如果我们在重写方法的时候，方法体中只有一行代码，并且这行代码是调用了第一个参数的成员方法，并且我们把要重写的抽象方法中剩余的所有的参数都按照顺序传入了这个成员方法中，这个时候我们就可以引用类的实例方法。\n例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 interface UseString{ String use(String str,int start,int length); } public static String subAuthorName(String str, UseString useString){ int start = 0; int length = 1; return useString.use(str,start,length); } public static void main(String[] args) { subAuthorName(\u0026#34;三更草堂\u0026#34;, new UseString() { @Override public String use(String str, int start, int length) { return str.substring(start,length); } }); } 优化后如下：\n1 2 3 4 5 public static void main(String[] args) { subAuthorName(\u0026#34;三更草堂\u0026#34;, String::substring); } 构造器引用 如果方法体中的一行代码是构造器的话就可以使用构造器引用。\n格式 1 类名::new 使用前提 如果我们在重写方法的时候，方法体中只有一行代码，并且这行代码是调用了某个类的构造方法，并且我们把要重写的抽象方法中的所有的参数都按照顺序传入了这个构造方法中，这个时候我们就可以引用构造器。\n例如：\n1 2 3 4 5 6 List\u0026lt;Author\u0026gt; authors = getAuthors(); authors.stream() .map(author -\u0026gt; author.getName()) .map(name-\u0026gt;new StringBuilder(name)) .map(sb-\u0026gt;sb.append(\u0026#34;-三更\u0026#34;).toString()) .forEach(str-\u0026gt; System.out.println(str)); 优化后：\n1 2 3 4 5 6 List\u0026lt;Author\u0026gt; authors = getAuthors(); authors.stream() .map(author -\u0026gt; author.getName()) .map(StringBuilder::new) .map(sb-\u0026gt;sb.append(\u0026#34;-三更\u0026#34;).toString()) .forEach(str-\u0026gt; System.out.println(str)); 高级用法 基本数据类型优化 我们之前用到的很多Stream的方法由于都使用了泛型。所以涉及到的参数和返回值都是引用数据类型。\n即使我们操作的是整数小数，但是实际用的都是他们的包装类。JDK5中引入的自动装箱和自动拆箱让我们在使用对应的包装类时就好像使用基本数据类型一样方便。但是你一定要知道装箱和拆箱肯定是要消耗时间的。虽然这个时间消耗很下。但是在大量的数据不断的重复装箱拆箱的时候，你就不能无视这个时间损耗了。\n所以为了让我们能够对这部分的时间消耗进行优化。Stream还提供了很多专门针对基本数据类型的方法。\n例如：mapToInt,mapToLong,mapToDouble,flatMapToInt,flatMapToDouble等。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 private static void test27() { List\u0026lt;Author\u0026gt; authors = getAuthors(); authors.stream() .map(author -\u0026gt; author.getAge()) .map(age -\u0026gt; age + 10) .filter(age-\u0026gt;age\u0026gt;18) .map(age-\u0026gt;age+2) .forEach(System.out::println); authors.stream() .mapToInt(author -\u0026gt; author.getAge()) .map(age -\u0026gt; age + 10) .filter(age-\u0026gt;age\u0026gt;18) .map(age-\u0026gt;age+2) .forEach(System.out::println); } 并行流 当流中有大量元素时，我们可以使用并行流去提高操作的效率。其实并行流就是把任务分配给多个线程去完全。如果我们自己去用代码实现的话其实会非常的复杂，并且要求你对并发编程有足够的理解和认识。而如果我们使用Stream的话，我们只需要修改一个方法的调用就可以使用并行流来帮我们实现，从而提高效率。\nparallel方法可以把串行流转换成并行流。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 private static void test28() { Stream\u0026lt;Integer\u0026gt; stream = Stream.of(1, 2, 3, 4, 5, 6, 7, 8, 9, 10); Integer sum = stream.parallel() .peek(new Consumer\u0026lt;Integer\u0026gt;() { @Override public void accept(Integer num) { System.out.println(num+Thread.currentThread().getName()); } }) .filter(num -\u0026gt; num \u0026gt; 5) .reduce((result, ele) -\u0026gt; result + ele) .get(); System.out.println(sum); } 也可以通过parallelStream直接获取并行流对象。\n1 2 3 4 5 6 7 List\u0026lt;Author\u0026gt; authors = getAuthors(); authors.parallelStream() .map(author -\u0026gt; author.getAge()) .map(age -\u0026gt; age + 10) .filter(age-\u0026gt;age\u0026gt;18) .map(age-\u0026gt;age+2) .forEach(System.out::println); 链接 https://objcoding.com/2019/03/04/lambda/\n","date":"2023-08-09T23:01:50+08:00","permalink":"https://blog.importzhh.me/p/i-%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B-stream%E6%B5%81/","title":"I: 函数式编程-Stream流"},{"content":"简介 ByteBuddy支持检测和修改现有Java字节码的工具。 添加新的构造函数、方法和实例变量 删除现有构造函数、方法和实Ï例变量 修改实例变量的值。 修改方法参数的值。 检查现有的Java类结构、构造函数签名、方法签名和实例变量。 生成新的泛型Java类、构造函数、方法和实例变量（上界、下界、多界、通配符、参数化类型、方法类型参数） 搜索并替换方法中的代码。 添加和删除对Java类、构造函数、方法、实例变量和参数进行注释的注释 添加和修改Java内部类（添加第一级和第二级linner类，拦截内部类和匿名类）。 生成新的Java类、Java接口、枚举、注释和抽象类。 生成简单方法和lambda表达式。 在Advice代码之间共享数据。 代码初体验 在进行Byte Buddy之前先创建一个项目，然后引入相关依赖：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;net.bytebuddy\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;byte-buddy\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.12.10\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.junit.jupiter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit-jupiter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.6.3\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-io\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-io\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.11.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.16\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-log4j12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.25\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 测试类如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public class CreateClassTest { private static String path; /** * 每个测试方法执行前获取CreateClassTest的路径 */ @BeforeAll private static void before(){ path = CreateClassTest.class.getClassLoader().getResource(\u0026#34;\u0026#34;).getPath(); System.out.println(path); } /** * 生成一个类 */ @Test public void create() throws IOException { //Unloaded代表生成字节码还未加载到jvm DynamicType.Unloaded\u0026lt;Object\u0026gt; unloaded = new ByteBuddy() //指定父类 .subclass(Object.class) .make(); //获取生成类的字节码 byte[] bytes = unloaded.getBytes(); } } DynamicType：在运行时创建的动态类型，通常作为应用DynamicType.Builder或.AuxiliaryType的结果。\nUnloaded：尚未被给定ClassLoader加载的动态类型。\ninterface Unloaded\u0026lt;T\u0026gt; extends DynamicType\n类型参数: -由该动态类型实现的最具体的已知加载类型，通常是类型本身、接口或直接的超类。\nByteBuddy：在创建时，Byte Buddy会尝试发现当前JVM的版本。如果不可能，则创建与Java 6兼容的类文件。\nsubclass：用于创建所提供类型的子类，如果提供的类型是接口，则创建实现此接口类型的新类。\n当扩展一个类时，Byte Buddy模仿子类类型的所有可见构造函数。任何构造函数都被实现为只调用其具有相同签名的超类型构造函数。另一种行为可以通过subclass(Class, ConstructorStrategy)提供显式的ConstructorStrategy来指定。 注意:如果所提供的类型声明了类型变量或所有者类型，则此方法以泛型状态实现它们。\n1 2 3 4 5 6 7 8 9 10 11 12 13 /** * 通过构造器策略生成一个类 */ @Test public void createByConstructorStrategy() throws IOException { //Unloaded代表生成字节码还未加载到jvm DynamicType.Unloaded\u0026lt;CreateByConstructorStrategyEntity\u0026gt; unloaded = new ByteBuddy() //指定父类 .subclass(CreateByConstructorStrategyEntity.class, ConstructorStrategy.Default.NO_CONSTRUCTORS) .make(); //获取生成类的字节码 byte[] bytes = unloaded.getBytes(); } 实例化 1 2 3 4 5 6 7 8 9 10 11 @Test public void create() throws Exception { //Unloaded代表生成字节码还未加载到jvm DynamicType.Unloaded\u0026lt;Object\u0026gt; unloaded = new ByteBuddy() //指定父类 .subclass(Object.class) .make(); Class\u0026lt;?\u0026gt; loaded = unloaded.load(this.getClass().getClassLoader()).getLoaded(); Method method = loaded.getMethod(\u0026#34;toString\u0026#34;); System.out.println(method.invoke(loaded.newInstance())); } 构造器策略 NO_CONSTRUCTORS：此策略不添加构造函数 DEFAULT_CONSTRUCTOR：这个策略是添加一个默认构造函数来调用它的超类型默认构造函数。 IMITATE_SUPER_CLASS：这种策略是添加插装类型的超类的所有构造函数，其中每个构造函数都直接调用其签名等效的超类构造函数。 IMITATE_SUPER_CLASS_PUBLIC：这种策略是添加插装类型的超类的所有构造函数，其中每个构造函数都直接调用其签名等效的超类构造函数，只添加公共构造函数。 IMITATE_SUPER_CLASS_OPENING：这种策略是添加插装类型的超类的所有构造函数，其中每个构造函数都直接调用其签名等效的超类构造函数，为超类的任何可调用且声明为public的构造函数添加构造函数。 上面是Byte Buddy注释的直译，具体的一些解释会在下面测试中解释。\n为了测试以上策略创建一个类作为生成类得超类，如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public class CreateByConstructorStrategyEntity { private Long id; private String name; private String content; public CreateByConstructorStrategyEntity() { } protected CreateByConstructorStrategyEntity(Long id) { this.id = id; } private CreateByConstructorStrategyEntity(String name) { this.name = name; } public CreateByConstructorStrategyEntity(Long id, String name) { this.id = id; this.name = name; } public CreateByConstructorStrategyEntity(Long id, String name, String content) { this.id = id; this.name = name; this.content = content; } } 然后用用上面五种策略分别生成新类，并保存到文件中\n1 2 3 4 5 6 7 8 9 @Test public void createByConstructorStrategy() throws IOException { //Unloaded代表生成字节码还未加载到jvm DynamicType.Unloaded\u0026lt;CreateByConstructorStrategyEntity\u0026gt; unloaded = new ByteBuddy() //指定父类 .subclass(CreateByConstructorStrategyEntity.class, ConstructorStrategy.Default.NO_CONSTRUCTORS) .make(); unloaded.saveIn(new File(path)); } NO_CONSTRUCTORS：不生成构造器\n1 2 public class CreateByConstructorStrategyEntity$ByteBuddy$3c1yosow extends CreateByConstructorStrategyEntity { } DEFAULT_CONSTRUCTOR：给一个默认构造器\n1 2 3 4 public class CreateByConstructorStrategyEntity$ByteBuddy$sEqzBgYH extends CreateByConstructorStrategyEntity { public CreateByConstructorStrategyEntity$ByteBuddy$sEqzBgYH() { } } IMITATE_SUPER_CLASS：添加超类所有可访问的构造器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class CreateByConstructorStrategyEntity$ByteBuddy$w3TC5Z4k extends CreateByConstructorStrategyEntity { public CreateByConstructorStrategyEntity$ByteBuddy$w3TC5Z4k(Long var1, String var2, String var3) { super(var1, var2, var3); } public CreateByConstructorStrategyEntity$ByteBuddy$w3TC5Z4k(Long var1, String var2) { super(var1, var2); } protected CreateByConstructorStrategyEntity$ByteBuddy$w3TC5Z4k(Long var1) { super(var1); } public CreateByConstructorStrategyEntity$ByteBuddy$w3TC5Z4k() { } } IMITATE_SUPER_CLASS_PUBLIC：添加超类所有Public修饰的构造器\n1 2 3 4 5 6 7 8 9 10 11 12 public class CreateByConstructorStrategyEntity$ByteBuddy$AnM1yaqS extends CreateByConstructorStrategyEntity { public CreateByConstructorStrategyEntity$ByteBuddy$AnM1yaqS(Long var1, String var2, String var3) { super(var1, var2, var3); } public CreateByConstructorStrategyEntity$ByteBuddy$AnM1yaqS(Long var1, String var2) { super(var1, var2); } public CreateByConstructorStrategyEntity$ByteBuddy$AnM1yaqS() { } } IMITATE_SUPER_CLASS_OPENING：添加超类所有可访问的构造器并转换为Public\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class CreateByConstructorStrategyEntity$ByteBuddy$wMHt2jMn extends CreateByConstructorStrategyEntity { public CreateByConstructorStrategyEntity$ByteBuddy$wMHt2jMn(Long var1, String var2, String var3) { super(var1, var2, var3); } public CreateByConstructorStrategyEntity$ByteBuddy$wMHt2jMn(Long var1, String var2) { super(var1, var2); } public CreateByConstructorStrategyEntity$ByteBuddy$wMHt2jMn(Long var1) { super(var1); } public CreateByConstructorStrategyEntity$ByteBuddy$wMHt2jMn() { } } 文件保存 生成类字节码可以进行保存unloaded.saveIn(new File(path));，也就是将你生成的字节码以.class文件保存到相应的位置，但，继承jdk原生类和继承自定义类的保存位置不同。\n继承jdk原生类，文件会被保存到\n1 net.bytebuddy.renamed 路径下然后加超类的路径\n1 net.bytebuddy.renamed.java.lang.Object$ByteBuddy$i0ivrOhL 继承用户自定义类，文件被保存和生成类同级路径下\n1 com.importzhh.bytebuddy.CreateByConstructorStrategyEntity$ByteBuddy$AnM1yaqS 新增一个测试类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public class UserService { public UserService() { System.out.println(\u0026#34;UserService实例化成功\u0026#34;); } public String queryUser(String userId) { System.out.println(\u0026#34;queryUser() is invoked\u0026#34;+userId); return \u0026#34;hello:\u0026#34;+userId; } public void updateUser(String userId, String userName) { System.out.println(\u0026#34;updateUser() is invoked\u0026#34;); } public void deleteUser(String userId) { System.out.println(\u0026#34;deleteUser() is invoked\u0026#34;); } public String foo() { return \u0026#34;foo\u0026#34;; } } 测试字节码保存策略\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 public class ByteBuddyDemo1 { String path; @BeforeEach public void init(){ path = ByteBuddyDemo1.class.getResource(\u0026#34;\u0026#34;).getPath(); System.out.println(path); } /** * 将字节码保存到文件 */ public void saveUnloaded(DynamicType.Unloaded\u0026lt;UserService\u0026gt; unloaded, String className) throws IOException { byte[] bytes = unloaded.getBytes(); FileUtils.writeByteArrayToFile(new File(path + className), bytes); } /** * 对于生成的类的命名规则 * 1：对于jdk的类 net.bytebuddy.renamed.java.lang.Object$ByteBuddy$QORhkzsm * 2：对于自定义的类 org.importzhh.Main$ByteBuddy$vfvp1e2G * * 指定生成的类的命名策略 * org.importzhh.Main$suffix$oq9xJ5w5 */ @Test public void test1() throws Exception { // unloaded 表示生成的字节码还未加载到jvm中 DynamicType.Unloaded\u0026lt;Object\u0026gt; unloaded = new ByteBuddy() .subclass(Object.class) .make(); // net.bytebuddy.renamed.java.lang.Object$ByteBuddy$QORhkzsm byte[] bytes = unloaded.getBytes(); System.out.println(bytes.length); unloaded.saveIn(new File(path)); } @Test public void test2() throws Exception { // unloaded 表示生成的字节码还未加载到jvm中 DynamicType.Unloaded\u0026lt;UserService\u0026gt; unloaded = new ByteBuddy() .subclass(UserService.class) .make(); // org.itstack.Main$ByteBuddy$vfvp1e2G byte[] bytes = unloaded.getBytes(); System.out.println(bytes.length); unloaded.saveIn(new File(path)); } @Test public void test3() throws Exception { NamingStrategy.SuffixingRandom suffix = new NamingStrategy.SuffixingRandom(\u0026#34;suffix\u0026#34;); // unloaded 表示生成的字节码还未加载到jvm中 DynamicType.Unloaded\u0026lt;UserService\u0026gt; unloaded = new ByteBuddy() // 不校验生成的类 默认是校验的 .with(TypeValidation.DISABLED) .with(suffix) .subclass(UserService.class) .make(); // org.itstack.Main$ByteBuddy$vfvp1e2G byte[] bytes = unloaded.getBytes(); FileUtils.writeByteArrayToFile(new File(path + \u0026#34;TestMain1.class\u0026#34;), bytes); // System.out.println(bytes.length); // unloaded.saveIn(new File(path)); // 生成的字节码还可以直接注入到某个jar包中 // unloaded.inject(new File(\u0026#34;a.jar\u0026#34;)); } } 命名策略 1 2 3 4 5 6 7 8 9 10 public void createWithNameStrategy() throws IOException { NamingStrategy.SuffixingRandom suffixingRandom = new NamingStrategy.SuffixingRandom(\u0026#34;importzhh\u0026#34;); //Unloaded代表生成字节码还未加载到jvm DynamicType.Unloaded\u0026lt;CreateByConstructorStrategyEntity\u0026gt; unloaded = new ByteBuddy() .with(suffixingRandom) //指定父类 .subclass(CreateByConstructorStrategyEntity.class) .make(); unloaded.saveIn(new File(path)); } 生成的类名为CreateByConstructorStrategyEntity$importzhh$OnHkVvgk,如果不指定生成策略则生成的类名如下\n超类类名+ByteBuddy+8位随机字符\n增加SuffixingRandom则自定义的后缀将替换ByteBuddy，如果将命名策略改为PrefixingRandom则命名结果如下：\n1 package importzhh.com.importzhh.bytebuddy 也可以直接指定类名\n1 2 3 4 5 6 7 8 9 10 @Test public void createWithName() throws IOException { //Unloaded代表生成字节码还未加载到jvm DynamicType.Unloaded\u0026lt;CreateByConstructorStrategyEntity\u0026gt; unloaded = new ByteBuddy() //指定父类 .subclass(CreateByConstructorStrategyEntity.class) .name(\u0026#34;cn.importzhh.NewCreateByConstructorStrategyEntity\u0026#34;) .make(); unloaded.saveIn(new File(path)); } 自定义命名策略\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Test public void createWithSelfDefineNameStrategy() throws IOException { DynamicType.Unloaded\u0026lt;CreateByConstructorStrategyEntity\u0026gt; unloaded = new ByteBuddy() //使用自定义命名策略 .with(new NamingStrategy.AbstractBase() { protected String name(TypeDescription superClass) { return \u0026#34;com.test.\u0026#34; + superClass.getSimpleName(); } }) .subclass(CreateByConstructorStrategyEntity.class) .make(); String canonicalName = unloaded.load(this.getClass().getClassLoader()).getLoaded().getCanonicalName(); System.out.println(canonicalName); } 字节码注入jar包或指定目录 也可以将字节码注入到已有的jar中，代码和操作结果如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 /** * 从指定目录或jar包中获取类的信息 * @throws IOException */ @Test public void test9() throws IOException { //从当前目录下的jar文件中找到类文件。 ClassFileLocator classFileLocator = ClassFileLocator.ForJarFile.of(new File(\u0026#34;/Users/importzhh/.m2/repository/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar\u0026#34;)); //创建一个Compound对象 用于将多个ClassFileLocator组合在一起。这里只有一个ClassFileLocator ClassFileLocator.Compound compound = new ClassFileLocator.Compound(classFileLocator); //从compound中获取类的信息 TypePool typePool = TypePool.Default.of(compound); //获取了\u0026#34;org.apache.commons.io.FileUtils\u0026#34;这个类的描述信息，并解析它。 TypeDescription typeDescription = typePool.describe(\u0026#34;org.apache.commons.io.FileUtils\u0026#34;).resolve(); new ByteBuddy() .redefine(typeDescription, classFileLocator) .method(ElementMatchers.named(\u0026#34;sizeOf\u0026#34;)) .intercept(FixedValue.nullValue()) .make() .load(getClass().getClassLoader()); } 类加载策略 DynamicType.Unloaded，代表一个尚未加载的类，顾名思义，这些类型不会加载到 Java 虚拟机中，它仅仅表示创建好了类的字节码，通过 DynamicType.Unloaded 中的 getBytes 方法你可以获取到该字节码。\n在应用程序中，可能需要将该字节码保存到文件，或者注入的现在的 jar 文件中，因此该类型还提供了一个 saveIn(File) 方法，可以将类存储在给定的文件夹中； inject(File) 方法将类注入到现有的 Jar 文件中，另外你只需要将该字节码直接加载到虚拟机使用，你可以通过 ClassLoadingStrategy 来加载。\nClassLoadingStrategy 是 Byte Buddy 库中用于定义如何加载动态创建或修改的类的策略，内置的策略定义在枚举ClassLoadingStrategy.Default中\n这个策略决定了新生成的类应该被哪个类加载器加载，以及生成的类应该如何与现有的类关联。这里有几种策略：\nWRAPPER：这个策略表示新生成的类将被与其父类相同的类加载器加载。 在这种策略下，新生成的类不能访问到比其父类更高级的类加载器所加载的类。生成的类只能访问到其父类可以访问到的类。\nWRAPPER_PERSISTENT：这个策略与 WRAPPER 类似，但是它会在类被加载时生成一个类清单（manifest）。 这个清单可以用于在运行时获取关于类的元数据。\nCHILD_FIRST：这个策略表示新生成的类将被一个新的类加载器加载，子加载器优先负责加载目标类 这个新的类加载器将其父类的类加载器作为其父加载器。在这种策略下，新生成的类可以访问到比其父类更高级的类加载器所加载的类。\nCHILD_FIRST_PERSISTENT：这个策略与 CHILD_FIRST 类似，但是它也会在类被加载时生成一个类清单。\nINJECTION：这个策略表示新生成的类将直接被注入到其父类所在的类加载器。利用反射机制注入动态类型 这需要类加载器支持类的动态注入。这些策略的选择取决于你的具体需求。例如，如果你需要新生成的类能够访问到更多的类， 你可能需要选择 CHILD_FIRST 或 CHILD_FIRST_PERSISTENT 策略。如果你不需要这样的功能， 那么 WRAPPER 或 WRAPPER_PERSISTENT 策略可能会是更好的选择。\n1 2 3 Class\u0026lt;?\u0026gt; dynamicClass = dynamicType .load(Object.class.getClassLoader(), ClassLoadingStrategy.Default.WRAPPER) .getLoaded(); 我们使用 WRAPPER 策略来加载适合大多数情况的类，这样生产的动态类不会被ApplicationClassLoader加载到，不会影响到项目中已经存在的类getLoaded 方法返回一个 Java Class 的实例，它就表示现在加载的动态类\n增强一个类 修改方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 /** * 测试继承UserManager 拦截 queryUser 方法 并返回空值 */ @Test public void test5() throws IOException { DynamicType.Unloaded\u0026lt;UserService\u0026gt; unloaded = new ByteBuddy() .subclass(UserService.class) .name(\u0026#34;a.b.subObjType\u0026#34;) .method(ElementMatchers.named(\u0026#34;queryUser\u0026#34;)) .intercept(FixedValue.nullValue()) .make(); saveUnloaded(unloaded, \u0026#34;TestUserManager1.class\u0026#34;); } /** * 测试redefine UserManager queryUser 方法 并返回空值 */ @Test public void test6() throws IOException { DynamicType.Unloaded\u0026lt;UserService\u0026gt; unloaded = new ByteBuddy() .redefine(UserService.class) .name(\u0026#34;a.b.subObjType\u0026#34;) .method(ElementMatchers.named(\u0026#34;queryUser\u0026#34;)) .intercept(FixedValue.nullValue()) .make(); saveUnloaded(unloaded, \u0026#34;TestUserManager2.class\u0026#34;); } } 插入方法 1 2 3 4 5 6 7 8 9 10 11 12 13 /** * 测试bytebuddy向 UserManager 插入一个新方法 testAdd */ @Test public void test7() throws IOException { DynamicType.Unloaded\u0026lt;UserService\u0026gt; unloaded = new ByteBuddy() .redefine(UserService.class) .defineMethod(\u0026#34;testAdd\u0026#34;, String.class, Modifier.PUBLIC+Modifier.STATIC) .withParameter(String[].class, \u0026#34;args\u0026#34;) .intercept(FixedValue.value(\u0026#34;这是bytebuddy新增的方法\u0026#34;)) //这里是新方法的实现，这里将toString()方法返回固定字符串 .make(); saveUnloaded(unloaded, \u0026#34;TestUserManager3.class\u0026#34;); } 插入属性 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /** * 向UserManager插入一个新属性 */ @Test public void test8() throws IOException { DynamicType.Unloaded\u0026lt;UserService\u0026gt; unloaded = new ByteBuddy() .redefine(UserService.class) .defineField(\u0026#34;systemShort\u0026#34;, String.class, Modifier.PRIVATE) // 定义属性 .defineMethod(\u0026#34;getSystemShort\u0026#34;, String.class, Modifier.PUBLIC) // 定义getter方法 .intercept(FieldAccessor.ofField(\u0026#34;systemShort\u0026#34;)) // 指定getter方法返回字段的值 .defineMethod(\u0026#34;setSystemShort\u0026#34;, void.class, Modifier.PUBLIC) // 定义setter方法 .withParameters(String.class) // 指定setter方法的参数 .intercept(FieldAccessor.ofField(\u0026#34;systemShort\u0026#34;)) // 指定setter方法设置字段的值 .make(); saveUnloaded(unloaded, \u0026#34;TestUserManager4.class\u0026#34;); } 增强类 增强一个类有两种方式redefine\u0026amp;rebase，与subclass不同的是这两种方式是在原有类的基础上修改，而subclass是生成一个子类。\nrebase：会保留所有被变基类的方法实现。Byte Buddy 会用兼容的签名复制所有方法的实现为一个私有的重命名过的方法， 而不像类重定义时丢弃覆写的方法。用这种方式的话，不存在方法实现的丢失，而且变基的方法可以通过调用这些重命名的方法， 继续调用原始的方法。 redifine：允许通过添加字段和方法或者替换已存在的方法实现来修改已存在的类。 但是，如果方法的实现被另一个实现所替换，之前的实现就会丢失。 使用rebase将之前测试的实体类增加一个getId方法并返回固定值0\n1 2 3 4 5 6 7 public class CreateByConstructorStrategyEntity { ..... public Long getId() { return 0L; } ..... } 然后以rebase的方式将其修改，使getId方法的返回值修改\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Test public void dynamicEnhanceByRebase() throws IOException { DynamicType.Unloaded\u0026lt;CreateByConstructorStrategyEntity\u0026gt; unloaded = new ByteBuddy() .with(new NamingStrategy.AbstractBase() { protected String name(TypeDescription superClass) { return \u0026#34;top.eacape.\u0026#34; + superClass.getSimpleName(); } }) .rebase(CreateByConstructorStrategyEntity.class) .method(ElementMatchers.\u0026lt;MethodDescription\u0026gt;named(\u0026#34;getId\u0026#34;)) .intercept(FixedValue.value(50L)) .make(); unloaded.saveIn(new File(path)); } 用idea直接查看没有什么问题，但是查看字节码发现这个文件中还有一个以getId开头的方法，它的返回值为0，代表getId的原始方法 反观使用redefine就没有这种效果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 /** * rebase方式 * * 这种方式表示的是在一个类的方法上重新定义实现（就像是在替换方法中的字节码一样），但保留原来方法的字节码，并且在新方法中调用。 */ public class ByteBuddyDemo2Rebase { @Test public void test1() throws Exception { Class\u0026lt;?\u0026gt; dynamicType = new ByteBuddy() .rebase(UserService.class) .method(ElementMatchers.named(\u0026#34;foo\u0026#34;)) .intercept(MethodDelegation.to(InterceptorClass.class)) .make() .load(ByteBuddyDemo2Rebase.class.getClassLoader()) .getLoaded(); Object instance = dynamicType.newInstance(); System.out.println(dynamicType.getDeclaredMethod(\u0026#34;hello\u0026#34;).invoke(instance)); } public static class InterceptorClass { public static String foo(@SuperCall Callable\u0026lt;String\u0026gt; zuper) throws Exception { System.out.println(\u0026#34;Before invoking original method\u0026#34;); String result = zuper.call(); System.out.println(\u0026#34;After invoking original method\u0026#34;); return result; } } } 匹配方法 委托方法 在大多数情况下，方法返回一个固定值当然是不够的。为了更好的灵活性，Byte Buddy 提供了MethodDelegation(方法委托)实现， 它在对方法调用做出反应时提供最大程度的自由。一个方法委托定义了动态创建的类方法，到另外一个可能存在于动态类型之外的方法的任何调用。 这样，动态类的逻辑可以用简单的 Java 表示，仅通过代码生成就可以与另外的方法绑定。\n实现委托方法有两种，一种是静态方法委托，一种是成员方法委托\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 import net.bytebuddy.ByteBuddy; import net.bytebuddy.implementation.MethodDelegation; import net.bytebuddy.implementation.bind.annotation.*; import net.bytebuddy.matcher.ElementMatchers; import org.itstack.UserService; import org.junit.jupiter.api.Test; import java.lang.reflect.Method; import java.util.concurrent.Callable; public class ByteBuddyDemo3Delegation { /** * 在这个例子中，原始的foo方法被插桩并替换为新的MethodInterceptor.intercept方法。 * 这就是使用ByteBuddy对实例方法进行插桩的一种基本方式。 * * 方法委托 与被拦截方法同签名 且是静态方法 */ @Test public void test1() throws Exception{ Class\u0026lt;?\u0026gt; dynamicType = new ByteBuddy() .subclass(UserService.class) .method(ElementMatchers.named(\u0026#34;foo\u0026#34;)) .intercept(MethodDelegation.to(MethodInterceptor.class)) .make() .load(getClass().getClassLoader()) .getLoaded(); Object instance = dynamicType.getDeclaredConstructor().newInstance(); Method foo = dynamicType.getMethod(\u0026#34;foo\u0026#34;); String result = (String) foo.invoke(instance); System.out.println(result); // prints \u0026#34;intercepted\u0026#34; } /** * 在这个例子中，原始的foo方法被插桩并替换为新的MethodInterceptor.intercept方法。 * 这就是使用ByteBuddy对实例方法进行插桩的一种基本方式。 * * 方法委托 与被拦截方法同签名 且是成员方法 */ @Test public void test2() throws Exception{ Class\u0026lt;?\u0026gt; dynamicType = new ByteBuddy() .subclass(UserService.class) .method(ElementMatchers.named(\u0026#34;queryUser\u0026#34;)) .intercept(MethodDelegation.to(new MethodInterceptor2())) .make() .load(getClass().getClassLoader()) .getLoaded(); Object instance = dynamicType.getDeclaredConstructor().newInstance(); Method queryUser = dynamicType.getMethod(\u0026#34;queryUser\u0026#34;, String.class); String result = (String) queryUser.invoke(instance, \u0026#34;testUserId\u0026#34;); System.out.println(result); // prints \u0026#34;intercepted\u0026#34; } @Test public void test3() throws Exception{ Class\u0026lt;?\u0026gt; dynamicType = new ByteBuddy() .subclass(UserService.class) .method(ElementMatchers.named(\u0026#34;queryUser\u0026#34;)) .intercept(MethodDelegation.to(new MethodInterceptor3())) .make() .load(getClass().getClassLoader()) .getLoaded(); Object instance = dynamicType.getDeclaredConstructor().newInstance(); Method queryUser = dynamicType.getMethod(\u0026#34;queryUser\u0026#34;, String.class); String result = (String) queryUser.invoke(instance, \u0026#34;testUserId\u0026#34;); System.out.println(result); // prints \u0026#34;intercepted\u0026#34; } public static class MethodInterceptor { public static String intercept() { return \u0026#34;intercepted\u0026#34;; } } public static class MethodInterceptor2 { public String intercept(String userId) { return \u0026#34;intercepted：\u0026#34;+userId; } } public class MethodInterceptor3{ /** * 方法签名或返回值可以与被拦截方法不一致 * @RuntimeType bytebuddy会在运行期间给被指定注解修饰的方法参数进行赋值 * @param targetObj 表示被拦截的目标对象 只有拦截实例方法时可用 * @param targetMethod 表示被拦截的目标方法 只有拦截实例方法或静态方法可用 * @param targetMethodArgs 目标方法的参数 * @param targetObj2 被拦截的目标对象 只有拦截实例方法时可用 * 若确定父类 也可用具体的父类来接收 * @param zuper 调用目标方法 * 1. `@RuntimeType`: 这是一个用于方法的注解，用于告知Byte Buddy，此方法在运行时将会被调用。 * 该注解通常用于拦截器类的方法，使其成为方法委托的目标。 * 2. `@This`: 该注解用于注入当前被拦截方法所属的对象（即目标对象）。在拦截器方法中，通过`@This`注解， * 您可以获取目标对象的引用，并在拦截器中使用它。 * 3. `@Origin`: 该注解用于注入当前被拦截方法的`Method`对象，代表目标方法的反射信息。通过`@Origin`注解， * 您可以获取目标方法的名称、参数列表、返回类型等信息。 * 4. `@AllArguments`: 该注解用于注入当前被拦截方法的所有参数。在拦截器方法中，通过`@AllArguments`注解， * 您可以获取目标方法的所有参数作为一个对象数组。 * 5. `@Super`: 该注解用于注入目标方法的父类或父接口实例。通常用于拦截器方法中，使得您可以调用目标方法，而不仅仅是拦截它。 * 6. `@SuperCall`: 该注解用于注入一个`Callable`对象，代表目标方法的调用。在拦截器方法中，通过`@SuperCall`注解， * 您可以手动调用目标方法，实现方法委托。 */ @RuntimeType public Object testDelegation( @This Object targetObj, @Origin Method targetMethod, @AllArguments Object[] targetMethodArgs, @Super Object targetObj2, @SuperCall Callable\u0026lt;?\u0026gt; zuper) throws Exception { System.out.println(\u0026#34;Before calling method: \u0026#34; + targetMethod.getName()); Object call = zuper.call(); System.out.println(\u0026#34;After calling method: \u0026#34; + targetMethod.getName()); return call; } } } 上面的委托方式看起来还是不够灵活，日常最常使用的委托方式是通过注解进行参数绑定。\n注解 说明 @Argument 绑定单个参数 @AllArguments 绑定所有参数的数组 @This 当前被拦截的、动态生成的那个对象 @Super 当前被拦截的、动态生成的那个对象,不会继承原有的类 @Origin 可以绑定到以下类型的参数： - Method 被调用的原始方法 - Constructor 被调用的原始构造器 - Class 当前动态创建的类 - MethodHandleMethodTypeString 动态类的toString()的返回值 - int 动态方法的修饰符 @DefaultCall 调用默认方法而非super的方法 @SuperCall 用于调用父类版本的方法 @RuntimeType 可以用在返回值、参数上，提示ByteBuddy禁用严格的类型检查 @Empty 注入参数的类型的默认值 @StubValue 注入一个存根值。对于返回引用、void的方法，注入null；对于返回原始类型的方法，注入0 @FieldValue 注入被拦截对象的一个字段的值 @Morph 类似于@SuperCall，但是允许指定调用参数 注意第三种注解方式和之前的委托方法同时存在时，会优先选择前者。\n动态修改参数 前面示例中，使用 @SuperCall 注解注入的 Callable 参数来调用目标方法时，是无法动态修改参数的，如果想要动态修改参数，则需要用到 @Morph 注解以及一些绑定操作，示例如下：\nInterceptor 会使用 @Morph 注解注入一个 OverrideCallable 对象作为参数，然后通过该 OverrideCallable 对象调用目标方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 import net.bytebuddy.ByteBuddy; import net.bytebuddy.dynamic.loading.ClassLoadingStrategy; import net.bytebuddy.implementation.MethodDelegation; import net.bytebuddy.implementation.bind.annotation.AllArguments; import net.bytebuddy.implementation.bind.annotation.Morph; import net.bytebuddy.implementation.bind.annotation.RuntimeType; import net.bytebuddy.matcher.ElementMatchers; import org.junit.jupiter.api.Test; import java.lang.reflect.Method; import java.util.Arrays; public class ByteBuddyDemo5Delegation { public static class MyInterceptor { @RuntimeType public Object modifyParameters( @AllArguments Object[] args, @Morph MyCallable zuper) { System.out.println(\u0026#34;Original arguments: \u0026#34; + Arrays.toString(args)); for (int i = 0; i \u0026lt; args.length; i++) { if (args[i] instanceof String) { args[i] = ((String) args[i]).toUpperCase(); } } System.out.println(\u0026#34;Modified arguments: \u0026#34; + Arrays.toString(args)); Object call = zuper.call(args); return call; } } /** * 测试动态修改方法参数 * @param args * @throws Exception * * `ClassLoadingStrategy` 是 Byte Buddy 库中用于定义如何加载动态创建或修改的类的策略。这个策略决定了新生成的类应该被哪个类加载器加载，以及生成的类应该如何与现有的类关联。这里有几种策略：1. `WRAPPER`：这个策略表示新生成的类将被与其父类相同的类加载器加载。在这种策略下，新生成的类不能访问到比其父类更高级的类加载器所加载的类。2. `WRAPPER_PERSISTENT`：这个策略与 `WRAPPER` 类似，但是它会在类被加载时生成一个类清单（manifest）。这个清单可以用于在运行时获取关于类的元数据。3. `CHILD_FIRST`：这个策略表示新生成的类将被一个新的类加载器加载，这个新的类加载器将其父类的类加载器作为其父加载器。在这种策略下，新生成的类可以访问到比其父类更高级的类加载器所加载的类。4. `CHILD_FIRST_PERSISTENT`：这个策略与 `CHILD_FIRST` 类似，但是它也会在类被加载时生成一个类清单。5. `INJECTION`：这个策略表示新生成的类将直接被注入到其父类所在的类加载器。这需要类加载器支持类的动态注入。这些策略的选择取决于你的具体需求。例如，如果你需要新生成的类能够访问到更多的类，你可能需要选择 `CHILD_FIRST` 或 `CHILD_FIRST_PERSISTENT` 策略。如果你不需要这样的功能，那么 `WRAPPER` 或 `WRAPPER_PERSISTENT` 策略可能会是更好的选择。 */ @Test public void test(String[] args) throws Exception { Class\u0026lt;?\u0026gt; dynamicType = new ByteBuddy() .subclass(MyTargetClass.class) .method(ElementMatchers.named(\u0026#34;process\u0026#34;)) // 要用@Morph注解之前，需要通过 Morph.Binder 告诉 Byte Buddy // 要注入的参数是什么类型 .intercept(MethodDelegation.withDefaultConfiguration() .withBinders(Morph.Binder.install(MyCallable.class)) .to(new MyInterceptor())) .make() .load(ByteBuddyDemo5Delegation.class.getClassLoader(), ClassLoadingStrategy.Default.INJECTION) .getLoaded(); MyTargetClass myInstance = (MyTargetClass) dynamicType.getDeclaredConstructor().newInstance(); Method queryUser = dynamicType.getMethod(\u0026#34;process\u0026#34;, String.class, String.class); queryUser.invoke(myInstance, \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;); } public static class MyTargetClass { public void process(String str1, String str2) { System.out.println(\u0026#34;Processing: \u0026#34; + str1 + \u0026#34; \u0026#34; + str2); } } public interface MyCallable { Object call(Object[] args); } } 构造器方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import net.bytebuddy.ByteBuddy; import net.bytebuddy.dynamic.DynamicType; import net.bytebuddy.implementation.MethodDelegation; import net.bytebuddy.implementation.SuperMethodCall; import net.bytebuddy.implementation.bind.annotation.RuntimeType; import net.bytebuddy.implementation.bind.annotation.This; import net.bytebuddy.matcher.ElementMatchers; import org.itstack.UserService; import org.junit.jupiter.api.Test; public class ByteBuddyDemo6 { /** * 测试拦截构造方法 */ @Test public void constructorDelegation() throws Exception { DynamicType.Unloaded\u0026lt;UserService\u0026gt; unloaded = new ByteBuddy() .subclass(UserService.class) .constructor(ElementMatchers.any()) .intercept( //在构造器方法执行完成之后进行拦截 SuperMethodCall.INSTANCE.andThen( MethodDelegation.to(new MethodInterceptor4()) ) ) .make(); Class\u0026lt;?\u0026gt; loaded = unloaded.load(this.getClass().getClassLoader()).getLoaded(); loaded.newInstance(); } public class MethodInterceptor4{ @RuntimeType public void constructorAspect( @This Object target ) { System.out.println(target + \u0026#34;：实例化后置操作\u0026#34;); } } } 常用核心API ByteBuddy 流式API方式的入口类 提供Subclassing/Redefining/Rebasing方式改写字节码 所有的操作依赖DynamicType.Builder进行,创建不可变的对象 ElementMatchers(ElementMatcher) 提供一系列的元素匹配的工具类(named/any/nameEndsWith等等) ElementMatcher(提供对类型、方法、字段、注解进行matches的方式,类似于Predicate) Junction对多个ElementMatcher进行了and/or操作 DynamicType(动态类型,所有字节码操作的开始,非常值得关注) Unloaded(动态创建的字节码还未加载进入到虚拟机,需要类加载器进行加载) Loaded(已加载到jvm中后,解析出Class表示) Default(DynamicType的默认实现,完成相关实际操作) Implementation(用于提供动态方法的实现) FixedValue(方法调用返回固定值) MethodDelegation(方法调用委托,支持两种方式: Class的static方法调用、object的instance method方法调用) Builder(用于创建DynamicType,相关接口以及实现后续待详解) MethodDefinition FieldDefinition AbstractBase 链接 https://bytebuddy.net/#/tutorial-cn\n","date":"2023-08-06T17:06:29+08:00","permalink":"https://blog.importzhh.me/p/i-bytebuddy%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/","title":"I: ByteBuddy入门笔记1"},{"content":"OnMethodEnter Advice 先编写一个入门示例 项目源码结构如下 pom.xml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 \u0026lt;project\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.8.1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;net.bytebuddy\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;byte-buddy-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.14.5\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;transform\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;transformations\u0026gt; \u0026lt;transformation\u0026gt; \u0026lt;plugin\u0026gt;com.importzhh.InterceptorPlugin\u0026lt;/plugin\u0026gt; \u0026lt;/transformation\u0026gt; \u0026lt;/transformations\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; DataProducer.java\n1 2 3 4 5 6 7 8 9 10 11 package com.importzhh; public class DataProducer { public DataProducer() { } public void create() { System.out.println(\u0026#34;Method start\u0026#34;); System.out.println(\u0026#34;create data\u0026#34;); } } LogInterceptor.java\n1 2 3 4 5 6 7 8 9 10 package com.importzhh; import net.bytebuddy.asm.Advice; public class LogInterceptor { @Advice.OnMethodEnter public static void start() { System.out.println(\u0026#34;Method start\u0026#34;); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package com.importzhh; import net.bytebuddy.asm.Advice; import net.bytebuddy.build.Plugin; import net.bytebuddy.description.type.TypeDescription; import net.bytebuddy.dynamic.ClassFileLocator; import net.bytebuddy.dynamic.DynamicType; import net.bytebuddy.matcher.ElementMatchers; import java.io.IOException; public class InterceptorPlugin implements Plugin { @Override public DynamicType.Builder\u0026lt;?\u0026gt; apply(DynamicType.Builder\u0026lt;?\u0026gt; builder, TypeDescription typeDescription, ClassFileLocator classFileLocator) { return builder.visit(Advice. to(LogInterceptor.class). on(ElementMatchers.named(\u0026#34;create\u0026#34;))); } @Override public void close() throws IOException { System.out.println(\u0026#34;InterceptorPlugin close method\u0026#34;); } @Override public boolean matches(TypeDescription target) { System.out.println(\u0026#34;Inspecting\u0026#34; + target.getName()); if (target.getName().equals(DataProducer.class.getName())) { System.out.println(\u0026#34;Found target code : \u0026#34; + target.getName()); return true; } else { System.out.println(\u0026#34;Inspected code\u0026#34; + target.getName() + \u0026#34;is not the target code\u0026#34;); return false; } } } App.java\n1 2 3 4 5 6 7 8 9 10 11 package com.importzhh; /** * Hello world! */ public class App { public static void main(String[] args) { System.out.println(\u0026#34;Hello World!\u0026#34;); new DataProducer().create(); } } 在InterceptorPlugin中定义了字节码转换规则（即为 DataProducer.create 方法添加日志），接下来的步骤是编译并测试这个插件。\n编译：你可以使用Maven进行编译，确保整个项目没有语法错误和依赖问题。在项目的根目录运行以下命令进行编译： 1 mvn clean install 运行：InterceptorPlugin 定义了对 DataProducer.create 方法的转换逻辑。 在编译阶段，ByteBuddy Maven插件应该会修改 DataProducer 类的字节码，并为 create 方法添加 \u0026ldquo;Method start\u0026rdquo; 的输出。因此，调用 DataProducer.create 应该能看到这个输出。\n运行这个 App.java 测试，如果一切正常，那么在控制台中应该看到 \u0026ldquo;Method start\u0026rdquo; 的输出。\n1 2 3 Hello World! Method start create data ","date":"2023-08-06T17:06:29+08:00","permalink":"https://blog.importzhh.me/p/ii-bytebuddy%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B02/","title":"II: ByteBuddy入门笔记2"},{"content":"\r理性 or 感性？\n孤独 自由 无意义 死亡 在这个竞争加剧的环境里，我们不得不更加强烈的直面内心的冲突，一些信念会被消解掉。\n释迦牟尼说人有八苦，分别是生、老、病、死、怨憎会、爱别离、五阴炽盛、求不得。\n生、老、病、死，是自然生理上的痛苦；怨憎会、爱别离、五阴炽盛和求不得，是精神上的痛苦。\n怨憎会，就是和怨恨、憎恶的人或事物在一起，无法摆脱，是一种痛苦；\n爱别离，和自己亲爱的人分离，是一种痛苦；\n五阴炽盛,五阴即色受想行识。‘炽’，火热也。盛，众多也。阴，障蔽也。是说这五种法能障蔽吾人本具妙觉真心，使之不得显现。\n求不得，想得到的东西却总是得不到，又是一种痛苦。\n人的本质是孤独 有研究表明 伴侣对生活的满意程度只有在结婚前几个月呈现上升状态 之后很快下降 因为当你离一个人越近 就越容易和她发生冲突 争吵，当你认为一段关系可以缓解某些事情的时候， 实际上你是把这段关系当做了你的工具 ，结婚成了你的一个目标， 当你对一个人一件事 期待太多 就会要求太多 控制太多 依赖太多 反而你会把对方越推越远， 你需要认识到，所有人和你一样承受着孤独的不可摆脱 脆弱的承受着。 你需要珍惜的是，在一段关系中看见彼此的价值。你不应该追求走向山顶的目标 你应该追求每个瞬间不断起舞的刹那。 如果你缺乏社交圈，多去尝试一些陌生的事物，拓展自己的兴趣，聚集一些有同样兴趣的人，建立自己的朋友圈、可以理解为流量池、亦或是做事情有根基。开始，做一个有趣的人吧。\n自由： 关于学习的一个例子。我总觉得学习不能带着功利心，我学习是为了充实大脑，是为了增长学识，不是为了实现某种成就，更不是为了赚更多钱，然后我的外在表现就是学得很佛，目的性不强，针对性不高，能让我好奇的我都愿意去学， 完全不觉得是浪费时间，同时对很多东西都三分钟热度， 因为我的感性不会为了学习成绩不好而感到害怕，同时我的理性也在告诉自己并不是成绩越好，就越有钱，不是越有钱就能越幸福，很多有钱人其实压力很大，活的并不快乐， 所以我学习的目的只是为了满足好奇心，充实生活，一旦学习无聊起来就不会强迫自己痛苦挣扎了，外在表现就是行动力很差， 但通过视频中的这个观点让我有了一个对我这种情况的一个新的理解，虽然理性总是告诉自己就是努力了也不一定成功，成功不一定有钱，有钱不一定幸福，但 是我们还是要感性地相信努力就一定会变得更好（哪怕这件事本身不一定），我问我爷爷为什么每天那么辛苦地去种地， 他们相信种瓜得瓜，他们的行动力是最强的，他们的信念是谁都撼动不了的， 建议从学习的功利性和非功利性来看待现在面对的问题，学习的非功利性是启迪智慧，丰富认知，满足好奇，这一层意义不仅驱动一个人的学生时代，更驱动一个人终身学习， 但在高考赛道上，在优绩主义的现实下，学习的功利性是无可逃避的，从这一层讲，把阶段性的学习当一个项目去做， 行动力，执行力，自律性，应试技巧等等都是要有的，不要用非功利性来试图解释自己的松懈和懒散，这两层意义其实不矛盾，而且学习上取得的成绩大部分都伴随着一定的痛感。\n无意义:\n十字军主义：盲目的希望投身于大事件 确不知道这些大事件到底为了什么 打着正义的旗号 用自己的道德框架审判别人 虚无主义: 伪装成清醒和成熟的姿态 一副看透人世间 人间不值得 一切都是空 强迫性活跃: 把社会强塞给人的东西当做自己的追求, 而没有自己的思考 我们常常会认为是过去影响了现在，斩不断的关系、治不好的童年、看不清的未来。 但过去和现在之间 忽略了 你的目的 当你想做某件事的时候 你会认为童年创伤提高了你的抗挫折能力。当你不想做某件事情的时候 你会认为童年创伤阻碍了你的行动。 特别是当你遇到困难或难以突破自己心理的时候，会本能的去选择逃避，因为你还没有做好充分的心理建设，去接受曾经失败的体验，仍然在抗拒或陷入习得性无助的状态。 下文会再次提到人本主义心理学，必须要垮过的第一道坎：因缺乏勇气而产生的自卑心理。\n有时候我们拿出所谓的理性，却是帮助一个感性的想法合理化。 对于很多人来说，感性是前置于理性的，所谓理性的行为，只不过是对预先存在的态度加以合理化罢了。理性不是帮助他们寻找答案的 而是帮助他们作证直觉上已经认可的答案的。\n如果你认为你有治不好的童年，你可以尝试回溯你的过往，从出生到现在，在你的身上发生了什么，你的关键事件是什么，转折点是什么，你是如何一步一步走到今天的。 如果你没有办法一下子写出来，你可以慢慢回忆，你感觉好或者不好的事情，或者记忆到的事情，来慢慢整理。 慢慢的你或许能意识到你到底是被什么影响了。你可以试着问自己，这个事件还有另外一种解释框架吗？ 很多心理影响会通过仪式隐喻的方式来影响，通过书写让自己梳理察觉意识到无奈的来源，并尝试找到另外一种应对的出口。\n要区分对一件事是描述还是评判，需要觉知，觉察到条件反射式的反应、习惯的思维误区等。 当你感觉不好的时候 甚至无法描述出自己的感觉 不知道自己的需求 困境 忧虑是什么。 先深呼吸提醒自己觉察。先不做评判，回溯刚刚的感受，不需要很有条理，不需要有什么结论，记录好即可。 吃 听 看 闻 摸 某样东西的感受，打开感官系统。 收集到体验和线索后，分析发生了什么 为什么会发生。\n条条大路通罗马 如果你走的太艰难 也许你应该换一个方法，找对适合自己的黄金方法，增强自我效能感。 需要自己对使用的方法做一个合理的评估 抛弃掉意志力等反人性的东西，将依赖意志力转变为依赖习惯 转变为依赖系统 评估你的习惯 你在习惯上花费的时间可能超乎你的想象，有些习惯可能不易察觉。目标拆解？ 推荐阅读 掌控习惯这本书 把关注圈 转移到影响圈 把发生了什么转移到我能做什么上面去，重要的不是被给予什么 而是你如何利用被给予的东西。\n感受型思维的特点：\n我知道你说的对 但是我就是觉得不舒服 所以我不听 厌恶使用脑力 需要动脑的地方都会有情绪 情绪大于内容 不高兴了不说 因为她自己也知道生气的理由是无理取闹 有需求不说出来希望你懂 但是不生气她又感受到委屈 所以需要你去猜 需要你去处理她的负面情绪 喜欢猜测 惯于按照自己的意思去解读他人的一些行为和语言，容易造成误解 人在有所求的时候状态会是什么样的？\n不自然 一种索取的姿态暴露需求感 或者 情感表达不流畅 没法正确表达自己的情感 (注意这里的喜欢欣赏 和 想得到 之间心态的差别导致的行为不同)\n想要试探 比如说开口说正事的时候 会铺垫一大堆 绕来绕去你要干嘛？没有把握的事情不敢去做，缺乏勇气与坚定。或者 想要一个确定的答复才敢走下一步。 亦或者你太容易放大负面反馈，缺乏积极的心态，放大微弱的机会。 大多数时间其实人的态度是不断变化的，太急于一个态度，大多数人都会很烦。即使反馈的事情变数也多，即使同样是一件事，在不同的时间不同的场景下，造成的结果可能完全不一样， 我们需要为多件事的这些情况提供容错。\n过度的防御心理 在意对方对你的看法与评价 生怕自己说错话做错事 内心没有脱敏 这当中有一直担心结果不好 或者 心随境变 或者 人际防御过重 当你的防御心理过重的时候，做一件事情的目的理由，就不再是讨论问题本身了，不是解决问题了，而是为了捍卫自尊。一旦陷入自尊陷阱，就会转换到对抗思维，而不是双赢思维。得出的理由结论往往是基于自己的角度， 而不是从系统全局，无法考虑对一个系统会产生什么影响。\n回应不积极或回应过于积极 不积极因为一直在考虑怎么说怎么做合适 而这个合适可能只是自己觉得合适 过于积极一个普通攻击你把大都交了\n自卑与超越这本书，讲了一种状态 说越是自卑的人 越喜欢去寻找优越感 寻找到这个优越感之后呢 又拼命去维护这个优越感 所以形成一种僵硬的自尊 拼命去维护这个优越感 就是僵硬的自尊嘛 越是有这种僵硬自尊的人 你就会发现越难跟别人处好关系 因为你的情绪不自然 你总是怎么样 总是以这种自己其实内心极度自卑 但是呢希望在跟别人在一起的时候 又表现的极度膨胀 跟个刺猬一样 就总是把别人刺的很难受 然后所有人都不愿意跟你来往 所以你越想跟人接近 越是觉得没有办法 因为你自尊很僵化嘛 你觉得人家这个什么一点点对你的不敬 一点点对你的冷漠 一点点对你怎么样 就是对你的否定 然后自己在那里情绪化 在那里纠结 就觉得啊我再跟你聊 我再跟你怎么样 我就是舔狗啊 怎么样怎么样的\n我们说这叫什么 这叫心随境转 有时候浮躁跟懦弱是同时会出现在一个人身上 就因为你太心随境转了 人家稍微有一点点这个反应不好 稍微有点怎样 就觉得这是不是对你没意思 我上赶着这个没劲对吧 这不是有个性 这是懦弱 这是为你的懦弱找借口 其实就是你想逃 你找不到好的借口 然后呢给自己找一个借口 你实际上就是觉得这个事情你压力扛不住 然后你想逃吗 你没有耐心包容 没有自己的坚持 心理防御机制会导致人们遇到压力时想逃避，人本主义心理学强调的开放性和接受经验的重要性，我们需要勇敢面对困难，不要逃避，接受无常，坚持不懈，增强自我效能感 在结果已定的情况下，反思和积极的意义更重要，同时要在情绪上寻找积极的意义，以及通过努力和心态的完善来成为更好的自己。\n心要在事上磨 把心里面两个东西磨掉 一个是浮躁 一个是焦虑 当人遇到急性压力的时候会浮躁 当人遇到慢性压力的时候会焦虑 这两者都是什么 都是你强行的去试图控制 你控制不了的事情 你才会浮躁 才会焦虑 越是困难的时候越要冷静 但是人性就是相反的 就是越困难的时候 往往越干不理智的事情 大家有没有这样的体验 当女孩跟你倾诉或者生你气的时候 你喜欢跟她讲道理 或者说另外一种情况是你只会说对不起 最糟糕的情况就是你跟她吵起来哈 为什么会出现这个问题 为什么你会跟她讲道理 或者说你只会说对不起 不是因为你是对的 是因为你只想快速搞定这件事情 在这种时候 你没有去听别人为什么生气 没有去尊重别人的感受 就当你跟他讲道理的时候 实际上是什么 是你在否定别人的感受\n为什么 有的时候我们很反感 在我们自己不高兴或者说生气的时候 别人跟自己讲道理的对吧 有的时候比如说你要去干某件事情的时候 你朋友站出来劝你说 你不要干这个事儿 你会什么感觉 再或者在面对这种女性的抗拒戒备或者各种情绪化的时候 大家最容易出现问题是什么 是不是上头啊 心灵一急躁 脾气一来对吧 然后怎么样就开始发生冲突 然后你心里面想的全是你自己的道理 就是人家对你表现出一种较强的攻击性的时候 抗拒性的时候 甚至戒备心的时候 对你发脾气啊 冷嘲热讽或者怎么样 这种反正让你很难受的这种感受的时候 心理上是不是特别容易急啊 这个时候要怎么样 心理上不能急躁 态度要柔和 要给予对方包容\n第二个呢在面对女生犹豫不决 模棱两可 拿不定主意的时候 你心理上不能退缩 态度要坚定 给予对方信心 就比如说你跟女生聊天的时候 女生不理你 对你态度冷淡啊 或者说呢就是敷衍你 然后你找她的时候 她好像是说哎我明天有事 我后天怎么样 反正就是给你态度很委婉 这个时候呢是不是大多数人觉得哎呀 人家都已经对你没意思了 你还去跟人家搞这个没劲对吧 这个时候怎么样 要不就算了吧 换一个吧对吧 你看很多人会这么说。这种情况下处理压力的能力决定了你的高度。 你需要重新审视自己并接受自己的不足后，能更豁达的接受不同的结局。\n再比如说某个up主 就是他一天到晚跟这个评论底下这个喷子撕逼 你怎么看这个博主 不管什么事 他只要怎么样 只要开始跟人家祖安话了 你是不是觉得这个人很low啊 就像博主跟粉丝一样 你要表现出一个能够去包容的这个气量 第二个呢 我们来看当底下的评论表现出犹豫不决 不知所措的时候 比如说评论区说这个女生我要去表白或者怎么样的 我们有时候不是说一定会给出这个 具体的一个建议啊 或者说什么样的 我一般就说没有关系啊 不管怎么样 我们会陪着你 你可以勇敢的走出去 没事我们一直会支持你的啊 要帮助别人去坚定自己的内心 就当有的时候别人犹豫啊 不果断啊 然后不知所措的时候 你要去鼓励和支持对方 然后呢要帮助别人能够去下决心 然后呢要给别人一些信心 给别人一些力量\n前段时间《长安十二时辰》热播，正好说一说贺知章。贺知章有一首诗叫做《答朝士》，背后有个小故事：他和顾况都来自南方（今天的浙江)，同朝官僚羡慕他们的才华，借着他们口音，嘲笑他们是”南金复生中土“[1]。\n贺知章于是就写了一首诗”鈒镂银盘盛蛤蜊，镜湖莼菜乱如丝。乡曲近来佳此味，遮渠不道是吴儿。“\n先是讲了讲南方的名菜蛤蜊和莼菜，京城里大家都很爱吃，然后结尾指出，好吃就行，随便人家说不说佳肴原材料的产地。言下之意：好东西就是好，大家喜欢即可，出自哪里，别人说不说来自南方根本不重要。\n顾况为这事也写了类似的一首诗，前两句类似贺的，后两句则不同”汉儿女嫁吴儿妇，吴儿尽是汉儿爷。“\n意思说南方人娶了北方女子当媳妇，所以南方人都是北方人的爹。顾况的诗有那么点阿Q的感觉，“挨了打”却说是儿子打了老子，在面子上死磕，不似贺知章的诗轻松平淡。\n贺知章知道自己的才华，既然别人都说了“金”，多少是一种肯定，至于这肯定和地域黑的混合“南金”，他也分得清什么是重要的，什么根本无所谓。\n要写出像贺知章那样的诗，说到底还是内心的修炼。很多时候，受伤是因为我们对自己的认识不够清晰、全面和稳定。\n一旦别人质疑/打击/嘲讽了自己，就仿佛自己真的做得不够好，有问题，但事实上并非如此。\n如果有一份稳定的自我评价的话，自己的心绪就不会轻易被他人不够言辞左右。\n要形成相对清晰、全面和稳定的自我认识并不是一夕之功。\n首先，它不是盲目自信或者自高自大，有时候人被夸了两句或取得了小小成就会忘乎所以，这反而会屏蔽真正的自我认识， 需要时不常提醒自己保持清醒；\n其次，真正认识自己需要实践也需要时间。因为人是一个非常复杂而又独特的个体，有很多个面，有优点也有缺点。哪怕是自己的一个特色，要认识清楚也需要一个过程（比如，在场合A可能特别招人待见，但是到了场合B又会不遭人喜欢），更何况人是那么多特色的综合。\n最后，还需要一定的自我接纳。全面的自我认识其实是看到“原来自己并没有那么好”，也是有一份遗憾得慢慢消化，并不是嘴上说接纳就真的接纳了。总之，”认识你自己“这事得慢慢做，一点一点的积累，没有捷径。\n至于遇到质疑/打击/嘲讽的当下，该怎么做，也想和大家分享两点。\n首先，不要太执着于所谓正确的做法和漂亮的回应。题主关心的”谦逊但不卑微“，关键还在于内心的修养——知道自己存在一些问题和特色（谦逊），但同时也知道自己并没有那么差（不卑微）。\n再举个例子吧：有一些佛教徒会说如果有人谩骂了你要对那个人顶礼膜拜，因为ta帮着打破了”我执“。这听上去不可思议，假如我让一个人这么去做的话十有八九会有反效果。\n但假如那个被骂的人自然而然这么做了，顺着这个机会精进了修为，那他人看到的“卑微”不过是表象而已。况且如果有人刻意挑衅却得知自己根本伤害不到别人，也会很挫败。\n当然，这个看似极端的例子我并不推荐，只是用它来说明持守内心的重要，太过关注行为反而跑偏。至于话怎么说事怎么办，每个人都有自己的人际风格，每个互动也都有它的特殊性，给出具体的做法没有大的意义。\n其次，不必强求自己一下子就境界很高，从中试着找找契机就好。既然内心的修炼是一个过程，那么遇到质疑也不必刻意让自己云淡风轻。\n虽然我并不认同质疑/打击/嘲讽这样的行为，但某一些自己感到憋屈的瞬间，也在不经意间打开了一扇窗户，是个契机。正好可以问问自己，为什么感到憋屈？是对方的哪一句话特别刺耳？\n到底伤到了自己哪一块？也算是对自己的认识查漏补缺。如果自己一时无解那就留给时间。\n就像上文说到的顾况，他在地域出身上有道过不去的坎，反而越陷越深。但也许写诗之后的某一天，他读着自己的旧作忽然就明白了自己当初为何执迷。\n每个人存在什么问题有待改进，以及哪些地方其实已经不错了保持就好，本来就是在人际互动中慢慢获得答案的。当下发现不了的，也许在未来会遇到新的契机，保持敞开最终总会有收获的。\n如何谦逊而不卑微？内心戏稳了，很多东西就可以迎刃而解。愿我们都能把握好契机，\n在自我认识的道路上不断前进，就像贺知章的另一首诗那样 “离别家乡岁月多，近来人事半消磨。 惟有门前镜湖水，春风不改旧时波。”\n在信息爆炸的时代，如果我问你你不信什么，你可能会列出一个长长的清单，如果我问你相信什么呢？\n人生有丰富的事情需要去理解，人要不断拓展自己的认知边界。\n最后以马尔克斯在自传中写到的一句话作为结尾: \u0026ldquo;我年轻过，落魄过，幸福过，我对生活一往情深\u0026rdquo;。\n","date":"2023-07-15T12:44:47+08:00","permalink":"https://blog.importzhh.me/p/b%E7%AB%99up-%E5%BD%AD%E6%98%A5%E8%8A%B1%E6%9C%AC%E4%BA%BA-%E5%BF%83%E7%90%86%E5%AD%A6%E7%AC%94%E8%AE%B0/","title":"b站up 彭春花本人 心理学笔记"},{"content":"图示说明 代表着主机 代表服务器 代表着路由器 代表着网络 计算机网络在信息时代的作用 计算机网络已由一种通信基础设施发展成为一种重要的信息服务基础设施 计算机网络已经像水，电，煤气这些基础设施一样，成为我们生活中不可或缺的一部分 我国互联网发展状况 中国互联网络信息中心CNNIC 网络、互连网（互联网）和因特网 网络：网络（Network）由若干结点-Node和连接这些结点的链路-Link组成。 互连网（互联网）：多个网络通过路由器互连起来，这样就构成了一个覆盖范围更大的网络，即互连网（互联网）。因此，互联网又称为“网络的网络（Network of Networks）”。 因特网：因特网（Internet）是世界上最大的互连网络（用户数以亿计，互连的网络数以百万计）。 internet与Internet的区别\ninternet(互联网或互连网)是一个通用名词，它泛指多个计算机网络互连而成的网络。在这些网络之间的通信协议可以是任意的。 Internet（因特网）则是一个专用名词，它指当前全球最大的、开放的、由众多网络互连而成的特定计算机网络，它采用TCP/IP协议族作为通信的规则，其前身是美国的ARPANET。 任意把几个计算机网络互连起来（不管采用什么协议），并能够相互通信，这样构成的是一个互连网(internet) ，而不是互联网(Internet)。\n因特网发展的三个阶段 因特网服务提供者 ISP(Internet Service Provider) 普通用户是如何接入到因特网的呢？\n答：通过ISP接入因特网\nISP可以从因特网管理机构申请到成块的IP地址，同时拥有通信线路以及路由器等联网设备。任何机构和个人只需缴纳费用，就可从ISP的得到所需要的IP地址。\n因为因特网上的主机都必须有IP地址才能进行通信，这样就可以通过该ISP接入到因特网\n中国的三大 ISP：中国电信，中国联通和中国移动 基于ISP的三层结构的因特网 一旦某个用户能够接入到因特网，那么他也可以成为一个ISP，所需要做的就是购买一些如调制解调器或路由器这样的设备，让其他用户可以和他相连。\n因特网的标准化工作 因特网的标准化工作对因特网的发展起到了非常重要的作用。 因特网在指定其标准上的一个很大的特点是面向公众。 因特网所有的RFC(Request For Comments)技术文档都可从因特网上免费下载； 任何人都可以随时用电子邮件发表对某个文档的意见或建议。 因特网协会ISOC是一个国际性组织，它负责对因特网进行全面管理，以及在世界范围内促进其发展和使用。 因特网体系结构委员会IAB，负责管理因特网有关协议的开发； 因特网工程部IETF，负责研究中短期工程问题，主要针对协议的开发和标准化； 因特网研究部IRTF，从事理论方面的研究和开发一些需要长期考虑的问题。 制订因特网的正式标准要经过一下4个阶段：\n1、因特网草案（在这个阶段还不是RFC文档）\n2、建议标准（从这个阶段开始就成为RFC文档）\n3、草案标准\n4、因特网标准\n因特网的组成 边缘部分\n由所有连接在因特网上的主机组成（台式电脑，大型服务器，笔记本电脑，平板，智能手机等）。这部分是用户直接使用的，用来进行通信（传送数据、音频或视频）和资源共享。\n核心部分\n由大量网络和连接这些网络的路由器组成。这部分是为边缘部分提供服务的（提供连通性和交换）。\n路由器是一种专用计算机，但我们不称它为主机，路由器是实现分组交换的关键构建，其任务是转发收到的分组，这是网络核心最重要的部分。\n处在互联网边缘的部分就是连接在互联网上的所有的主机。这些主机又称为端系统 (end system)。\n端系统在功能上可能有很大的差别：\n小的端系统可以是一台普通个人电脑，具有上网功能的智能手机，甚至是一个很小的网络摄像头。 大的端系统则可以是一台非常昂贵的大型计算机。 端系统的拥有者可以是个人，也可以是单位（如学校、企业、政府机关等），当然也可以是某个ISP。 补充 端系统之间通信的含义\n“主机 A 和主机 B 进行通信”实际上是指：“运行在主机 A 上的某个程序和运行在主机 B 上的另一个程序进行通信”。即“主机 A 的某个进程和主机 B 上的另一个进程进行通信”。简称为“计算机之间通信”。\n端系统之间的通信方式通常可划分为两大类：\n客户-服务器方式：\n客户 (client) 和服务器 (server) 都是指通信中所涉及的两个应用进程。 客户 - 服务器方式所描述的是进程之间服务和被服务的关系。 客户是服务的请求方，服务器是服务的提供方。 服务请求方和服务提供方都要使用网络核心部分所提供的服务。\n对等连接方式：\n对等连接 (peer-to-peer，简写为 P2P ) 是指两个主机在通信时并不区分哪一个是服务请求方还是服务提供方。 只要两个主机都运行了对等连接软件 ( P2P 软件) ，它们就可以进行平等的、对等连接通信。 双方都可以下载对方已经存储在硬盘中的共享文档。 三种交换方式 网络核心部分是互联网中最复杂的部分。\n网络中的核心部分要向网络边缘中的大量主机提供连通性，使边缘部分中的任何一个主机都能够向其他主机通信（即传送或接收各种形式的数据）。\n在网络核心部分起特殊作用的是路由器(router)。\n路由器是实现分组交换 (packet switching) 的关键构件，其任务是转发收到的分组，这是网络核心部分最重要的功能。\n电路交换（Circuit Switching） 传统两两相连的方式，当电话数量很多时，电话线也很多，就很不方便\n所以要使得每一部电话能够很方便地和另一部电话进行通信，就应该使用一个中间设备将这些电话连接起来，这个中间设备就是电话交换机 电话交换机接通电话线的方式称为电路交换；\n从通信资源的分配角度来看，交换（Switching）就是按照某种方式动态地分配传输线路的资源；\n电路交换的三个步骤：\n1、建立连接（分配通信资源）\n2、通话（一直占用通信资源）\n3、释放连接（归还通信资源）\n当使用电路交换来传送计算机数据时，其线路的传输效率往往很低。\n这是因为计算机数据是突发式地出现在传输线路上的。\n所以计算机通常采用的是分组交换，而不是线路交换\n分组交换（Packet Switching） 通常我们把表示该消息的整块数据成为一个报文。\n在发送报文之前，先把较长的报文划分成一个个更小的等长数据段，在每一个数据段前面。加上一些由必要的控制信息组成的首部后，就构成一个分组，也可简称为“包”，相应地，首部也可称为“包头”。\n首部包含了分组的目的地址\n分组从源主机到目的主机，可走不同的路径。\n发送方\n构造分组 发送分组 路由器\n缓存分组 转发分组 简称为“分组转发” 在路由器中的输入和输出端口之间没有直接连线。\n路由器处理分组的过程是：\n把收到的分组先放入缓存（暂时存储）； 查找转发表，找出到某个目的地址应从哪个端口转发； 把分组送到适当的端口转发出去。 接收方\n接收分组 还原报文 报文交换（Message Switching） 报文交换中的交换结点也采用存储转发方式，但报文交换对报文的大小没有限制，这就要求交换结点需要较大的缓存空间。报文交换主要用于早期的电报通信网，现在较少使用，通常被较先进的分组交换方式所取代。\n三种交换方式的对比 假设A，B，C，D是分组传输路径所要经过的4个结点交换机，纵坐标为时间\n分析：\n电路交换：\n通信之前首先要建立连接；连接建立好之后，就可以使用已建立好的连接进行数据传送；数据传送后，需释放连接，以归还之前建立连接所占用的通信线路资源。 一旦建立连接，中间的各结点交换机就是直通形式的，比特流可以直达终点； 报文交换：\n可以随时发送报文，而不需要事先建立连接；整个报文先传送到相邻结点交换机，全部存储下来后进行查表转发，转发到下一个结点交换机。 整个报文需要在各结点交换机上进行存储转发，由于不限制报文大小，因此需要各结点交换机都具有较大的缓存空间。 分组交换：\n可以随时发送分组，而不需要事先建立连接。构成原始报文的一个个分组，依次在各结点交换机上存储转发。各结点交换机在发送分组的同时，还缓存接收到的分组。 构成原始报文的一个个分组，在各结点交换机上进行存储转发，相比报文交换，减少了转发时延，还可以避免过长的报文长时间占用链路，同时也有利于进行差错控制。 计算机网络的定义和分类 定义 计算机网络的精确定义并未统一 计算机网络的最简单的定义是：一些互相连接的、自治的计算机的集合。 互连：是指计算机之间可以通过有线或无线的方式进行数据通信； 自治：是指独立的计算机，他有自己的硬件和软件，可以单独运行使用； 集合：是指至少需要两台计算机； 计算机网络的较好的定义是：计算机网络主要是由一些通用的，可编程的硬件（一定包含有中央处理机CPU）互连而成的，而这些硬件并非专门用来实现某一特定目的（例如，传送数据或视频信号）。这些可编程的硬件能够用来传送多种不同类型的数据，并能支持广泛的和日益增长的应用。 计算机网络所连接的硬件，并不限于一般的计算机，而是包括了智能手机等智能硬件。 计算机网络并非专门用来传送数据，而是能够支持很多种的应用（包括今后可能出现的各种应用）。 分类 按交换技术分类：\n电路交换网络 报文交换网络 分组交换网络 按使用者分类：\n公用网 专用网 按传输介质分类：\n有线网络 无线网络 按覆盖范围分类：\n广域网WAN（Wide Area Network） 作用范围通常为几十到几千公里，因而有时也称为远程网（long haul network）。广域网是互联网的核心部分，其任务是通过长距离（例如，跨越不同的国家）运送主机所发送的数据。\n城域网MAN 作用范围一般是一个城市，可跨越几个街区甚至整个城市。并有趋势将传统的电信服务、有线电视服务、互联网服务融为一体。\n局域网LAN 一般用微型计算机或工作站通过高速通信线路相连（速率通常在 10 Mbit/s 以上），但地理上范围较小（1 km 左右）。例如校园网、企业内网。\n个域网WPAN 就是在个人工作的地方把个人使用的电子设备例如耳机、鼠标、键盘、打印机等用无线技术连接起来的网络。需要注意的是若中央处理机之间的距离非常近，例如仅1m的数量级或更小，\n则一般称为多处理机系统，而不称它为计算机网络\n按拓扑结构分类：\n总线型网络 星型网络 环形网络 网状型网络 计算机网络的性能指标 速率 带宽 吞吐量 带宽1 Gb/s的以太网，代表其额定速率是1 Gb/s，这个数值也是该以太网的吞吐量的绝对上限值。因此，对于带宽1 Gb/s的以太网，可能实际吞吐量只有 700 Mb/s，甚至更低。\n注意：吞吐量还可以用每秒传送的字节数或帧数表示\n时延 时延时指数据（一个报文或分组，甚至比特）从网络（或链路）的一端传送到另一端所需的时间。\n网络时延由几部分组成：\n发送时延 主机或路由器发送数据帧所需要的时间，也就是从发送数据帧的第一个比特算起，到该帧的最后一个比特发送完毕所需的时间。\n传播时延 电磁波在信道中传播一定的距离需要花费的时间。\n处理时延 主机或路由器在收到分组时要花费一定时间进行处理\n排队时延 分组在进过网络传输时，要经过许多路由器。但分组在进入路由器后要先在输入队列中排队等待处理。\n有时会把排队时延看成处理时延 一部分\n总时延 = 发送时延 + 传播时延 + 处理时延 （处理时延 + 排队时延）\n当处理时延忽略不计时，发送时延 和 传播时延谁占主导，要具体情况具体分析\n时延带宽积 时延带宽积 = 传播时延 * 带宽\n往返时间 互联网上的信息不仅仅单方向传输而是双向交互的。因此，我们有时很需要知道双向交互一次所需的时间。\n利用率 利用率有信道利用率和网络利用率两种。\n丢包率 计算机网络体系结构 常见的计算机网络体系结构 如今用的最多的是TCP/IP体系结构，现今规模最大的、覆盖全球的、基于TCP/IP的互联网并未使用OSI标准。\nTCP/IP体系结构相当于将OSI体系结构的物理层和数据链路层合并为了网络接口层，并去掉了会话层和表示层。\nTCP/IP在网络层使用的协议是IP协议，IP协议的意思是网际协议，因此TCP/IP体系结构的网络层称为网际层\n在用户主机的操作系统中，通常都带有符合TCP/IP体系结构标准的TCP/IP协议族。\n而用于网络互连的路由器中，也带有符合TCP/IP体系结构标准的TCP/IP协议族。\n只不过路由器一般只包含网络接口层和网际层。\n网络接口层：并没有规定具体内容，这样做的目的是可以互连全世界各种不同的网络接口，例如：有线的以太网接口，无线局域网的WIFI接口等。\n网际层：它的核心协议是IP协议。\n传输层：TCP和UDP是这层的两个重要协议。\n应用层：这层包含了大量的应用层协议，如 HTTP , DNS 等。\n**IP协议（网际层）可以将不同的网络接口（网络接口层）进行互连，并向其上的TCP协议和UDP协议（传输层）**提供网络互连服务\n而TCP协议在享受IP协议提供的网络互连服务的基础上，可向应用层的相应协议提供可靠的传输服务。\nUDP协议在享受IP协议提供的网络互连服务的基础上，可向应用层的相应协议提供不可靠的传输服务。\nTCP/IP体系结构中最重要的是IP协议和TCP协议，因此用TCP和IP来表示整个协议大家族。\n教学时把TCP/IP体系结构的网络接口层分成了物理层和数据链路层\n计算机网络体系结构分层的必要性 物理层问题\n这图说明\n第一，严格来说，传输媒体并不属于物理层 计算机传输的信号，并不是图示的方波信号 这样举例只是让初学者容易理解\n数据链路层问题\n需要说明的是，这种总线型的网络早已经被淘汰，现在常用的是以太网交换机将多台主机互连形成的交换式以太网\n网络层问题\n传输层问题\n如何标识与网络通信相关的应用进程：一个分组到来，我们应该交给哪个进程处理呢？浏览器进程还是QQ进程\n应用层问题\n应用层该用什么方法（应用层协议）去解析数据\n总结\n计算机网络体系结构分层思想举例 例子：主机的浏览器如何与Web服务器进行通信\n解析：\n主机和Web服务器之间基于网络的通信，实际上是主机中的浏览器应用进程与Web服务器中的Web服务器应用进程之间基于网络的通信\n体系结构的各层在整个过程中起到怎样的作用？\n发送方发送 第一步：http报文 应用层按照HTTP协议的规定构建一个HTTP请求报文 应用层将HTTP请求报文交付给传输层处理 第二步：TCP报文段 传输层给HTTP请求报文添加一个TCP首部，使之成为TCP报文段 TCP报文段的首部格式作用是区分应用进程以及实现可靠传输 传输层将TCP报文段交付给网络层处理 第三步：IP数据段 网络层给TCP报文段添加一个IP首部，使之成为IP数据报 IP数据报的首部格式作用是使IP数据报可以在互联网传输，也就是被路由器转发 网络层将IP数据报交付给数据链路层处理 第四步：帧 数据链路层给IP数据报添加一个首部和一个尾部，使之成为帧 （图示右边为首部，左边为尾部） 该首部的作用主要是为了让帧能够在一段链路上或一个网络上传输，能够被相应的目的主机接收 该尾部的作用是让目的主机检查所接收到的帧是否有误码 数据链路层将帧交付给物理层 第五步：比特流 前导码 物理层先将帧看做是比特流，这里的网络N1假设是以太网，所以物理层还会给该比特流前面添加前导码 前导码的作用是为了让目的主机做好接收帧的准备 物理层将装有前导码的比特流变换成相应的信号发送给传输媒体 第六步：到达路由器 信号通过传输媒体到达路由器 路由器转发 在路由器中\n物理层将信号变为比特流，然后去掉前导码后，将其交付给数据链路层 数据链路层将帧的首部和尾部去掉后，将其交付给网络层，这实际交付的是IP数据报 网络层解析IP数据报的首部，从中提取目的网络地址 在路由器中\n提取目的网络地址后查找自身路由表。确定转发端口， 以便进行转发 网络层将IP数据报交付给数据链路层 数据链路层给IP数据报添加一个首部和一个尾部，使之成为帧 数据链路层将帧交付给物理层 物理层先将帧看成比特流，这里的网络N2假设是以太网，所以物理层还会给该比特流前面添加前导码 物理层将装有前导码的比特流变换成相应的信号发送给传输媒体，信号通过传输媒体到达Web服务器 接收方接收 和发送方（主机）发送过程的封装正好是反着来\n在Web 服务器上\n物理层将信号变换为比特流，然后去掉前导码后成为帧，交付给数据链路层 数据链路层将帧的首部和尾部去掉后成为IP数据报，将其交付给网络层 网络层将IP数据报的首部去掉后成为TCP报文段，将其交付给传输层 传输层将TCP报文段的首部去掉后成为HTTP请求报文，将其交付给应用层 应用层对HTTP请求报文进行解析，然后给主机发回响应报文 发回响应报文的步骤和之前过程类似\n补充 我们如果想通过域名来访问某个网站的话 就必须得到这个域名所绑定的ip地址 那这个ip地址的话我们应该找 dns服务器获取 但是他不是一开始就去找dns服器的 首先他会检查那个浏览器的缓存还有本地的缓存 里面 之前有没有访问过baidu.com 里面有没有他的域名 如果说之前没有访问 没有缓存的话 他就会检查那个本地的hosts文件 看看里面有没有添加映射关系 如果说hosts文件里面也没有的话 他就会将请求发送到电脑中配置的dns服务器 也就是这8.8.8.8这个服务器上去\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 dns是一个应用层的协议 它主要就是 用来获取域名所绑定的ip地址 那么它就会构造一个 比如说 请告诉我 baidu.com的ip地址 这个协议 它会生成一个这个请求 这是dns的协议 生成这个请求之后dns是应用层的数据 对不对 然后他要通过这个模型一层一层往下传传到物理层 也就是我们的网卡的那个接口 生成的那个请求数据 之后他会传到传输层 传输层 它会对应用层的数据进行封装 它封装了一个**源端口**和**目标端口** 比如说 我们前面是源端口 我们写个404 目标端口的话 dns默认是53号端口 它会封装成这么一个 数据包 然后接着往下传 这里的话我们就要注意一下 传输层他主要做什么 传输层主要是有端口这个概念对不对 然后来到网络层 网络层 他也会在前面封装一个 包头 他前面封装的是什么 **源ip**和**目标ip** 也就是本机的电脑ip和你要访问的服务器ip 这里我们的源ip是什么 这个192.168.1.10对不对 我们的目标ip都8.8.8.8 对不对 那么我们把这个数据封装好了 之后我们再来到链路层 那么我们这现在知道网络层 他会干什么 他会添加ip地址 对不对 网络层封装这个数据后 一看 你这个目标的ip地址 并不在我的这个网段范围之内 我找不到这个ip地址 当我们的电脑找不到这个ip地址的时候 他就会去找网关 我们家里的网关一般是路由器 那么这里的话我们就会把这个请求 发送给网关来到下面这一层之后 我们这里的网关地址是多少 这个GW就是网关 是192.168.1.1对不对也就是我们这路由器的这个端口 在同一个局域网里面通信 他使用的是MAC地址 也是我们的MAC地址 那么这个数据他会来到下一层 准备发往网关数据面 数据链路层会在前面加上网关的 **MAC地址**和你的**源MAC地址** 也就是我们本机的MAC地址是什么 AA对不对 这是我们本机的MAC地址 目标mac地址是CC 但我们怎么得到这个CC呢 这个的话是通过**ARP协议**来获取到这个网关的CC的MAC地址 这个的话我们就不展开讲了 然后的话 假设我们就直接获取到了cc的MAC地址 当然这个数据链路层的话 最也会在后面插上一些 其他数据用作校验 这里的话我们就不涉及那么多细节了 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 然后这条数据就会顺着物理层来到网卡的接口 它就会通过网卡从网线中发出去 来到了交换机这个位置 交换机它是一个二层的设备 也就是说他只能解析两层的数据 比如说我们这里复制一层出来 这是 这是那个交换机能干的事情 所有上面的数据他都是没办法进行解析的 那么 从这里 过来 这个数据包从这里来到交换机 交换机收到这个数据之后 他可以 看一下它里面的MAC地址 源MAC地址是这个地方 目标MAC地址是cc 他一看 这cc不是在我这个端口挂着吗 于是乎 他就将这个数据包转发到路由器的这个端口 也就是MAC地址为cc的这个端口 而这个路由器 它是一个三层设备 所以说 它能解析三层的数据 这个是路由器能干的事情 从过这里过来 所以说这数据从这里到了这个端口 之后数据从路由器的网口进来 传到数据链路层 数据链路层一看 哎 这个cc这个目标地址不就是我吗 于是乎他就接受了这个数据包 接收之后 他会把这个 头部拿掉 拿掉之后 他会往上传 传到这个网络层 网络层拿到这个数据 一看源ip地址和目标ip地址 哦 这是192.168.1.10这个小伙子发过来的数据包 那么他要发到哪里 发到8.8.8.8他一看 这不妙 我这里没有跟8.8.8.8直接连接呀 我也找不到他 他不在我附近 那么路由器 他的工作就是把这个数据再路由出去 再转发出去 那么他会转发到哪里去了 路由器他会设置一个**默认路由** 当我发现我这里找不到这个8.8.8.8的时候 他就会发送到我这个默认路由一般来说 就是直接发送到公网的其他的路由器了 这里就我们就不用管了 于是乎他会把这个数据转发给公网上的其他路由器 他准备把我们的这个数据包从我们的 WAN口 我们的路由器不是有一个颜色不一样的吗 叫做WAN口 那是通往互联网的道路 这个WAN口它有一个公网的ip地址 也就是这个 我们这里是配的20.20.20.20 那么他需要将这个里面的内网的数据转发出去 但是这里的话路由器他还要做一个操作 就是**NAT** 也就是网络地址转换 为什么要做转换呢 因为我们这个 源IP地址 他是一个内网的网段 他不能在公网上进行传输 必须转成这个WAN口他允许公网传输的这么一个IP地址 那么他是怎么转的 首先 这个路由器 它有一个NAT的映射表 它会先将我们的本地的这个地址先保存在这里 还有 本地的端口404就保存在这里 直接重新写一个吧 192.168.1.10 那端口是404这是这个数据包里面的 源ip地址和源端口 都会放到这里 然后把他的公网i p 也就是20.20.20.20 塞到这里去 然后他的端口的话也会随机找一个端口 比如叫505吧 改成这样子 同时他也会在这里记录一下他们的映射关系 也就是20.20.20.20 端口号是505 他们是一个对应关系 他会先保存在这里 路由器就把你的包改成了这个样子 然后他就会再回到这个数据链路层 数据链路层的话 他就会前面再加上源MAC地址和目标MAC地址 源MAC地址就是我们现在的这个WAN口的mac地址了 我们要改成DD 目标MAC地址就是他的下一跳的路由器的那个MAC地址 这里的话我们就简化了没画 假设是XX吧 那么他这个数据包就会从 你家里面的那个路由器 那个WAN口出去来到了互联网上其他路由器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 然后他会经过十来个路由器的样子 最终找到这个dns服务器 然后将数据交给这台dns服务器 我们知道dns是一个应用层的协议 所以说这台服务器也能够解析到应用层 所以说 这个整个层 我们都要拿过来 当这个服务器收到这条数据之后 他会从下面往上读 另外 要要再说明的是 他经过每一跳他的MAC地址他都会修改 也就是说从DD跳到XX XX再解开数据包一看 哦 这是DD发过来的 他的目标地址是8.8.8.8 那么我也可以发给我的下一个路由器 他发的时候他就会把他的源MAC地址改成他自己 也就是XX 目标MAC地址就改成他的下一条路由器 假如我们这里就直接改成这个 这个FF这是最终 他的最后一个路由器传给他的这个数据包 这个服务器拿到数据包之后 从物理层到数据链路 链路层一看目标MAC地址是FF 我的 MAC地址是FF 没错 这个包是发给我的 于是乎他会把这里拿掉 拿掉之后 他就会在往上传这个数据包 传到网络层的时候 一看是发给我这个ip地址的 他是从20.20.20.20这个地方发到给我这个服务器的 于是乎这一段也拿掉了然后再往上传传都传输层的时候 传输层是干的是什么 他是添加了端口 对不对 他是从他的505端口发到我的53号端口 我的53号端口是提供dns服务对不对 因为他默认dns的端口就是53号 于是乎 他知道了他是想干什么 他是想做dns解析 那么他就会把这个报文 发送给应用层的dns应用 这个dns收到这个请求之后 知道了他要干嘛 他想要百度的ip地址 至此的话 我们这个数据包 就走到了这个地方 那么他要百度的ip地址 这个dns服务器就是干这个事的 然后他会经过一些手段把他的百度ip地址获取到 比如说 他得到了百度的ip地址 百度 ip地址是7.7.7.7 这是这台服务器的响应 生成了一个dns的响应 之后它就会往回传 来到传输层 现在这里就要改一下了 他的源端口是什么 源端口是他本机的端口是53号 对不对 53号发往哪里 发往505对不对 就刚才我们发过来的他现在要原地返还给他 反还给的505端口 这个传输层的事就干完了 然后他就会发到这个网络层 网络层会干什么 网络层会 添加一个ip地址对不对 所以他这里会加上一个ip地址 源ip地址就是本机的ip 8.8.8.8 目标ip地址就是刚才数据包发给他的这个20.20.20.20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 那么他就会再往下走来到数据链路层 数据链路层会添加一个MAC地址 比如说我的源MAC地址是什么FF对不对 那他的下一跳就是网络中的那个某一台路由器 假设是XX 那么这个数据包就算是完成了 然后他就会从这个 物理的网口出去 来到互联网上 再经过好几个路由器跳跳跳跳 跳来跳去 最后跳到这个路由器这个端口 这个路由器拿到数据一看 当然这个 中间的这个MAC地址一直都在在发生变化的 所以说我们这个MAC地址我们要给他改一下 跳到最后将会是XX发送给 DD的对不对 源MAC地址是XX 那目标那个是DD 收到的是将是这么一个数据 那么这里的时候物理层 它会往上传传到这个数据链路层 这个路由器一看这个DD这不是我的MAC地址吗 于是乎他接受了这个数据包 然后将前面拿掉 然后继续将这个数据包往上传 因为这个网络 他只能解析到网络层 不过他加了NAT之后也能解析到这个 传输层 但实际上 他还是一个三层设备 那么他会检查 是8.8.8.8发到我这个地址的 那么它是从53号端口发往 505这个时候 他检查一 看 这505号不是在这个NAT这里映射了关系吗 于是乎 他会 把这个数据包改回来 20.20.20.20 他实际上对应的是192.168.1.10对不对 然后这个端口505端口对应的是404对不对 就这192.168.1.10目标端口就改成404 然后将这个数据包改好之后重新 返回到了数据链路层 然后再添加头部 这个头部还是MAC地址 源MAC地是什么 源MAC地址 他会转到内网这边来 源MAC地址就变成了cc 目标MAC地址就是a a 你说这里我们知怎么知道他是要a a 因为他这个ip地址就是192.168.1.10他已经获取到了这个192.168.1.10的MAC地址 那么这个数据包弄装好之后 他就会来到数据链路层 在原路返回 这里是二层的交换 机 交换机一看 你的目标MAC地址是a a对不对 aa在我这个口上插着 你去吧 来到这里 这个数据帮拿到之后 他来到数据链路层这个aa 这不是发给我的吗 于是乎 他拿掉了这前面的 我们就不再一直拿了 拿掉之后 他会发到网络层 网络上一看 这不是8.8.8.8给我回的吗 给我192.168.1.10回的吗 确实是我的数据包 那我要了 来到传输层 一看这是53号端口 给我的404端口回的 404是谁开的 404是浏览器开的一个随机的端口 所以说这个请求 他会发送给浏览器来到应用层 浏览器拿到这个数据中获取到了 百度的ip地址是7.7.7.7 这个时候把dns解析的部分就算是结束了 也就是说这台电脑 它得到了百度的ip地址 这台主机收到了百度的ip地址之后就会重新构造一个请求 请把首页数据给我 这个时候 这个数据包又会跟刚才一样来到应用层 这个是HTTP协议 来到这里之后 前面还会添加之前一样端口号 源端口 他也会随机起一个端口 那目标端口的话就是80端口 HTTP协议的默认端口你没指定的话 他就是默认的就是80 然后再到网络层 网络层一样前面加一个ip地址 源ip地址和目标ip地址往后的顺序都是 跟刚才dns解析的流程是一模一样的 数据最终将会到达这个百度的WEB服务器 一看哦 原来你是要我的首页数据 然后他就会构造一个 回应首页的内容这么一个数据包 里面放了就是百度的首页的内容 那么他也会跟 刚才回包的时候 一样 通过这个传输层 添加那个端口号 再添加那个ip地址 再添加那个MAC地址 然后就从互联网上经过 路由器不停的跳跳跳 最后跳到你家里面的路由器 最后再通过nat转换 映射到你的内网中 某一台机器最终将会获得百度的内容 具体就是这么个流程 计算机网络体系结构中的专用术语 以下介绍的专用术语来源于OSI的七层协议体系结构，但也适用于TCP/IP的四层体系结构和五层协议体系结构\n实体 协议 协议：控制两个对等实体进行逻辑通信的规则的集合\n协议三要素：\n语法：定义所交换信息的格式 语义：定义收发双方所要完成的操作 同步：定义收发双发的时序关系 例如TCP三次握手 服务 术语总结 习题 体系结构题目 1 2 3 4 5 6 7 8 9 10 11 TCP/IP模型的网络层（对应OSI参考模型的网络层）主要负责数据包的逐跳（hop-to-hop）路由和传输，也就是确定如何将数据包从源主机发送到目标主机。在这一层，最常见的协议就是IP协议（Internet Protocol）。 网络层提供的是无连接的服务，主要是因为它负责的是分包（IP）转发，而并非是数据报文的传输状态管理。在每一跳传输过程中，网络层都独立而不与其他传输节点的状态相关，它不怎么关心是否建立了一个连接，也不关心数据报是否被成功接收。它只是根据目标地址尽可能把分包推送到下一跳。 网络层提供的服务是不可靠的，主要是因为其设计原则是尽力而为（best effort）的服务模型。也就是说，网络层会尽可能地传送数据包，但不提供任何确保到达目标主机或者保证数据包顺序或错误检查的机制，也不提供处理丢包、延迟等问题的机制。 有时，这种无连接和不可靠的服务更高效，因为它省去了需要建立和维护复杂连接的开销，并可以充分地利用网络资源和适应复杂变化的互联网环境。 要理解为什么理解这种无连接和不可靠服务的设置是合理的，首先需要认识到网络层的主要职责、功能、规模和其在网络架构中的位置。 这种无连接、不可靠的服务并不意味着高层的应用就不能得到有连接、可靠的服务。例如，在TCP/IP模型中，更高层次的传输层（包括TCP协议）提供了为端到端通信提供可靠性、有序性、错误检查和修复等服务。这样，应用层在使用TCP协议的情况下，可以享受到建立在不可靠服务之上的可靠服务。 1 2 3 4 5 6 7 8 9 RTP（Real-time Transport Protocol）是即时传输协议，它是一种网络协议，专门用于Internet上进行实时的音频或视频数据传输。它是由Internet Engineering Task Force（IETF）的多媒体数据传输工作小组开发的，RTP的详细规定在RFC 3550中。 RTP在OSI模型中位于应用层，它通常与RTCP（RTP Control Protocol, RTP控制协议）一起使用，主要处理实时数据传输过程中的流控和同步问题。 RTP在实时应用（如VoIP呼叫、视频会议）中定义了传输数据的标准包格式，包括时间戳、序列号等元素。这些元素帮助接收端重新组装数据流并实现实时播放。 RTP使用上游协议（通常是UDP，有时也可是TCP或其他）来传输数据。此外，RTP不提供任何机制来实现服务质量（QoS），也不提供任何机制来确保数据的安全传输。这些功能需要通过RTP之外的工具或协议来实现。 为了加强可靠性，RTP经常和RTCP一起使用，它们可以为媒体数据提供同步、组件探测、统计信息等功能。 时延相关题目 ","date":"2023-07-15T12:44:47+08:00","permalink":"https://blog.importzhh.me/p/i-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%BE%AE%E8%AF%BE%E5%A0%82-%E6%A6%82%E8%BF%B0/","title":"I: 计算机网络微课堂-概述"},{"content":"算法时间复杂度 时间复杂度如何计算\n时间复杂度按数量级递增排序\n算法空间复杂度 ","date":"2023-07-15T12:44:47+08:00","permalink":"https://blog.importzhh.me/p/i-%E7%BB%AA%E8%AE%BA/","title":"I: 绪论"},{"content":"https://github.com/rogertan30/Love-Leetcode\nhttps://github.com/youngyangyang04/leetcode-master\nhttps://programmercarl.com/other/xunlianying.html\n","date":"2023-07-15T12:44:47+08:00","permalink":"https://blog.importzhh.me/p/i-%E7%BB%AA%E8%AE%BA/","title":"I: 绪论"},{"content":"物理层的基本概念 物理层下面的传输媒体 传输媒体也称为传输介质或传输媒介，他就是数据传输系统中在发送器和接收器之间的物理通路。传输媒体课分为两大类，即导引型传输媒体和非导引型传输媒体\n传输媒体不属于计算机网络体系结构的任何一层。如果非要将它添加到体系结构中，那只能将其放置到物理层之下。\n导引型传输媒体 在导引型传输媒体中，电磁波被导引沿着固体媒体传播。\n同轴电缆\n双绞线\n光纤\n多模光纤\n可以存在多条不同角度入射的光线在一条光纤中传输。这种光纤就称为多模光纤。 单模光纤\n若光纤的直径减小到只有一个光的波长，则光纤就像一根波导那样，它可使光线一直向前传播，而不会产生多次反射。这样的光纤称为单模光纤。 电力线\n非导引型传输媒体 非导引型传输媒体是指自由空间。\n无线电波\n微波\n红外线\n可见光\nLIFI\n传输方式 串行传输和并行传输 串行传输：\n数据是一个比特一个比特依次发送的，因此在发送端与接收端之间，只需要一条数据传输线路即可 并行传输：\n一次发送n个比特，因此，在发送端和接收端之间需要有n条传输线路\n并行传输的优点是比串行传输的速度n倍，但成本高\n数据在传输线路上的传输采用是串行传输，计算机内部的数据传输常用并行传输\n同步传输和异步传输 同步传输：\n数据块以稳定的比特流的形式传输。字节之间没有间隔 接收端在每个比特信号的中间时刻进行检测，以判别接收到的是比特0还是比特1 由于不同设备的时钟频率存在一定差异，不可能做到完全相同，在传输大量数据的过程中，所产生的判别时刻的累计误差，会导致接收端对比特信号的判别错位 所以要使收发双发时钟保持同步\n异步传输：\n以字节为独立的传输单位，字节之间的时间间隔不是固定 接收端仅在每个字节的起始处对字节内的比特实现同步 通常在每个字节前后分别加上起始位和结束位 单向通信（单工）、双向交替通信（半双工）和双向同时通信（全双工） 在许多情况下，我们要使用“信道（channel）”这一名词。信道和电路并不等同。信道一般都是用来表示向某一个方向传送信息的媒体。因此，一条通信电路往往包含一条发送信道和一条接收信道。\n从通信的双方信息交互的方式来看，可以有以下三种基本方式：\n单向通信：\n又称为单工通信，即只能有一个方向的通信而没有反方向的交互。无线电广播或有线电以及电视广播就属于这种类型\n双向交替通信：\n又称为半双工通信，即通信的双方可以发送信息，但不能双方同时发送（当然也就不能同时接收）。这种通信方式使一方发送另一方接收，过一段时间后可以再反过来\n双向同时通信：\n又称为全双工通信，即通信的双发可以同时发送和接收信息。\n单向通信只需要一条信道，而双向交替通信或双向同时通信则需要两条信道（每个方向各一条）\n双向同时通信的传输效率最高\n编码与调制 常用术语\n数据 (data) —— 运送消息的实体。\n信号 (signal) —— 数据的电气的或电磁的表现。\n模拟信号 (analogous signal) —— 代表消息的参数的取值是连续的。\n数字信号 (digital signal) —— 代表消息的参数的取值是离散的。\n码元 (code) —— 在使用时间域（或简称为时域）的波形表示数字信号时，代表不同离散数值的基本波形。\n基带信号（即基本频带信号）—— 来自信源的信号。像计算机输出的代表各种文字或图像文件的数据信号都属于基带信号。\n基带信号往往包含有较多的低频成分，甚至有直流成分，而许多信道并不能传输这种低频分量或直流分量。因此必须对基带信号进行调制 (modulation)。\n在计算机网络中，常见的是将数字基带信号通过编码或调制的方法在相应信道进行传输\n传输媒体与信道的关系 信道的几个基本概念\n信道 —— 一般用来表示向某一个方向传送信息的媒体。 单向通信（单工通信）——只能有一个方向的通信而没有反方向的交互。 双向交替通信（半双工通信）——通信的双方都可以发送信息，但不能双方同时发送(当然也就不能同时接收)。 双向同时通信（全双工通信）——通信的双方可以同时发送和接收信息。 严格来说，传输媒体不能和信道划等号\n对于单工传输，传输媒体只包含一个信道，要么是发送信道，要么是接收信道\n对于半双工和全双工，传输媒体中要包含两个信道，一个发送信道，另一个是接收信道\n如果使用信道复用技术，一条传输媒体还可以包含多个信道\n常用编码 不归零编码\n正电平表示比特1/0\n负电平表示比特0/1\n中间的虚线是零电平，所谓不归零编码，就是指在整个码元时间内，电平不会出现零电平\n实际比特1和比特0的表示要看现实怎么规定\n这需要发送方的发送与接收方的接收做到严格的同步\n需要额外一根传输线来传输时钟信号，使发送方和接收方同步，接收方按时钟信号的节拍来逐个接收码元 但是对于计算机网络，宁愿利用这根传输线传输数据信号，而不是传输时钟信号 由于不归零编码存在同步问题，因此计算机网络中的数据传输不采用这类编码！\n归零编码\n归零编码虽然自同步，但编码效率低\n曼彻斯特编码\n在每个码元时间的中间时刻，信号都会发生跳变\n负跳变表示比特1/0 正跳变表示比特0/1 码元中间时刻的跳变即表示时钟，又表示数据 实际比特1和比特0的表示要看现实怎么规定\n传统以太网使用的就是曼切斯特编码\n差分曼彻斯特编码\n在每个码元时间的中间时刻，信号都会发送跳变，但与曼彻斯特不同\n跳变仅表示时钟 码元开始处电平是否变换表示数据 变化表示比特1/0 不变化表示比特0/1 实际比特1和比特0的表示要看现实怎么规定\n比曼彻斯特编码变化少，更适合较高的传输速率\n总结\n调制 数字信号转换为模拟信号，在模拟信道中传输，例如WiFi，采用补码键控CCK/直接序列扩频DSSS/正交频分复用OFDM等调制方式。\n模拟信号转换为另一种模拟信号，在模拟信道中传输，例如，语音数据加载到模拟的载波信号中传输。频分复用FDM技术，充分利用带宽资源。\n基本调制方法\n调幅AM：所调制的信号由两种不同振幅的基本波形构成。每个基本波形只能表示1比特信息量。 调频FM：所调制的信号由两种不同频率的基本波形构成。每个基本波形只能表示1比特信息量。 调相PM：所调制的信号由两种不同初相位的基本波形构成。每个基本波形只能表示1比特信息量。 但是使用基本调制方法，1个码元只能包含1个比特信息\n混合调制\n上图码元所对应的4个比特是错误的，码元不能随便对应4个比特\n码元 在使用时间域的波形表示数字信号时，代表不同离散数值的基本波形。\n信道的极限容量 任何实际的信道都不是理想的，在传输信号时会产生各种失真以及带来多种干扰。 码元传输的速率越高，或信号传输的距离越远，或传输媒体质量越差，在信道的输出端的波形的失真就越严重。 失真的原因：\n码元传输的速率越高 信号传输的距离越远 噪声干扰越大 传输媒体质量越差 奈氏准则和香农公式对比：\n补充：信道复用技术 本节内容视频未讲到，是《计算机网络（第7版）谢希仁》物理层的内容\n频分复用、时分复用和统计时分复用 复用 (multiplexing) 是通信技术中的基本概念。\n它允许用户使用一个共享信道进行通信，降低成本，提高利用率。\n频分复用 FDM (Frequency Division Multiplexing)\n将整个带宽分为多份，用户在分配到一定的频带后，在通信过程中自始至终都占用这个频带。 频分复用的所有用户在同样的时间占用不同的带宽资源（请注意，这里的“带宽”是频率带宽而不是数据的发送速率）。 时分复用TDM (Time Division Multiplexing)\n时分复用则是将时间划分为一段段等长的时分复用帧（TDM帧）。每一个时分复用的用户在每一个 TDM 帧中占用固定序号的时隙。 每一个用户所占用的时隙是周期性地出现（其周期就是TDM帧的长度）的。 TDM 信号也称为等时 (isochronous) 信号。 时分复用的所有用户在不同的时间占用同样的频带宽度。 时分复用可能会造成线路资源的浪费 使用时分复用系统传送计算机数据时，由于计算机数据的突发性质，用户对分配到的子信道的利用率一般是不高的。 统计时分复用 STDM (Statistic TDM)\n波分复用 波分复用 WDM(Wavelength Division Multiplexing)\n码分复用 码分复用 CDM (Code Division Multiplexing)\n常用的名词是码分多址 CDMA (Code Division Multiple Access)。 各用户使用经过特殊挑选的不同码型，因此彼此不会造成干扰。 这种系统发送的信号有很强的抗干扰能力，其频谱类似于白噪声，不易被敌人发现。 习题 物理层的基本概念 编码与调制习题 信道的极限容量习题 ","date":"2023-07-15T12:44:47+08:00","permalink":"https://blog.importzhh.me/p/ii-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%BE%AE%E8%AF%BE%E5%A0%82-%E7%89%A9%E7%90%86%E5%B1%82/","title":"II: 计算机网络微课堂-物理层"},{"content":"概述 链路是从一个结点到相邻结点的一段物理线路，数据链路则是在链路的基础上增加了一些必要的硬件（如网络适配器）和软件（如协议的实现）\n网络中的主机、路由器等都必须实现数据链路层\n局域网中的主机、交换机等都必须实现数据链路层\n从层次上来看数据的流动\n仅从数据链路层观察帧的流动\n主机H1 到主机H2 所经过的网络可以是多种不同类型的\n注意：不同的链路层可能采用不同的数据链路层协议\n数据链路层使用的信道\n数据链路层属于计算机网路的低层。数据链路层使用的信道主要有以下两种类型：\n点对点信道 广播信道 局域网属于数据链路层\n局域网虽然是个网络。但我们并不把局域网放在网络层中讨论。这是因为在网络层要讨论的是多个网络互连的问题，是讨论分组怎么从一个网络，通过路由器，转发到另一个网络。\n而在同一个局域网中，分组怎么从一台主机传送到另一台主机，但并不经过路由器转发。从整个互联网来看，局域网仍属于数据链路层的范围\n三个重要问题 数据链路层传送的协议数据单元是帧\n封装成帧 封装成帧 (framing) 就是在一段数据的前后分别添加首部和尾部，然后就构成了一个帧。 首部和尾部的一个重要作用就是进行帧定界。 差错控制 在传输过程中可能会产生比特差错：1 可能会变成 0， 而 0 也可能变成 1。\n可靠传输 接收方主机收到有误码的帧后，是不会接受该帧的，会将它丢弃\n如果数据链路层向其上层提供的是不可靠服务，那么丢弃就丢弃了，不会再有更多措施\n如果数据链路层向其上层提供的是可靠服务，那就还需要其他措施，来确保接收方主机还可以重新收到被丢弃的这个帧的正确副本\n以上三个问题都是使用点对点信道的数据链路层来举例的\n补充说明 如果使用广播信道的数据链路层除了包含上面三个问题外，还有一些问题要解决\n如图所示，主机A，B，C，D，E通过一根总线进行互连，主机A要给主机C发送数据，代表帧的信号会通过总线传输到总线上的其他各主机，那么主机B，D，E如何知道所收到的帧不是发送给她们的，主机C如何知道发送的帧是发送给自己的\n可以用编址（地址）的来解决\n将帧的目的地址添加在帧中一起传输\n还有数据碰撞问题\n随着技术的发展，交换技术的成熟，\n在 有线（局域网）领域 使用点对点链路和链路层交换机的交换式局域网取代了共享式局域网\n在无线局域网中仍然使用的是共享信道技术\n封装成帧 介绍 封装成帧是指数据链路层给上层交付的协议数据单元添加帧头和帧尾使之成为帧\n帧头和帧尾中包含有重要的控制信息\n发送方的数据链路层将上层交付下来的协议数据单元封装成帧后，还要通过物理层，将构成帧的各比特，转换成电信号交给传输媒体，那么接收方的数据链路层如何从物理层交付的比特流中提取出一个个的帧？\n答：需要帧头和帧尾来做帧定界\n但比不是每一种数据链路层协议的帧都包含有帧定界标志，例如下面例子\n前导码\n前同步码：作用是使接收方的时钟同步 帧开始定界符：表明其后面紧跟着的就是MAC帧 另外以太网还规定了帧间间隔为96比特时间，因此，MAC帧不需要帧结束定界符\n透明传输 透明\n指某一个实际存在的事物看起来却好像不存在一样。\n透明传输是指数据链路层对上层交付的传输数据没有任何限制，好像数据链路层不存在一样\n帧界定标志也就是个特定数据值，如果在上层交付的协议数据单元中， 恰好也包含这个特定数值，接收方就不能正确接收\n所以数据链路层应该对上层交付的数据有限制，其内容不能包含帧定界符的值\n解决透明传输问题\n解决方法：面向字节的物理链路使用字节填充 (byte stuffing) 或字符填充 (character stuffing)，面向比特的物理链路使用比特填充的方法实现透明传输 发送端的数据链路层在数据中出现控制字符“SOH”或“EOT”的前面插入一个转义字符“ESC”(其十六进制编码是1B)。 接收端的数据链路层在将数据送往网络层之前删除插入的转义字符。 如果转义字符也出现在数据当中，那么应在转义字符前面插入一个转义字符 ESC。当接收端收到连续的两个转义字符时，就删除其中前面的一个。 帧的数据部分长度\n总结 差错检测 介绍 奇偶校验 循环冗余校验CRC(Cyclic Redundancy Check) 例题\n总结\n循环冗余校验 CRC 是一种检错方法，而帧校验序列 FCS 是添加在数据后面的冗余码\n可靠传输 基本概念 下面是比特差错\n其他传输差错\n分组丢失 路由器输入队列快满了，主动丢弃收到的分组\n分组失序 数据并未按照发送顺序依次到达接收端\n分组重复 由于某些原因，有些分组在网络中滞留了，没有及时到达接收端，这可能会造成发送端对该分组的重发，重发的分组到达接收端，但一段时间后，滞留在网络的分组也到达了接收端，这就造成分组重复的传输差错\n三种可靠协议 停止-等待协议SW 回退N帧协议GBN 选择重传协议SR 这三种可靠传输实现机制的基本原理并不仅限于数据链路层，可以应用到计算机网络体系结构的各层协议中\n停止-等待协议 停止-等待协议可能遇到的四个问题 确认与否认\n超时重传\n确认丢失\n既然数据分组需要编号，确认分组是否需要编号？\n要。如下图所示\n确认迟到\n注意，图中最下面那个数据分组与之前序号为0的那个数据分组不是同一个数据分组\n注意事项\n停止-等待协议的信道利用率 假设收发双方之间是一条直通的信道\nTD：是发送方发送数据分组所耗费的发送时延 RTT：是收发双方之间的往返时间 TA：是接收方发送确认分组所耗费的发送时延 TA一般都远小于TD，可以忽略，当RTT远大于TD时，信道利用率会非常低\n像停止-等待协议这样通过确认和重传机制实现的可靠传输协议，常称为自动请求重传协议ARQ(Automatic Repeat reQuest)，意思是重传的请求是自动进行，因为不需要接收方显式地请求，发送方重传某个发送的分组\n回退N帧协议GBN 为什么用回退N帧协议 在相同的时间内，使用停止-等待协议的发送方只能发送一个数据分组，而采用流水线传输的发送方，可以发送多个数据分组\n回退N帧协议在流水线传输的基础上，利用发送窗口来限制发送方可连续发送数据分组的个数\n无差错情况流程 发送方将序号落在发送窗口内的0~4号数据分组，依次连续发送出去\n他们经过互联网传输正确到达接收方，就是没有乱序和误码，接收方按序接收它们，每接收一个，接收窗口就向前滑动一个位置，并给发送方发送针对所接收分组的确认分组，在通过互联网的传输正确到达了发送方\n发送方每接收一个、发送窗口就向前滑动一个位置，这样就有新的序号落入发送窗口，发送方可以将收到确认的数据分组从缓存中删除了，而接收方可以择机将已接收的数据分组交付上层处理\n累计确认 累计确认\n优点:\n即使确认分组丢失，发送方也可能不必重传 减小接收方的开销 减小对网络资源的占用 缺点：\n不能向发送方及时反映出接收方已经正确接收的数据分组信息 有差错情况 例如\n在传输数据分组时，5号数据分组出现误码，接收方通过数据分组中的检错码发现了错误\n于是丢弃该分组，而后续到达的这剩下四个分组与接收窗口的序号不匹配\n接收同样也不能接收它们，讲它们丢弃，并对之前按序接收的最后一个数据分组进行确认，发送ACK4，每丢弃一个数据分组，就发送一个ACK4\n当收到重复的ACK4时，就知道之前所发送的数据分组出现了差错，于是可以不等超时计时器超时就立刻开始重传，具体收到几个重复确认就立刻重传，根据具体实现决定\n如果收到这4个重复的确认并不会触发发送立刻重传，一段时间后。超时计时器超时，也会将发送窗口内以发送过的这些数据分组全部重传\n若WT超过取值范围，例如WT=8，会出现什么情况？\n习题\n总结 回退N帧协议在流水线传输的基础上利用发送窗口来限制发送方连续发送数据分组的数量，是一种连续ARQ协议 在协议的工作过程中发送窗口和接收窗口不断向前滑动，因此这类协议又称为滑动窗口协议 由于回退N帧协议的特性，当通信线路质量不好时，其信道利用率并不比停止-等待协议高 选择重传协议SR 具体流程请看视频\n习题\n总结\n点对点协议PPP 点对点协议PPP（Point-to-Point Protocol）是目前使用最广泛的点对点数据链路层协议 PPP协议是因特网工程任务组IEIF在1992年制定的。经过1993年和1994年的修订，现在的PPP协议已成为因特网的正式标准[RFC1661，RFC1662] 数据链路层使用的一种协议，它的特点是：简单；只检测差错，而不是纠正差错；不使用序号，也不进行流量控制；可同时支持多种网络层协议 PPPoE 是为宽带上网的主机使用的链路层协议 帧格式 必须规定特殊的字符作为帧定界符\n透明传输 必须保证数据传输的透明性\n实现透明传输的方法\n面向字节的异步链路：字节填充法（插入“转义字符”） 面向比特的同步链路：比特填充法（插入“比特0”） 差错检测 能够对接收端收到的帧进行检测，并立即丢弃有差错的帧。\n工作状态 当用户拨号接入 ISP 时，路由器的调制解调器对拨号做出确认，并建立一条物理连接。 PC 机向路由器发送一系列的 LCP 分组（封装成多个 PPP 帧）。 这些分组及其响应选择一些 PPP 参数，并进行网络层配置，NCP 给新接入的 PC 机 分配一个临时的 IP 地址，使 PC 机成为因特网上的一个主机。 通信完毕时，NCP 释放网络层连接，收回原来分配出去的 IP 地址。接着，LCP 释放数据链路层连接。最后释放的是物理层的连接。 可见，PPP 协议已不是纯粹的数据链路层的协议，它还包含了物理层和网络层的内容。\n媒体接入控制（介质访问控制）——广播信道 媒体接入控制（介质访问控制）使用一对多的广播通信方式\nMedium Access Control翻译成媒体接入控制，有些翻译成介质访问控制\n局域网的数据链路层\n局域网最主要的特点是： 网络为一个单位所拥有； 地理范围和站点数目均有限。 局域网具有如下主要优点： 具有广播功能，从一个站点可很方便地访问全网。局域网上的主机可共享连接在局域网上的各种硬件和软件资源。 便于系统的扩展和逐渐地演变，各设备的位置可灵活调整和改变。 提高了系统的可靠性、可用性和残存性。 数据链路层的两个子层\n为了使数据链路层能更好地适应多种局域网标准，IEEE 802 委员会就将局域网的数据链路层拆成两个子层：\n逻辑链路控制 LLC (Logical Link Control)子层； 媒体接入控制 MAC (Medium Access Control)子层。 与接入到传输媒体有关的内容都放在 MAC子层，而 LLC 子层则与传输媒体无关。 不管采用何种协议的局域网，对 LLC 子层来说都是透明的。\n基本概念 为什么要媒体接入控制（介质访问控制）？\n共享信道带来的问题\n若多个设备在共享信道上同时发送数据，则会造成彼此干扰，导致发送失败。\n随着技术的发展，交换技术的成熟和成本的降低，具有更高性能的使用点对点链路和链路层交换机的交换式局域网在有线领域已完全取代了共享式局域网，但由于无线信道的广播天性，无线局域网仍然使用的是共享媒体技术\n静态划分信道 信道复用\n频分复用FDM (Frequency Division Multiplexing)\n将整个带宽分为多份，用户在分配到一定的频带后，在通信过程中自始至终都占用这个频带。\n频分复用的所有用户在同样的时间占用不同的带宽资源（请注意，这里的“带宽”是频率带宽而不是数据的发送速率）。\n时分复用TDM (Time Division Multiplexing)\n时分复用则是将时间划分为一段段等长的时分复用帧（TDM帧）。每一个时分复用的用户在每一个 TDM 帧中占用固定序号的时隙。 每一个用户所占用的时隙是周期性地出现（其周期就是TDM帧的长度）的。 TDM 信号也称为等时 (isochronous) 信号。 时分复用的所有用户在不同的时间占用同样的频带宽度。 波分复用 WDM(Wavelength Division Multiplexing)\n波分复用就是光的频分复用，使用一根光纤来同时传输多个光载波信号\n光信号传输一段距离后悔衰减，所以要用 掺铒光纤放大器 放大光信号\n码分复用 CDM (Code Division Multiplexing)\n动态接入控制 受控接入\n受控接入在局域网中使用得较少，本书不再讨论\n随机接入\n重点\n随机接入（CSMA/CD协议） 总线局域网使用协议：CSMA/CD\n基本概念 最初的以太网是将许多计算机都连接到一根总线上。易于实现广播通信。当初认为这样的连接方法既简单又可靠，因为总线上没有有源器件。\n以太网（Ethernet）是一种计算机局域网技术。IEEE组织的IEEE 802.3标准制定了**以太网（Ethernet）**的技术标准\n以太网采用无连接的工作方式，对发送的数据帧不进行编号，也不要求对方发回确认。目的站收到有差错帧就把它丢弃，其他什么也不做\n多址接入MA 表示许多主机以多点接入的方式连接在一根总线上。\n载波监听CS 是指每一个站在发送数据之前先要检测一下总线上是否有其他计算机在发送数据，如果有，则暂时不要发送数据，以免发生碰撞。\n总线上并没有什么“载波”。因此， “载波监听”就是用电子技术检测总线上有没有其他计算机发送的数据信号。\n碰撞检测CD “碰撞检测”就是计算机边发送数据边检测信道上的信号电压大小。 当几个站同时在总线上发送数据时，总线上的信号电压摆动值将会增大（互相叠加）。 当一个站检测到的信号电压摆动值超过一定的门限值时，就认为总线上至少有两个站同时在发送数据，表明产生了碰撞。 所谓“碰撞”就是发生了冲突。因此“碰撞检测”也称为“冲突检测”。 在发生碰撞时，总线上传输的信号产生了严重的失真，无法从中恢复出有用的信息来。 每一个正在发送数据的站，一旦发现总线上出现了碰撞，就要立即停止发送，免得继续浪费网络资源，然后等待一段随机时间后再次发送。 为什么要进行碰撞检测？ 因为信号传播时延对载波监听产生了影响\nA 需要单程传播时延的 2 倍的时间，才能检测到与 B 的发送产生了冲突\nCSMA/CD 协议工作流程 CSMA/CD 协议工作——争用期（碰撞窗口） CSMA/CD 协议工作——最小帧长 CSMA/CD 协议工作——最大帧长 CSMA/CD 协议工作——截断二进制指数退避算法 CSMA/CD 协议工作——信道利用率 CSMA/CD 协议工作——帧接收流程 CSMA/CD 协议的重要特性 使用 CSMA/CD 协议的以太网不能进行全双工通信而只能进行双向交替通信（半双工通信）。 每个站在发送数据之后的一小段时间内，存在着遭遇碰撞的可能性。 这种发送的不确定性使整个以太网的平均通信量远小于以太网的最高数据率。 CSMA/CD协议曾经用于各种总线结构以太网和双绞线以太网的早起版本中。\n现在的以太网基于交换机和全双工连接，不会有碰撞，因此没有必要使用CSMA/CS协议\n随机接入（CSMA/CA协议） 无线局域网使用的协议：CSMA/CA\n为什么无线局域网要使用CSMA/CA协议 帧间间隔IFS（InterFrame Space） CSMA/CA协议的工作原理 源站为什么在检测到信道空闲后还要再等待一段时间DIFS？\n考虑到可能有其他的站有高优先级的帧要发送。若有，就要让高优先级帧先发送 目的站为什么正确接收数据帧后还要等待一段时间SIFS才能发送ACK帧？\nSIFS是最短的帧间间隔，用来分隔开属于一次对话的各帧，在这段时间内，一个站点应当能够从发送方式切换到接收方式 信道由忙转为空闲且经过DIFS时间后，还要退避一段随机时间才能使用信道？\n防止多个站点同时发送数据而产生碰撞\n使用退避算法的时机\nCSMA/CA协议的退避算法 退避算法的示例\nCSMA/CA协议的信道预约和虚拟载波监听 虚拟载波监听机制能减少隐蔽站带来的碰撞问题的示例\nMAC地址、IP地址以及ARP协议 MAC地址 使用点对点信道的数据链路层不需要使用地址 使用广播信道的数据链路层必须使用地址来区分各主机 广播信道的数据链路层必须使用地址（MAC） MAC地址又称为硬件地址或物理地址。请注意：不要被 “物理” 二字误导认为物理地址属于物理层范畴，物理地址属于数据链路层范畴\nIEEE 802局域网的MAC地址格式 组织唯一标识符OUI\n生产网络设备的厂商，需要向IEEE的注册管理机构申请一个或多个OUI 网络接口标识符\n由获得OUI的厂商自行随意分配 EUI-48\n48是这个MAC地址的位数 对于使用EUI-48空间的应用程序，IEEE的目标寿命为100年（直到2080年），但是鼓励采用EUI-64作为替代\n关于无效的 MAC 帧\n数据字段的长度与长度字段的值不一致； 帧的长度不是整数个字节； 用收到的帧检验序列 FCS 查出有差错； 数据字段的长度不在 46 ~ 1500 字节之间。 有效的 MAC 帧长度为 64 ~ 1518 字节之间。 对于检查出的无效 MAC 帧就简单地丢弃。以太网不负责重传丢弃的帧。\nIEEE 802局域网的MAC地址发送顺序 单播MAC地址举例 主机B给主机C发送单播帧，主机B首先要构建该单播帧，在帧首部中的目的地址字段填入主机C的MAC地址，源地址字段填入自己的MAC地址，再加上帧首部的其他字段、数据载荷以及帧尾部，就构成了该单播帧\n主机B将该单播帧发送出去，主机A和C都会收到该单播帧\n主机A的网卡发现该单播帧的目的MAC地址与自己的MAC地址不匹配，丢弃该帧\n主机C的网卡发现该单播帧的目的MAC地址与自己的MAC地址匹配，接受该帧\n并将该帧交给其上层处理\n广播MAC地址举例 假设主机B要发送一个广播帧，主机B首先要构建该广播帧，在帧首部中的目的地址字段填入广播地址，也就是十六进制的全F，源地址字段填入自己的MAC地址，再加上帧首部中的其他字段、数据载荷以及帧尾部，就构成了该广播帧\n主机B讲该广播帧发送出去，主机A和C都会收到该广播帧，发现该帧首部中的目的地址字段的内容是广播地址，就知道该帧是广播帧，主机A和主机C都接受该帧，并将该帧交给上层处理\n多播MAC地址举例 假设主机A要发送多播帧给该多播地址。将该多播地址的左起第一个字节写成8个比特，第一个字节的最低比特位是1，这就表明该地址是多播地址。\n快速判断地址是不是多播地址，就是上图所示箭头所指的第十六进制数不能整除2（1,3,5,7,9,B,D,F），则该地址是多播地址\n假设主机B，C和D支持多播，各用户给自己的主机配置多播组列表如下所示\n主机B属于两个多播组，主机C也属于两个多播组，而主机D不属于任何多播组\n主机A首先要构建该多播帧，在帧首部中的目的地址字段填入该多播地址，源地址点填入自己的MAC地址，再加上帧首部中的其他字段、数据载荷以及帧尾部，就构成了该多播帧\n主机A将该多播帧发送出去，主机B、C、D都会收到该多播帧\n主机B和C发现该多播帧的目的MAC地址在自己的多播组列表中，主机B和C都会接受该帧\n主机D发现该多播帧的目的MAC地址不在自己得多播组列表中，则丢弃该多播帧\n给主机配置多播组列表进行私有应用时，不得使用公有的标准多播地址\nIP地址 IP地址属于网络层的范畴，不属于数据链路层的范畴\n下面内容讲的是IP地址的使用，详细的IP地址内容在网络层中介绍\n基本概念 从网络体系结构看IP地址与MAC地址 数据包转发过程中IP地址与MAC地址的变化情况 图上各主机和路由器各接口的IP地址和MAC地址用简单的标识符来表示\n如何从IP地址找出其对应的MAC地址？\nARP协议\nARP协议 如何从IP地址找出其对应的MAC地址？\nARP（地址解析协议）\n流程 ARP高速缓存表\n当主机B要给主机C发送数据包时，会首先在自己的ARP高速缓存表中查找主机C的IP地址所对应的MAC地址，但未找到，因此，主机B需要发送ARP请求报文，来获取主机C的MAC地址\nARP请求报文有具体的格式，上图的只是简单描述\nARP请求报文被封装在MAC帧中发送，目的地址为广播地址\n主机B发送封装有ARP请求报文的广播帧，总线上的其他主机都能收到该广播帧\n收到ARP请求报文的主机A和主机C会把ARP请求报文交给上层的ARP进程\n主机A发现所询问的IP地址不是自己的IP地址，因此不用理会\n主机C的发现所询问的IP地址是自己的IP地址，需要进行相应\n动态与静态的区别\nARP协议只能在一段链路或一个网络上使用，而不能跨网络使用\nARP协议的使用是逐段链路进行的\n总结 ARP表中的IP地址与MAC地址的对应关系记录，是会定期自动删除的，因为IP地址与MAC地址的对应关系不是永久性的\n集线器与交换机的区别 集线器-在物理层扩展以太网 概念 传统以太网最初是使用粗同轴电缆，后来演进到使用比较便宜的细同轴电缆，最后发展为使用更便宜和更灵活的双绞线。 采用双绞线的以太网采用星形拓扑，在星形的中心则增加了一种可靠性非常高的设备，叫做集线器 (hub)。 集线器是也可以看做多口中继器，每个端口都可以成为一个中继器，中继器是对减弱的信号进行放大和发送的设备 集线器的以太网在逻辑上仍是个总线网，需要使用CSMA/CD协议来协调各主机争用总线，只能工作在半双工模式，收发帧不能同时进行 集线器HUB在物理层扩展以太网 使用集线器扩展：将多个以太网段连成更大的、多级星形结构的以太网\n优点 使原来属于不同碰撞域的以太网上的计算机能够进行跨碰撞域的通信。 扩大了以太网覆盖的地理范围。 缺点 碰撞域增大了，但总的吞吐量并未提高。 如果不同的碰撞域使用不同的数据率，那么就不能用集线器将它们互连起来。 碰撞域\n碰撞域（collision domain）又称为冲突域，是指网络中一个站点发出的帧会与其他站点发出的帧产生碰撞或冲突的那部分网络。 碰撞域越大，发生碰撞的概率越高。 以太网交换机-在数据链路层扩展以太网 概念 扩展以太网更常用的方法是在数据链路层进行。 早期使用网桥，现在使用以太网交换机。 网桥\n网桥工作在数据链路层。 它根据 MAC 帧的目的地址对收到的帧进行转发和过滤。 当网桥收到一个帧时，并不是向所有的接口转发此帧，而是先检查此帧的目的MAC 地址，然后再确定将该帧转发到哪一个接口，或把它丢弃。 交换机\n1990 年问世的交换式集线器 (switching hub) 可明显地提高以太网的性能。 交换式集线器常称为以太网交换机 (switch) 或第二层交换机 (L2 switch)，强调这种交换机工作在数据链路层。 以太网交换机实质上就是一个多接口的网桥 集线器HUB与交换机SWITCH区别 使用集线器互连而成的共享总线式以太网上的某个主机，要给另一个主机发送单播帧，该单播帧会通过共享总线传输到总线上的其他各个主机\n使用交换机互连而成的交换式以太网上的某个主机，要给另一个主机发送单播帧，该单播帧进入交换机后，交换机会将该单播帧转发给目的主机，而不是网络中的其他各个主机\n这个例子的前提条件是忽略ARP过程，并假设交换机的帧交换表已经学习或配置好了\n以太网交换机的交换方式\n存储转发方式 把整个数据帧先缓存后再进行处理。 直通 (cut-through) 方式 接收数据帧的同时就立即按数据帧的目的 MAC 地址决定该帧的转发接口，因而提高了帧的转发速度。 缺点是它不检查差错就直接将帧转发出去，因此有可能也将一些无效帧转发给其他的站。 这个例子的前提条件是忽略ARP过程，并假设交换机的帧交换表已经学习或配置好了\n对比集线器和交换机\n多台主机同时给另一台主机发送单播帧\n集线器以太网：会产生碰撞，遭遇碰撞的帧会传播到总线上的各主机\n交换机以太网：会将它们缓存起来，然后逐个转发给目的主机，不会产生碰撞\n这个例子的前提条件是忽略ARP过程，并假设交换机的帧交换表已经学习或配置好了\n集线器扩展以太网和交换机扩展以太网区别\n单播\n广播\n多个单播\n广播域（broadcast domain）：指这样一部分网络，其中任何一台设备发出的广播通信都能被该部分网络中的所有其他设备所接收。\n总结 工作在数据链路层的以太网交换机，其性能远远超过工作在物理层的集线器，而且价格并不贵，这就使得集线器逐渐被市场淘汰\n以太网交换机自学习和转发帧的流程 概念 自学习和转发帧的例子 以下例子假设各主机知道网络中其他各主机的MAC地址（无需进行ARP）\nA -\u0026gt; B\nA 先向 B 发送一帧。该帧从接口 1 进入到交换机 交换机收到帧后，先查找（图中左边）交换表。没有查到应从哪个接口转发这个帧给 B 交换机把这个帧的源地址 A 和接口 1 写入（图中左边）交换表中 交换机向除接口 1 以外的所有的接口广播这个帧 接口 4到接口 2，先查找（图中右边）交换表。没有查到应从哪个接口转发这个帧给 B 交换机把这个帧的源地址 A 和接口 1 写入（图中右边）交换表中 除B主机之外与该帧的目的地址不相符，将丢弃该帧 主机B发现是给自己的帧，接受该帧 B -\u0026gt; A\nB 向 A 发送一帧。该帧从接口 3 进入到交换机 交换机收到帧后，先查找（图中左边）交换表。发现（图中左边）交换表中的 MAC 地址有 A，表明要发送给A的帧应从接口1转发出去。于是就把这个帧传送到接口 1 转发给 A。 主机 A 发现目的地址是它，就接受该帧 交换机把这个帧的源地址 B 和接口 3 写入（图中左边）交换表中 E -\u0026gt; A\nE 向 A发送一帧 交换机收到帧后，先查找（图中右边）交换表。发现（图中右边）交换表中的 MAC 地址有 A，表明要发送给A的帧应从接口2转发出去。于是就把这个帧传送到接口 2 转发给 接口 4。 交换机把这个帧的源地址 E 和接口 3 写入（图中右边）交换表中 接口 4 到 左边的交换机，先查找（图中左边）交换表。发现（图中左边）交换表中的 MAC 地址有 A，表明要发送给A的帧应从接口1转发出去。于是就把这个帧传送到接口 1 转发给 A。 交换机把这个帧的源地址 E 和接口 4 写入（图中左边）交换表中 主机 A 发现目的地址是它，就接受该帧 G -\u0026gt; A\n主机 A、主机 G、交换机 1的接口 1就共享同一条总线（相当于总线式网络，可以想象成用集线器连接了）\n主机 G 发送给 主机 A 一个帧 主机 A 和 交换机接口 1都能接收到 主机 A 的网卡收到后，根据帧的目的MAC地址A，就知道是发送给自己的帧，就接受该帧 交换机 1收到该帧后，首先进行登记工作 然后交换机 1对该帧进行转发，该帧的MAC地址是A，在（图中左边）交换表查找MAC 地址有 A MAC 地址为 A的接口号是1，但是该帧正是从接口 1 进入交换机的，交换机不会再从该接口 1 将帧转发出去，因为这是没有必要，于是丢弃该帧 随着网络中各主机都发送了帧后，网络中的各交换机就可以学习到各主机的MAC地址，以及它们与自己各接口的对应关系\n考虑到可能有时要在交换机的接口更换主机，或者主机要更换其网络适配器，这就需要更改交换表中的项目。为此，在交换表中每个项目都设有一定的有效时间。过期的项目就自动被删除。\n以太网交换机的这种自学习方法使得以太网交换机能够即插即用，不必人工进行配置，因此非常方便。\n总结 交换机自学习和转发帧的步骤归纳\n以太网交换机的生成树协议STP 如何提高以太网的可靠性 生成树协议STP IEEE 802.1D 标准制定了一个生成树协议 STP (Spanning Tree Protocol)。 其要点是：不改变网络的实际拓扑，但在逻辑上则切断某些链路，使得从一台主机到所有其他主机的路径是无环路的树状结构，从而消除了兜圈子现象。 虚拟局域网VLAN 为什么要虚拟局域网VLAN 广播风暴\n分割广播域的方法\n为了分割广播域，所以虚拟局域网VLAN技术应运而生\n概念 利用以太网交换机可以很方便地实现虚拟局域网 VLAN (Virtual LAN)。 IEEE 802.1Q 对虚拟局域网 VLAN 的定义： 虚拟局域网 VLAN 是由一些局域网网段构成的与物理位置无关的逻辑组，而这些网段具有某些共同的需求。每一个 VLAN 的帧都有一个明确的标识符，指明发送这个帧的计算机是属于哪一个 VLAN。 同一个VLAN内部可以广播通信，不同VLAN不可以广播通信 虚拟局域网其实只是局域网给用户提供的一种服务，而并不是一种新型局域网。 由于虚拟局域网是用户和网络资源的逻辑组合，因此可按照需要将有关设备和资源非常方便地重新组合，使用户从不同的服务器或数据库中存取所需的资源。 虚拟局域网VLAN的实现机制 虚拟局域网VLAN技术是在交换机上实现的，需要交换机能够实现以下功能\n能够处理带有VLAN标记的帧——IEEE 802.1 Q帧 交换机的各端口可以支持不同的端口类型，不同端口类型的端口对帧的处理方式有所不同 Access端口\n交换机与用户计算机之间的互连\n同一个VLAN内部可以广播通信，不同VLAN不可以广播通信\nTruck端口\n交换机之间或交换机与路由器之间的互连\n小例题\n华为交换机私有的Hybrid端口类型\n总结 虚拟局域网优点\n虚拟局域网（VLAN）技术具有以下主要优点：\n改善了性能 简化了管理 降低了成本 改善了安全性 习题 封装成帧习题 差错检测习题 可靠传输习题 媒体接入控制习题 MAC地址、IP地址以及ARP协议习题 以太网交换机自学习和转发帧的流程 ","date":"2023-07-15T12:44:47+08:00","permalink":"https://blog.importzhh.me/p/iii-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%BE%AE%E8%AF%BE%E5%A0%82-%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/","title":"III: 计算机网络微课堂-数据链路层"},{"content":"网络层概述 简介 网络层的主要任务是实现网络互连，进而实现数据包在各网络之间的传输 这些异构型网络N1~N7如果只是需要各自内部通信，他们只要实现各自的物理层和数据链路层即可\n但是如果要将这些异构型网络互连起来，形成一个更大的互联网，就需要实现网络层设备路由器\n有时为了简单起见，可以不用画出这些网络，图中N1~N7，而将他们看做是一条链路即可\n要实现网络层任务，需要解决一下主要问题：\n网络层向运输层提供怎样的服务（“可靠传输”还是“不可靠传输”） 在数据链路层那课讲过的可靠传输，详情可以看那边的笔记：网络层对以下的分组丢失、分组失序、分组重复的传输错误采取措施，使得接收方能正确接受发送方发送的数据，就是可靠传输，反之，如果什么措施也不采取，则是不可靠传输\n网络层寻址问题 路由选择问题 路由器收到数据后，是依据什么来决定将数据包从自己的哪个接口转发出去？\n依据数据包的目的地址和路由器中的路由表\n但在实际当中，路由器是怎样知道这些路由记录？\n由用户或网络管理员进行人工配置，这种方法只适用于规模较小且网络拓扑不改变的小型互联网 另一种是实现各种路由选择协议，由路由器执行路由选择协议中所规定的路由选择算法，而自动得出路由表中的路有记录，这种方法更适合规模较大且网络拓扑经常改变的大型互联网 补充 网络层（网际层）除了 IP协议外，还有之前介绍过的地址解析协议ARP，还有网际控制报文协议ICMP，网际组管理协议IGMP\n总结 网络层提供的两种服务 在计算机网络领域，网络层应该向运输层提供怎样的服务（“面向连接”还是“无连接”）曾引起了长期的争论。 争论焦点的实质就是：在计算机通信中，可靠交付应当由谁来负责？是网络还是端系统？ 面向连接的虚电路服务 一种观点：让网络负责可靠交付\n这种观点认为，应借助于电信网的成功经验，让网络负责可靠交付，计算机网络应模仿电信网络，使用面向连接的通信方式。 通信之前先建立虚电路 (Virtual Circuit)，以保证双方通信所需的一切网络资源。 如果再使用可靠传输的网络协议，就可使所发送的分组无差错按序到达终点，不丢失、不重复。 发送方 发送给 接收方 的所有分组都沿着同一条虚电路传送\n虚电路表示这只是一条逻辑上的连接，分组都沿着这条逻辑连接按照存储转发方式传送，而并不是真正建立了一条物理连接。 请注意，电路交换的电话通信是先建立了一条真正的连接。 因此分组交换的虚连接和电路交换的连接只是类似，但并不完全一样 无连接的数据报服务 另一种观点：网络提供数据报服务\n互联网的先驱者提出了一种崭新的网络设计思路。 网络层向上只提供简单灵活的、无连接的、尽最大努力交付的数据报服务。 网络在发送分组时不需要先建立连接。每一个分组（即 IP 数据报）独立发送，与其前后的分组无关（不进行编号）。 网络层不提供服务质量的承诺。即所传送的分组可能出错、丢失、重复和失序（不按序到达终点），当然也不保证分组传送的时限。 发送方 发送给 接收方 的分组可能沿着不同路径传送\n尽最大努力交付\n如果主机（即端系统）中的进程之间的通信需要是可靠的，那么就由网络的主机中的运输层负责可靠交付（包括差错处理、流量控制等） 。 采用这种设计思路的好处是：网络的造价大大降低，运行方式灵活，能够适应多种应用。 互连网能够发展到今日的规模，充分证明了当初采用这种设计思路的正确性。 虚电路服务与数据报服务的对比 对比的方面 虚电路服务 数据报服务 思路 可靠通信应当由网络来保证 可靠通信应当由用户主机来保证 连接的建立 必须有 不需要 终点地址 仅在连接建立阶段使用，每个分组使用短的虚电路号 每个分组都有终点的完整地址 分组的转发 属于同一条虚电路的分组均按照同一路由进行转发 每个分组独立选择路由进行转发 当结点出故障时 所有通过出故障的结点的虚电路均不能工作 出故障的结点可能会丢失分组，一些路由可能会发生变化 分组的顺序 总是按发送顺序到达终点 到达终点时不一定按发送顺序 端到端的差错处理和流量控制 可以由网络负责，也可以由用户主机负责 由用户主机负责 IPv4 概述 分类编址的IPv4地址 简介 每一类地址都由两个固定长度的字段组成，其中一个字段是网络号 net-id，它标志主机（或路由器）所连接到的网络，而另一个字段则是主机号 host-id，它标志该主机（或路由器）。 主机号在它前面的网络号所指明的网络范围内必须是唯一的。 由此可见，一个 IP 地址在整个互联网范围内是唯一的。 A类地址 B类地址 C类地址 练习 总结 IP 地址的指派范围\n一般不使用的特殊的 IP 地址\nIP 地址的一些重要特点\n(1) IP 地址是一种分等级的地址结构。分两个等级的好处是：\n第一，IP 地址管理机构在分配 IP 地址时只分配网络号，而剩下的主机号则由得到该网络号的单位自行分配。这样就方便了 IP 地址的管理。 第二，路由器仅根据目的主机所连接的网络号来转发分组（而不考虑目的主机号），这样就可以使路由表中的项目数大幅度减少，从而减小了路由表所占的存储空间。 (2) 实际上 IP 地址是标志一个主机（或路由器）和一条链路的接口。\n当一个主机同时连接到两个网络上时，该主机就必须同时具有两个相应的 IP 地址，其网络号 net-id 必须是不同的。这种主机称为多归属主机 (multihomed host)。 由于一个路由器至少应当连接到两个网络（这样它才能将 IP 数据报从一个网络转发到另一个网络），因此一个路由器至少应当有两个不同的 IP 地址。 (3) 用转发器或网桥连接起来的若干个局域网仍为一个网络，因此这些局域网都具有同样的网络号 net-id。\n(4) 所有分配到网络号 net-id 的网络，无论是范围很小的局域网，还是可能覆盖很大地理范围的广域网，都是平等的。\n划分子网的IPv4地址 为什么要划分子网 在早期，IP 地址的设计存在以下问题：\nIP 地址空间的利用率有时很低。 给每一个物理网络分配一个网络号会使路由表变得太大因而使网络性能变坏。 两级的 IP 地址不够灵活。 如果想要将原来的网络划分成三个独立的网路 是否可以从主机号部分借用一部分作为子网号\n但是如果未在图中标记子网号部分，那么我们和计算机又如何知道分类地址中主机号有多少比特被用作子网号了呢？\n所以就有了划分子网的工具：子网掩码\n从 1985 年起在 IP 地址中又增加了一个“子网号字段”，使两级的 IP 地址变成为三级的 IP 地址。 这种做法叫做划分子网 (subnetting) 。 划分子网已成为互联网的正式标准协议。 如何划分子网 基本思路\n划分子网纯属一个单位内部的事情。单位对外仍然表现为没有划分子网的网络。 从主机号借用若干个位作为子网号 subnet-id，而主机号 host-id 也就相应减少了若干个位。 凡是从其他网络发送给本单位某个主机的 IP 数据报，仍然是根据 IP 数据报的目的网络号 net-id，先找到连接在本单位网络上的路由器。 然后此路由器在收到 IP 数据报后，再按目的网络号 net-id 和子网号 subnet-id 找到目的子网。 最后就将 IP 数据报直接交付目的主机。 划分为三个子网后对外仍是一个网络\n优点 \u0026gt; 1. 减少了 IP 地址的浪费 2. 使网络的组织更加灵活 3. 更便于维护和管理 划分子网纯属一个单位内部的事情，对外部网络透明，对外仍然表现为没有划分子网的一个网络。 子网掩码 (IP 地址) AND (子网掩码) = 网络地址 重要，下面很多相关知识都会用到\n你可能记住需要一些常见的二进制与十进制的对应值，例如：\n00000000 = 0 00000001 = 1 00000010 = 2 00000100 = 4 00001000 = 8 00010000 = 16 00100000 = 32 01000000 = 64 10000000 = 128 这些都是只有一位为 1 的情况。你还应该记住一些其他常见的值，\n例如 255（11111111），254（11111110），252（11111100）， 248（11111000），240（11110000），224（11100000）， 192（11000000），128（10000000）。这些都是网络掩码常用的值。\n举例 例子1\n例子2\n默认子网掩码\n总结 子网掩码是一个网络或一个子网的重要属性。 路由器在和相邻路由器交换路由信息时，必须把自己所在网络（或子网）的子网掩码告诉相邻路由器。 路由器的路由表中的每一个项目，除了要给出目的网络地址外，还必须同时给出该网络的子网掩码。 若一个路由器连接在两个子网上，就拥有两个网络地址和两个子网掩码。 无分类编址的IPv4地址 为什么使用无分类编址 无分类域间路由选择 CIDR (Classless Inter-Domain Routing)。\nCIDR 最主要的特点\nCIDR使用各种长度的“网络前缀”(network-prefix)来代替分类地址中的网络号和子网号。 IP 地址从三级编址（使用子网掩码）又回到了两级编址。 如何使用无分类编址 举例\n路由聚合（构造超网） 总结 IPv4地址的应用规划 给定一个IPv4地址快，如何将其划分成几个更小的地址块，并将这些地址块分配给互联网中不同网络，进而可以给各网络中的主机和路由器接口分配IPv4地址\n定长的子网掩码FLSM（Fixed Length Subnet Mask） 划分子网的IPv4就是定长的子网掩码\n举例\n通过上面步骤分析，就可以从子网1~8中任选5个分配给左图中的N1~N5\n采用定长的子网掩码划分，只能划分出2^n个子网，其中n是从主机号部分借用的用来作为子网号的比特数量，每个子网所分配的IP地址数量相同\n但是也因为每个子网所分配的IP地址数量相同，不够灵活，容易造成IP地址的浪费\n变长的子网掩码VLSM（Variable Length Subnet Mask） 无分类编址的IPv4就是变长的子网掩码\n举例\nIP数据报的发送和转发过程 举例\n源主机如何知道目的主机是否与自己在同一个网络中，是直接交付，还是间接交付？\n可以通过目的地址IP和源地址的子网掩码进行逻辑与运算得到目的网络地址\n如果目的网络地址和源网络地址 相同，就是在同一个网络中，属于直接交付 如果目的网络地址和源网络地址 不相同，就不在同一个网络中，属于间接交付，传输给主机所在网络的默认网关（路由器——下图会讲解）,由默认网关帮忙转发 主机C如何知道路由器R的存在？\n用户为了让本网络中的主机能和其他网络中的主机进行通信，就必须给其指定本网络的一个路由器的接口，由该路由器帮忙进行转发，所指定的路由器，也被称为默认网关\n例如。路由器的接口0的IP地址192.168.0.128做为左边网络的默认网关\n主机A会将该IP数据报传输给自己的默认网关，也就是图中所示的路由器接口0\n路由器收到IP数据报后如何转发？\n检查IP数据报首部是否出错： 若出错，则直接丢弃该IP数据报并通告源主机 若没有出错，则进行转发 根据IP数据报的目的地址在路由表中查找匹配的条目： 若找到匹配的条目，则转发给条目中指示的吓一跳 若找不到，则丢弃该数据报并通告源主机 假设IP数据报首部没有出错，路由器取出IP数据报首部各地址字段的值\n接下来路由器对该IP数据报进行查表转发\n逐条检查路由条目，将目的地址与路由条目中的地址掩码进行逻辑与运算得到目的网络地址，然后与路由条目中的目的网络进行比较，如果相同，则这条路由条目就是匹配的路由条目，按照它的下一条指示，图中所示的也就是接口1转发该IP数据报\n路由器是隔离广播域的\n静态路由配置及其可能产生的路由环路问题 概念 多种情况举例 静态路由配置\n举例\n默认路由\n举例\n默认路由可以被所有网络匹配，但路由匹配有优先级，默认路由是优先级最低的\n特定主机路由\n举例\n有时候，我们可以给路由器添加针对某个主机的特定主机路由条目\n一般用于网络管理人员对网络的管理和测试\n多条路由可选，匹配路由最具体的\n静态路由配置错误导致路由环路\n举例\n假设将R2的路由表中第三条目录配置错了下一跳\n这导致R2和R3之间产生了路由环路\n聚合了不存在的网络而导致路由环路\n举例\n正常情况\n错误情况\n解决方法\n黑洞路由的下一跳为null0，这是路由器内部的虚拟接口，IP数据报进入它后就被丢弃\n网络故障而导致路由环路\n举例\n解决方法\n添加故障的网络为黑洞路由\n假设。一段时间后故障网络恢复了\nR1又自动地得出了其接口0的直连网络的路由条目\n针对该网络的黑洞网络会自动失效\n如果又故障\n则生效该网络的黑洞网络\n总结 路由选择协议 概述 因特网所采用的路由选择协议的主要特点\n因特网采用分层次的路由选择协议\n自治系统 AS：在单一的技术管理下的一组路由器，而这些路由器使用一种 AS 内部的路由选择协议和共同的度量以确定分组在该 AS 内的路由，同时还使用一种 AS 之间的路由选择协议用以确定分组在 AS之间的路由。 自治系统之间的路由选择简称为域间路由选择，自治系统内部的路由选择简称为域内路由选择\n域间路由选择使用外部网关协议EGP这个类别的路由选择协议\n域内路由选择使用内部网关协议IGP这个类别的路由选择协议\n网关协议的名称可称为路由协议\n常见的路由选择协议\n路由器的基本结构\n路由器是一种具有多个输入端口，和输出端口的专用计算机，其任务是转发分组\n路由器结构可划分为两大部分：\n1、分组转发部分\n由三部分构成\n交换结构\n一组输入端口：\n信号从某个输入端口进入路由器\n物理层将信号转换成比特流，送交数据链路层处理\n数据链路层识别从比特流中识别出帧，去掉帧头和帧尾后，送交网络层处理\n如果送交网络层的分组是普通待转发的数据分组\n则根据分组首部中的目的地址进行查表转发\n若找不到匹配的转发条目，则丢弃该分组，否则，按照匹配条目中所指示的端口进行转发\n一组输出端口\n网络层更新数据分组首部中某些字段的值，例如将数据分组的生存时间减1，然后送交数据链路层进行封装\n数据链路层将数据分组封装成帧，交给物理层处理\n物理层将帧看成比特流将其变换成相应的电信号进行发送\n路由器的各端口还会有输入缓冲区和输出缓冲区\n输入缓冲区用来暂存新进入路由器但还来不及处理的分组\n输出缓冲区用来暂存已经处理完毕但还来不及发送的分组\n路由器的端口一般都具有输入和输出功能，这些实例分出了输入端口和输出端口是更好演示路由基本工作过程\n2、路由选择部分\n路由选择部分的核心构件是路由选择处理机，它的任务是根据所使用的路由选择协议。周期性地与其他路由器 进行路由信息的交互，来更新路由表\n如果送交给输入端口的网络层的分组是路由器之间交换路由信息的路由报文，则把这种分组送交给路由选择处理机\n路由选择处理机根据分组的内容来更新自己的路由表\n路由选择处理机还会周期性地给其他路由器发送自己所知道的路由信息\n路由信息协议RIP RIP的基本工作过程\n举例\nRIP的路由条目的更新规则\n举例1\n路由器C的表到达各目的网络的下一条都记为问号，可以理解为路由器D并不需要关心路由器C的这些内容\n假设路由器C的RIP更新报文发送周期到了，则路由器C将自己路由表中的相关路由信息封装到RIP更新报文中发送给路由器D\n路由器C能到达这些网络，说明路由器C的相邻路由器也能到达，只是比路由器C的距离大1，于是根据距离的对比，路由器D更新自己的路由表\n举例2\nRIP存在“坏消息传播得慢”的问题\n解决方法\n但是，这些方法也不能完全解决“坏消息传播得慢”的问题，这是距离向量的本质决定\n总结\nRIP 协议的优缺点\n优点：\n实现简单，开销较小。 缺点：\nRIP 限制了网络的规模，它能使用的最大距离为 15（16 表示不可达）。\n路由器之间交换的路由信息是路由器中的完整路由表，因而随着网络规模的扩大，开销也就增加。\n“坏消息传播得慢”，使更新过程的收敛时间过长。\n开放最短路径优先OSPF 开放最短路径优先 OSPF (Open Shortest Path First)\n注意：OSPF 只是一个协议的名字，它并不表示其他的路由选择协议不是“最短路径优先”。\n概念\n问候（Hello）分组\nIP数据报首部中协议号字段的取值应为89，来表明IP数据报的数据载荷为OSPF分组\n发送链路状态通告LSA\n洪泛法有点类似于广播，就是从一个接口进来，从其他剩余所有接口出去\n链路状态数据库同步\n使用SPF算法计算出各自路由器到达其他路由器的最短路径\nOSPF五种分组类型\nOSPF的基本工作过程\nOSPF在多点接入网络中路由器邻居关系建立\n如果不采用其他机制，将会产生大量的多播分组\n若DR出现问题，则由BDR顶替DR\n为了使OSPF能够用于规模很大的网络，OSPF把一个自治系统再划分为若干个更小的范围，叫做区域（Area）\n在该自治系统内，所有路由器都使用OSPF协议，OSPF将该自治系统再划分成4个更小的区域\n每个区域都有一个32比特的区域标识符\n主干区域的区域标识符必须为0，主干区域用于连通其他区域\n其他区域的区域标识符不能为0且不相同\n每个区域一般不应包含路由器超过200个\n划分区域的好处就是，利用洪泛法交换链路状态信息局限于每一个区域而不是自治系统，这样减少整个网络上的通信量\n总结\n边界网关协议BGP BGP（Border Gateway Protocol） 是不同自治系统的路由器之间交换路由信息的协议\n总结\n直接封装RIP、OSPF和BGP报文的协议 IPv4数据报的首部格式 各字段的作用 一个 IP 数据报由首部和数据两部分组成。 首部的前一部分是固定长度，共 20 字节，是所有 IP 数据报必须具有的。 在首部的固定部分的后面是一些可选字段，其长度是可变的。 图中的每一行都由32个比特（也就是4个字节）构成，每个小格子称为字段或者域，每个字段或某些字段的组合用来表达IP协议的相关功能\nIP数据报的首部长度一定是4字节的整数倍\n因为首部中的可选字段的长度从1个字节到40个字节不等，那么，当20字节的固定部分加上1到40个字节长度不等的可变部分，会造成首部长度不是4字节整数倍时，就用取值为全0的填充字段填充相应个字节，以确保IP数据报的首部长度是4字节的整数倍\n对IPv4数据报进行分片\n现在假定分片2的IP数据报经过某个网络时还需要进行分片\n总结 网际控制报文协议ICMP 概念 架构IP网络时需要特别注意两点：\n确认网络是否正常工作 遇到异常时进行问题诊断 而ICMP就是实现这些问题的协议\nICMP的主要功能包括：\n确认IP包是否成功送达目标地址 通知在发送过程当中IP包被废弃的具体原因 改善网络设置等 有了这些功能以后，就可以获得网络是否正常，设置是否有误以及设备有何异常等信息，从而便于进行网络上的问题诊断\nICMP 不是高层协议（看起来好像是高层协议，因为 ICMP 报文是装在 IP 数据报中，作为其中的数据部分），而是 IP 层的协议\nICMP 报文的格式\nICMP差错报告报文 终点不可达 源点抑制 时间超过 参数问题 改变路由（重定向） 不应发送ICMP差错报告报文情况 ICMP应用举例 分组网间探测PING（Packet InterNet Groper） 跟踪路由（traceroute） tracert命令的实现原理\n总结 虚拟专用网VPN与网络地址转换NAT 虚拟专用网VPN（Virtual Private Network） 由于 IP 地址的紧缺，一个机构能够申请到的IP地址数往往远小于本机构所拥有的主机数。 考虑到互联网并不很安全，一个机构内也并不需要把所有的主机接入到外部的互联网。 假定在一个机构内部的计算机通信也是采用 TCP/IP 协议，那么从原则上讲，对于这些仅在机构内部使用的计算机就可以由本机构自行分配其 IP 地址。 上图是因特网数字分配机构IANA官网查看IPv4地址空间中特殊地址的分配方案\n用粉红色标出来的地址就是无需申请的、可自由分配的专用地址，或称私有地址\n私有地址只能用于一个机构的内部通信，而不能用于和因特网上的主机通信\n私有地址只能用作本地地址而不能用作全球地址\n因特网中所有路由器对目的地址是私有地址的IP数据报一律不进行转发\n本地地址与全球地址\n本地地址——仅在机构内部使用的 IP 地址，可以由本机构自行分配，而不需要向互联网的管理机构申请。 全球地址——全球唯一的 IP 地址，必须向互联网的管理机构申请。 问题：在内部使用的本地地址就有可能和互联网中某个 IP 地址重合，这样就会出现地址的二义性问题。 所以部门A和部门B至少需要一个 路由器具有合法的全球IP地址，这样各自的专用网才能利用公用的因特网进行通信\n部门A向部门B发送数据流程\n两个专用网内的主机间发送的数据报是通过了公用的因特网，但在效果上就好像是在本机构的专用网上传送一样\n数据报在因特网中可能要经过多个网络和路由器，但从逻辑上看，R1和R2之间好像是一条直通的点对点链路\n因此也被称为IP隧道技术\n网络地址转换NAT（Network Address Translation） 举例\n使用私有地址的主机，如何才能与因特网上使用全球IP地址的主机进行通信？\n这需要在专用网络连接到因特网的路由器上安装NAT软件\n专有NAT软件的路由器叫做NAT路由器\n它至少有一个有效的外部全球IP地址\n这样，所有使用私有地址的主机在和外界通信时，都要在NAT路由器上将其私有地址转换为全球IP地址\n假设，使用私有地址的主机要给因特网上使用全球IP地址的另一台主机发送IP数据报\n因特网上的这台主机给源主机发回数据报\n当专用网中的这两台使用私有地址的主机都要给因特网使用全球地址的另一台主机发送数据报时，在NAT路由器的NAT转换表中就会产生两条记录，分别记录两个私有地址与全球地址的对应关系\n这种基本转换存在一个问题\n解决方法\n我们现在用的很多家用路由器都是这种NART路由器\n内网主机与外网主机的通信，是否能由外网主机首先发起？\n否定\n总结 ","date":"2023-07-15T12:44:47+08:00","permalink":"https://blog.importzhh.me/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%BE%AE%E8%AF%BE%E5%A0%82-%E7%BD%91%E7%BB%9C%E5%B1%82/","title":"Ⅳ: 计算机网络微课堂-网络层"},{"content":"运输层概述 概念 进程之间的通信\n从通信和信息处理的角度看，运输层向它上面的应用层提供通信服务，它属于面向通信部分的最高层，同时也是用户功能中的最低层。 当网络的边缘部分中的两个主机使用网络的核心部分的功能进行端到端的通信时，只有位于网络边缘部分的主机的协议栈才有运输层，而网络核心部分中的路由器在转发分组时都只用到三层（到网络层）的功能。 进程之间通信流程\n“逻辑通信”是指运输层之间的通信好像是沿水平方向传送数据，但事实上，这两条数据并没有一条水平方向的物理连接，要传送的数据是沿着图中上下多次的虚线方向传送的\n进程Ap1与Ap4之间进行基于网络的通信，进程Ap2与Ap3之间进行基于网络的通信\n在运输层使用不同的端口，来对应不同的应用进程\n然后通过网络层及其下层来传输应用层报文\n接收方的运输层通过不同的端口，将收到的应用层报文，交付给应用层中相应的应用进程\n这里端口并不是指看得见、摸得着的物理端口，而是指用来区分不同应用进程的标识符\n总结 运输层端口号、复用与分用的概念 为什么用端口号 发送方的复用和接收方的分用 多个进程（这里一个端口表示一个进程） 利用一个运输层协议（或者称为运输层接口）发送数据称为 复用\n多个进程（这里一个端口表示一个进程） 利用一个运输层协议（或者称为运输层接口）接收时叫做 分用。\nTCP/IP体系的应用层常用协议所使用的运输层熟知端口号 运输层传输流程 举例\n在浏览器输入域名，回车浏览\n然后用户PC中的DNS客户端进程会发送一个DNS查询请求报文\nDNS查询请求报文需要使用运输层的UDP协议\n首部中的源端口字段的值，在短暂端口号49151~65535中挑选一个未被占用的，用来表示DNS客户端进程\n首部中的目的端口字段的值：53，是DNS服务器端进程所使用的熟知端口号\n之后，将UDP用户数据报封装在IP数据报中，通过以太网发送给DNS服务器\nDNS服务器收到该IP数据报后，从中解封出UDP用户数据报\nUDP首部中的目的端口号为53，这表明应将该UDP用户数据报的数据载荷部分，也就是DNS查询请求报文，交付给本服务器中的DNS服务器端进程\nDNS服务器端进程解析DNS查询请求报文的内容，然后按其要求查找对应的IP地址\n之后，会给用户PC发送DNS响应报文，DNS响应报文需要使用运输层的UDP协议封装成UDP用户数据报\n其首部中的源端口字段的值设置为熟知端口号53，表明这是DNS服务器端进程所发送的UDP用户数据报，目的端口的值设置为49152，这是之前用户PC中发送DNS查询请求报文的DNS客户端进程所使用的短暂端口号\n将UDP用户数据报封装在IP数据报中，通过以太网发送给用户PC\n用户PC收到该数据报后，从中解封出UDP用户数据报\nUDP首部中的目的端口号为49152，这表明应将该UDP用户数据报的数据载荷部分，也就是DNS响应报文，交付给用户PC中的DNS客户端进程\nDNS客户端进程解析DNS响应报文的内容，就可知道自己之前所请求的Web服务器的域名对应的IP地址\n现在用户PC中的HTTP客户端进程可以向Web服务器发送HTTP请求报文（和DNS发送和接收流程差不多）\nUDP和TCP的对比 概念 UDP 和 TCP 是TCP/IP体系结构运输层中的两个重要协议 当运输层采用面向连接的 TCP 协议时，尽管下面的网络是不可靠的（只提供尽最大努力服务），但这种逻辑通信信道就相当于一条全双工的可靠信道。 当运输层采用无连接的 UDP 协议时，这种逻辑通信信道是一条不可靠信道。 可靠信道与不可靠信道\n两个对等运输实体在通信时传送的数据单位叫作运输协议数据单元 TPDU (Transport Protocol Data Unit)。\nTCP 传送的数据单位协议是 TCP 报文段(segment)。\nUDP 传送的数据单位协议是 UDP 报文或用户数据报。\nUDP的通信是无连接的，不需要套接字（Socket）\nTCP是面向连接的，TCP之间的通信必须要在两个套接字（Socket）之间建立连接\n用户数据报协议UDP（User Datagram Protocol） 可以发送广播\n可以向某个多播组发送多播\n还可以发送单播\nUDP 支持单播、多播以及广播\n换句话说，UDP支持一对一，一对多，以及一对全的通信\n运输过程\nUDP对应用进程交下来的报文既不合并也不拆分，而是保留这些报文的边界\n换句话说，UDP是面向应用报文的\nUDP向上层提供无连接不可靠传输服务\nUDP结构\n传输控制协议TCP（Transmission Control Protocol） 使用TCP协议的通信双方，在进行数据传输之前，必须使用“三报文握手”建立TCP连接\nTCP连接建立成功后，通信双方之间就好像有一条可靠的通信信道，通信双方使用这条基于TCP连接的可靠信道进行通信\n很显然，TCP仅支持单播，也就是一对一的通信\n运输过程\n发送方\nTCP会把应用进程交付下来的数据块看作是一连串无结构的字节流，TCP并不知道这些待传送的字节流的含义\n并将他们编号，并存储在自己发送缓存中\nTCP会根据发送策略，提取一定量的字节构建TCP报文并发送\n接收方\n一方面从所接受到的TCP报文段中，取出数据载荷部分并存储在接收缓存中；一方面将接收缓存中的一些字节交付给应用进程 TCP不保证接收方应用进程所收到的数据块与发送方发送的数据块，具有对应大小的关系（例如，发送方应用进程交给发送方的TCP共10个数据块，但接收方的TCP可能只用了4个数据块，就把收到的字节流交付给了上层的应用进程，但接收方收到的字节流必须和发送方应用进程发出的字节流完全一样） 接收方的应用进程必须有能力识别收到的字节流，把它还原成有意义的应用层数据 TCP是面向字节流的，这正是TCP实现可靠传输、流量控制、以及拥塞控制的基础\n本图只画了一个方向的数据流，在实际网络中，基于TCP连接的两端，可以同时进行TCP报文段的发送和接收\nTCP向上层提供面向连接的可靠传输服务\nTCP结构\n总结 TCP的流量控制 概念 举例\n具体流程的视频\n上图主机A现在可将发送缓存中序号1~200的字节数据全部删除，因为已经收到了主机B对它们的累计确认\n上图主机A现在可将发送缓存中序号201~500的字节数据全部删除，因为已经收到了主机B对它们的累计确认\n上图主机A现在可将发送缓存中序号501~600的字节数据全部删除，因为已经收到了主机B对它们的累计确认\n上图如果零窗口探测报文在发送过程中如果丢失，还是能打破死锁局面\n因为零窗口探测报文段也有重传计时器，重传计时器超时后，零窗口探测报文段会被重传\n总结 TCP的拥塞控制 概念 网络拥塞往往是由许多因素引起的。例如：\n点缓存的容量太小； 链路的容量不足； 处理机处理的速率太慢； 拥塞本身会进一步加剧拥塞； 拥塞控制的一般原理\n拥塞控制的前提：网络能够承受现有的网络负荷。 实践证明，拥塞控制是很难设计的，因为它是一个动态问题。 分组的丢失是网络发生拥塞的征兆而不是原因。 在许多情况下，甚至正是拥塞控制本身成为引起网络性能恶化、甚至发生死锁的原因。 开环控制和闭环控制\n监测网络的拥塞\n主要指标有：\n由于缺少缓存空间而被丢弃的分组的百分数； 平均队列长度； 超时重传的分组数； 平均分组时延； 分组时延的标准差，等等。 上述这些指标的上升都标志着拥塞的增长。\n拥塞控制的算法 真正的发送窗口值 = Min (接收方窗口值，拥塞窗口值)\n下图的实例横纵坐标的意思\n传输轮次：\n发送方给接收方发送数据报文段后，接收方给发送方发发回相应的确认报文段 一个传输轮次所经历的时间其实就是往返时间，往返时间并非是恒定的数值 使用传输轮次是为了强调把拥塞窗口所允许发送的报文段都连续发送出去，并受到了对已发送的最后一个报文段的确认 拥塞窗口：\n它会随网络拥塞程度，以及所使用的拥塞控制算法动态变化 慢开始和拥塞避免 慢开始（slow-start） 目的：用来确定网络的负载能力或拥塞程度。 算法的思路：由小到大逐渐增大拥塞窗口数值。 两个变量： 拥塞窗口（cwnd）：初始拥塞窗口值：2 种设置方法。窗口值逐渐增大。 1 至 2 个最大报文段 （旧标准） 2 至 4 个最大报文段 （RFC 5681） 慢开始门限（ssthresh）：防止拥塞窗口增长过大引起网络拥塞。 图中swnd是发送窗口\n每经过一个传输轮次，拥塞窗口就加倍\n窗口大小按指数增加，2的n-1次方\n拥塞避免（congestion avoidance） 思路：让拥塞窗口 cwnd 缓慢地增大，避免出现拥塞。 每经过一个传输轮次，拥塞窗口 cwnd = cwnd + 1。 使拥塞窗口 cwnd 按线性规律缓慢增长。 在拥塞避免阶段，具有 “加法增大” (Additive Increase) 的特点。 如果在发送过程中出现部分报文段丢失，这必然会造成发送方对这些丢失报文段的超时重传\n这个时候又回到了慢开始\n两个算法完整示意图 快重传和快恢复 快重传（fast retrasmit） 快恢复（fast recovery） 改进后的整体算法的示意图 TCP超时重传时间的选择 如果超时重传时间RTO的值设置得比RTT0的值小很多，这会引起报文段不必要的重传，使网络负荷增大\n如果超时重传时间RTO的值设置得远大于RTT0的值，这会使重传时间推迟的太长，使网络的空闲时间增大，降低传输效率\nRFC6298建议使用下式计算超时重传时间RTO\n往返时间RTT的测量比较复杂\nTCP超时重传的计算\n举例\n总结\nTCP可靠传输的实现 本集具体讲解\nTCP的运输连接管理 概念 TCP的连接建立 TCP 建立连接的过程叫做握手。 握手需要在客户和服务器之间交换三个 TCP 报文段。称之为三报文握手。 采用三报文握手主要是为了防止已失效的连接请求报文段突然又传送到了，因而产生错误。 TCP的连接建立要解决以下三个问题 TCP使用“三报文握手”建立连接 TCP 连接的建立采用客户服务器方式。 主动发起连接建立的应用进程叫做TCP客户 (client)。 被动等待连接建立的应用进程叫做TCP服务器 (server)。 “握手”需要在TCP客户端和服务器之间交换三个TCP报文段\n过程\n最初两端的TCP进程都处于关闭状态\n一开始，TCP服务器进程首先创建传输控制块，用来存储TCP连接中的一些重要信息。例如TCP连接表、指向发送和接收缓存的指针、指向重传队列的指针，当前的发送和接收序号等\n之后，就准备接受TCP客户端进程的连接请求\n此时，TCP服务器进程就进入监听状态，等待TCP客户端进程的连接请求\nTCP服务器进程是被动等待来自TCP客户端进程的连接请求，因此成为被动打开连接\nTCP客户进程也是首先创建传输控制块\n由于TCP连接建立是由TCP客户端主动发起的，因此称为主动打开连接\n然后，在打算建立TCP连接时，向TCP服务器进程发送TCP连接请求报文段，并进入同步已发送状态\nTCP连接请求报文段首部中\n同步位SYN被设置为1，表明这是一个TCP连接请求报文段 序号字段seq被设置了一个初始值x，作为TCP客户端进程所选择的初始序号 请注意：TCP规定SYN被设置为1的报文段不能携带数据，但要消耗掉一个序号\nTCP服务器进程收到TCP连接请求报文段后，如果同意建立连接，则向TCP客户进程发送TCP连接请求确认报文段，并进入同步已接收状态\nTCP连接请求确认报文段首部中\n同步位SYN和确认为ACK都设置为1，表明这是一个TCP连接请求确认报文段 序号字段seq被设置了一个初始值y，作为TCP服务器进程所选择的初始序号， 确认号字段ack的值被设置成了x+1，这是对TCP客户进程所选择的初始序号（seq）的确认 请注意：这个报文段也不能携带数据，因为它是SYN被设置为1的报文段，但同样要消耗掉一个序号\nTCP客户进程收到TCP连接请求确认报文段后，还要向TCP服务器进程发送一个普通的TCP确认报文段，并进入连接已连接状态\n普通的TCP确认报文段首部中\n确认位ACK被设置为1，表明这是一个普通的TCP确认报文段 序号字段seq被设置为x+1，这是因为TCP客户进程发送的第一个TCP报文段的序号为x，所以TCP客户进程发送的第二个报文段的序号为x+1 确认号字段ack被设置为y+1，这是对TCP服务器进程所选择的初始序号的确认 请注意：TCP规定普通的TCP确认报文段可以携带数据，但如果不携带数据，则不消耗序号\nTCP服务器进程收到该确认报文段后也进入连接已建立状态\n现在，TCP双方都进入了连接已建立状态，它们可以基于已建立好的TCP连接，进行可靠的数据传输\n为什么TCP客户进程最后还要发送一个普通的TCP确认报文段？能否使用“两报文握手”建立连接？\n下图实例是“两报文握手”\n为了防止已经失效的连接请求报文段突然又传到服务端，因而产生错误”，这种情况是：一端(client)A发出去的第一个连接请求报文并没有\u0026gt; 丢失，而是因为某些未知的原因在某个网络节点上发生滞留，导致延迟到连接释放以后的某个时间才到达另一端(server)B。本来这是一个\u0026gt; 早已失效的报文段，但是B收到此失效的报文之后，会误认为是A再次发出的一个新的连接请求，于是B端就向A又发出确认报文，表示同\u0026gt; 意建立连接。如果不采用“三次握手”，那么只要B端发出确认报文就会认为新的连接已经建立了，但是A端并没有发出建立连接的请求，因\u0026gt; 此不会去向B端发送数据，B端没有收到数据就会一直等待，这样B端就会白白浪费掉很多资源。\n所以并不多余，这是为了防止已失效的连接请求报文段突然又传送到了TCP服务器，因而导致错误\n总结 TCP的连接释放 TCP 连接释放过程比较复杂。 数据传输结束后，通信的双方都可释放连接。 TCP 连接释放过程是四报文握手。 TCP通过“四报文挥手”来释放连接 TCP 连接的建立采用客户服务器方式。 主动发起连接建立的应用进程叫做TCP客户 (client)。 被动等待连接建立的应用进程叫做TCP服务器 (server)。 任何一方都可以在数据传送结束后发出连接释放的通知 过程\n现在TCP客户进程和TCP服务器进程都处于连接已建立状态\nTCP客户进程的应用进程通知其主动关闭TCP连接\nTCP客户进程会发送TCP连接释放报文段，并进入终止等待1状态\nTCP连接释放报文段首部中\n终止位FIN和确认为ACK的值都被设置为1，表明这是一个TCP连接释放报文段，同时也对之前收到的报文段进行确认 序号seq字段的值设置为u，它等于TCP客户进程之前已传送过的数据的最后一个字节的序号加1 确认号ack字段的值设置为v，它等于TCP客户进程之前已收到的、数据的最后一个字节的序号加1 请注意：TCP规定终止位FIN等于1的报文段即使不携带数据，也要消耗掉一个序号\nTCP服务器进程收到TCP连接释放报文段后，会发送一个普通的TCP确认报文段并进入关闭等待状态\n普通的TCP确认报文段首部中\n确认位ACK的值被设置为1，表明这是一个普通的TCP确认报文段 序号seq字段的值设置为v，它等于TCP服务器进程之前已传送过的数据的最后一个字节的序号加1，这也与之前收到的TCP连接释放报文段中的确认号匹配 确认号ack字段的值设置为u+1，这是对TCP连接释放报文段的确认 TCP服务器进程应该通知高层应用进程，TCP客户进程要断开与自己的TCP连接\n此时，从TCP客户进程到TCP服务器进程这个方向的连接就释放了\n这时的TCP连接属于半关闭状态，也就是TCP客户进程已经没有数据要发送了\n但如果TCP服务器进程还有数据要发送，TCP客户进程仍要接收，也就是说从TCP服务器进程到TCP客户进程这个方向的连接并未关闭\nTCP客户进程收到TCP确认报文段后就进入终止等待2状态，等待TCP服务器进程发出的TCP连接释放报文段\n若使用TCP服务器进程的应用进程已经没有数据要发送了，应用进程就通知其TCP服务器进程释放连接\n由于TCP连接释放是由TCP客户进程主动发起的，因此TCP服务器进程对TCP连接的释放称为被动关闭连接\nTCP服务器进程发送TCP连接释放报文段并进入最后确认状态\n该报文段首部中\n终止位FIN和确认位ACK的值都被设置为1，表明这是一个TCP连接释放报文段，同时也对之前收到的报文段进行确认 序号seq字段的值为w，这是因为在半关闭状态下，TCP服务器进程可能又发送 确认号ack字段的值为u+1，这是对之前收到的TCP连接释放报文段的重复确认 TCP客户进程收到TCP连接释放报文段后，必须针对该报文段发送普通的TCP确认报文段，之后进入时间等待状态\n该报文段首部中\n确认为ACK的值被设置为1，表明这是一个普通的TCP确认报文段 序号seq字段的值设置为u+1，这是因为TCP客户进程之前发送的TCP连接释放报文段虽然不携带数据，但要消耗掉一个序号 确认号ack字段的值设置为w+1，这是对所收到的TCP连接释放报文段的确认 TCP服务器进程收到该报文段后就进入关闭状态，而TCP客户进程还要进过2MSL后才能进入关闭状态\nTCP客户进程在发送完最后一个确认报文后，为什么不直接进入关闭状态？而是要进入时间等待状态？\n因为时间等待状态以及处于该状态2MSL时长，可以确保TCP服务器进程可以收到最后一个TCP确认报文段而进入关闭状态\n另外，TCP客户进程在发送完最后一个TCP确认报文段后，在经过2MSL时长，就可以使本次连接持续时间内所产生的所有报文段都从网络中消失，这样就可以使下一个新的TCP连接中，不会出现旧连接中的报文段\nTCP保活计时器的作用 TCP双方已经建立了连接，后来，TCP客户进程所在的主机突然出现了故障\nTCP服务器进程以后就不能再收到TCP客户进程发来的数据\n因此，应当有措施使TCP服务器进程不要再白白等待下去\nTCP报文段的首部格式 各字段的作用 源端口和目的端口\n序号、确认号和确认标志位\n数据偏移、保留、窗口和校验和\n同步标志位、终止标志位、复位标志位、推送标志位、紧急标志位和紧急指针\n选项和填充\n习题 TCP的流量控制 TCP的拥塞控制 TCP可靠传输的实现 TCP的运输连接管理 ","date":"2023-07-15T12:44:47+08:00","permalink":"https://blog.importzhh.me/p/v-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%BE%AE%E8%AF%BE%E5%A0%82-%E8%BF%90%E8%BE%93%E5%B1%82/","title":"V: 计算机网络微课堂-运输层"},{"content":"应用层概述 我们在浏览器的地址中输入某个网站的域名后，就可以访问该网站的内容，这个就是万维网WWW应用，其相关的应用层协议为超文本传送协议HTTP\n用户在浏览器地址栏中输入的是“见名知意”的域名，而TCP/IP的网际层使用IP地址来表示目的主机，从域名到IP地址的装转换，由属于应用层范畴的域名系统DNS，在后台帮用户完成\n常见的应用\n总结\n客户/服务器方式（C/S方式）和对等方式（P2P方式） 概念 客户/服务器方式（C/S方式） 对等方式（P2P方式） 总结 动态主机配置协议DHCP 概念 互联网广泛使用的动态主机配置协议 DHCP (Dynamic Host Configuration Protocol) 提供了即插即用连网 (plug-and-play networking) 的机制。 这种机制允许一台计算机加入新的网络和获取 IP 地址，而不用手工配置。 DHCP的作用 在之前，如果要正常访问Web服务器，需要该主机手动配置网络的相关配置信息\n如果主机数很多，就工作量大，容易出错\n如果我们给网络中添加一台DHCP服务器\nDHCP的工作过程 DHCP 使用客户 - 服务器方式\n需要 IP 地址的主机在启动时就向 DHCP 服务器广播发送发现报文 （DHCP DISCOVER），这时该主机就成为 DHCP 客户。 本地网络上所有主机都能收到此广播报文，但只有 DHCP 服务器才回答此广播报文。 DHCP 服务器先在其数据库中查找该计算机的配置信息。若找到，则返回找到的信息。若找不到，则从服务器的 IP 地址池 (address pool) 中取一个地址分配给该计算机。DHCP服务器的回答报文叫做提供报文（DHCP OFFER）。 DHCP 工作方式\nDHCP 使用客户-服务器方式，采用请求/应答方式工作。 DHCP 基于 UDP 工作（DHCP报文在运输层会被封装成为UDP用户数据报），DHCP 服务器运行在 67 号端口， DHCP客户运行在 68 号端口。 DHCP交互过程\nDHCP客户端将广播发送DHCP发现报文（DHCP DISCOVER）\n事务ID DHCP客户端的MAC地址 封装该报文的IP数据报的源IP地址为0.0.0.0，这是因为主机目前还未分配到IP地址，因此使用该地址代替\n目的IP地址为广播地址255.255.255.255，之所以广播发送，是因为主机现在并不知道网络中有哪几个DHCP服务器。它们的IP地址各是什么\nDHCP服务器收到DHCP发现报文后，根据其中封装的DHCP客户端的MAC地址来查找自己的数据库，如果查到匹配信息，则使用这些配置信息来构建并发送DHCP提供报文，如果没有则采用默认配置信息来构建报文并发送\nDHCP服务端将广播发送DHCP提供报文（DHCP OFFER）\n事务ID：DHCP客户端会与之前DHCP发现报文的事务ID做对比，来判断该DHCP提供报文是否是自己的 配置信息： \u0026gt; * IP地址：DHCP服务器从自己的IP地址池中挑选待租用给主机的IP地址（使用ARP来确保所选IP地址未被网络中其他主机占用） 子网掩码 地址租期 默认网关 DNS服务器 源IP地址：发送DHCP提供报文的DHCP服务器的IP\n目的地址：因为目的主机还没分配到IP，所以使用广播地址\n在本例中，DHCP客户会收到两个DHCP服务器发来的DHCP提供报文，DHCP客户从中选择一个，一般选择先到的，并向所选择的DHCP服务器发送DHCP请求报文\nDHCP客户端将广播发送DHCP请求报文（DHCP REQUEST）\n事务ID DHCP客户端的MAC地址 接收的租约中的IP地址 提供此租约的DHCP服务器端的IP地址 源地址：0.0.0.0，因为此时DHCP客户才从多个DHCP服务器中挑选一个作为自己的DHCP服务器。它首先要征得该服务器的同意，之后才能正式使用向该DHCP服务器租用的IP地址\n目的地址：广播地址，这样可以一次性向所有DHCP服务器发送DHCP请求报文，来告知它们是否请求它们作为自己的DHCP服务器\n在本例中，假设DHCP客户端选择DHCP服务器1作为自己的DHCP服务器，DHCP服务器1接受该请求，于是DHCP服务器1给DHCP客户端发送DHCP确认报文\n源地址：DHCP服务器1的IP地址\n目的地址：广播地址\nDHCP客户收到该报文后就可以使用租用的IP地址\n在使用前还会进行ARP检测\n剩下流程图示\nDHCP中继代理 下图的网络拓扑中的各主机是否可以通过DHCP来自动获取到网络配置？\n使用DHCP中继代理是因为我们不用给每一个网络上都设置一个DHCP服务器，这样会使DHCP服务器的数量太多\n总结 域名系统DNS 概述 域名相比IP地址更容易记忆\n因特网是否可以只使用一台DNS服务器？\n不行\n名称相同的域名其等级未必相同\n域名解析过程 总结 文件传送协议FTP 概念 文件传送协议FTP的应用 FTP采用C/S方式（客户/服务器方式）\nFTP客户计算机可将各种类型的文件上传到FTP服务器计算机\nFTP客户计算机也可以从FTP服务器计算机下载文件\nFTP基本工作原理 FTP服务器监听熟知端口（端口号为 21），使客户进程能够连接上。\nFTP客户随机选择一个临时端口号与其建立TCP连接，这条TCP连接用于FTP客户与服务器之间传送FTP的相关控制命令（这条连接是FTP客户与服务器之间的命令通道）\n下图为建立数据通道的TCP连接\nFTP服务器使用自己的熟知端口号20与其建立TCP连接，这条TCP连接用于FTP客户与服务器之间传送文件\n上面例子是主动模式：建立数据通道时，FTP服务器主动连接FTP客户\n下图实例为被动模式\n两种模式对比\n注意两种模式都是\n控制连接在整个会话期间保持打开状态\n数据连接传输完毕后就关闭\n总结 电子邮件 概念 邮件发送和接收过程 简单邮件传送协议SMTP（Simple Mail Transfer Protocol）的基本工作原理 电子邮件的信息格式 邮件读取 基于万维网的电子邮件 总结 万维网WWW 概念 概述\n万维网 WWW (World Wide Web) 并非某种特殊的计算机网络。 万维网是一个大规模的、联机式的信息储藏所。 万维网用链接的方法能非常方便地从互联网上的一个站点访问另一个站点，从而主动地按需获取丰富的信息。 这种访问方式称为“链接”。 万维网的工作方式\n万维网以客户 - 服务器方式工作。 浏览器就是在用户计算机上的万维网客户程序。万维网文档所驻留的计算机则运行服务器程序，因此这个计算机也称为万维网服务器。 客户程序向服务器程序发出请求，服务器程序向客户程序送回客户所要的万维网文档。 在一个客户程序主窗口上显示出的万维网文档称为页面 (page)。 万维网应用举例\n访问网页\n怎样标志分布在整个互联网上的万维网文档？\n万维网的文档\n超文本传输协议HTTP（Hyper Transfer Protocol） 概念和传输过程 在万维网客户程序与万维网服务器程序之间进行交互所使用的协议，是超文本传送协议 HTTP (HyperText Transfer Protocol)。 HTTP 是一个应用层协议，它使用 TCP 连接进行可靠的传送。 每个万维网网点都有一个服务器进程，它不断地监听 TCP 的端口 80，以便发现是否有浏览器向它发出连接建立请求。 一旦监听到连接建立请求并建立了 TCP 连接之后，浏览器就向万维网服务器发出浏览某个页面的请求，服务器接着就返回所请求的页面作为响应。 最后，TCP 连接就被释放了。 HTTP报文格式 HTTP请求报文格式\nHTTP响应报文格式\n使用Cookie在服务器上记录用户信息 万维网缓存与代理服务器 如果该请求有缓存\n如果该请求没有缓存\n若WEb缓存的命中率比较高\n则会大大减小了该链路上的通信量，因而减少了访问因特网的时延\n假设原始服务器的文档被更改，这样代理服务器的文档就不是最新的\n所以原始服务器通常会为每个响应的对象设定一个修改时间字段和一个有效日期字段\n若未过期\n若过期并且代理服务器的文档和原始服务器的文档一致，原始服务器则给代理服务器发送不包含实体主体的响应\n若过期并且代理服务器的文档和原始服务器的文档不一致，原始服务器则给代理服务器发送封装有该文档的响应报文\n总结 习题 域名系统DNS 文件传送协议FTP 电子邮件 超文本传输协议HTTP ","date":"2023-07-15T12:44:47+08:00","permalink":"https://blog.importzhh.me/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%BE%AE%E8%AF%BE%E5%A0%82-%E5%BA%94%E7%94%A8%E5%B1%82/","title":"Ⅵ: 计算机网络微课堂-应用层"},{"content":"DNS污染 DNS污染的原理是，攻击者在你查找电话号码（即请求DNS解析）的过程中，用假的电话协议（即篡改的IP地址）替换了真正的电话协议。这就导致当你拨打电话（访问网站）时，实际上连接的是一个错误的或恶意的地方。\n以一个具体的例子来说，假设你想要访问一个网站，例如www.baidu.com。在正常情况下，你的计算机会发送一个DNS请求到DNS服务器，以找出这个域名对应的IP地址。然后，你的计算机将用这个IP地址来和网站建立连接。\n但是，在DNS污染攻击中，当你的计算机向DNS服务器发送请求时，攻击者可能会干扰这个请求的响应，使DNS服务器返回一个错误的IP地址（这个地址可能指向一个攻击者控制的，看起来与真正银行网站相似的钓鱼网站）。因此，你的计算机会被引导到错误的网站，而你可能会在不知情的情况下，将你的用户名和密码输入到这个伪造的网站中。\n预防DNS污染的策略主要包括使用DNSSEC（DNS安全扩展）以验证DNS响应的真实性，或使用VPN来创建一个安全的网络连接。\nTCP重置攻击 TCP重置攻击（TCP Reset Attack）是一种网络攻击方式，攻击者通过发送伪造的TCP RST（Reset）报文来中断TCP连接。在TCP协议中，RST报文是用来中断一个已经建立的连接的。\n在TCP协议中，每个TCP报文都有一个序列号，这个序列号用于保证数据的有序传输和对失序、丢失数据的重传。在TCP连接中，如果接收到一个RST报文，且这个报文的序列号在当前的有效窗口内，那么TCP连接就会被立即中断。\nTCP重置攻击的原理是，攻击者首先要监听或预测出TCP连接的序列号，然后伪造一个RST报文，把序列号设置在有效窗口内，然后发送给通信的一方或双方。由于接收方收到了序列号有效的RST报文，所以会立即中断连接，导致通信失败。\n举个例子，假设Alice和Bob正在进行TCP连接通信，攻击者Evil想要中断他们的通信。首先，Evil需要知道Alice和Bob通信的TCP序列号，这可能通过嗅探网络流量或者预测序列号获得。然后，Evil伪造一个RST报文，把序列号设置在Alice和Bob通信的有效窗口内，然后发送给Alice或Bob。当Alice或Bob收到这个RST报文，就会认为对方要求中断连接，从而立即中断TCP连接。\nhttps://zhuanlan.zhihu.com/p/468184471\nss 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 假如搭建了一个ss的服务端代理 服务端口8388 同时本机运行了一个ss的客户端代理 监听的本地端口1080 同时 我们的浏览器设置了一个代理端口 也就是说 我们浏览器的访问请求会先走这个代理的端口 就相当于我们在浏览器里面发送一个访问谷歌请求的时候 这个请求就会转交到我们的1080端口 客户端拿到这么一个请求 他会对这个请求进行加密 把里面的内容加密 加密之后 我们是看不到数据包里面的内容 然后他会从这个应用层来到这个传输层 传输层会干什么 添加端口对不对 那么他会从源端口是监听1080 目标端口就是我们在这个ss客户端里面配置好的那个需要连接的那个服务器 也就是8388 然后构建这么一个数据包 那么这个数据包来到网络层 网络层的话是添加ss服务器ip地址 注意 ss服务器ip一般都是国际互联网的ip 那么这个数据包就会来到防火墙这边 防火墙拆给你的数据包一看 这个6.6.6.6 没有在我的黑名单里面 然后再拆开数据这里一看 由于被加密了 所以说他就会放行通过来到这里之后 跳跳跳将会路由到这台服务器 这服务器拿到数据后进行解密之后 他就会获得原始的请求 也就是说 我们要访问google的这个请求 那么他得到这个请求之后 你小子想要访问谷歌对不对 那么我就帮你去访问 于是我他代替你去访问谷歌 他会从谷歌这里得到一份响应 首页的数据 这是谷歌返回给ss代理的响应 收到数据之后 同样的也会对数据进行加密 那么加密之后的这个数据会重新构造一个数据包从他的8388端口发往你的1080端口 源地址和目标地址 换一下 他会回应这么一个数据报 来到防火墙这一看这个6.6.6.6这个ip地址不在我的黑名单列表 同时这个数据包的内容由于加密了 于是乎他就放行通过来到你的电脑同样的一层一层解开 最后你的ss客户端得到这么一个数据包 你拿到这个加密的数据包 也可以进行解密 解密之后将会获得 首页的内容 你的SS客户端得到这个数据之后 将会 把它返回给浏览器 那么浏览器拿到这个数据之后 它就可以愉快的访问内容了 但是防火墙除了这种被动的接受这个加密数据之外 他还可以发起主动探测 也就是防火墙主动往你这里面发送请求 如果说你所用的这个ss协议存在漏洞 那么他可以通过一些探测手段来检查你这个服务器是不是运行了ss的服务 中间人攻击 在中间人攻击中，攻击者位于通信的两个端点之间，会将自己伪装成这两个通信端点的另一方，收到一方的信息后，修改信息然后发送给另一方，或者完全控制通信信息。这样，通信两方可能会认为他们正在和预期的通信方进行通信，但实际上所有的信息都被攻击者截取和控制了。\n中间人攻击可以在许多场景中发生，例如公共WiFi网络、电子邮件通信、网络银行等。而防止中间人攻击的主要手段之一就是加密，如使用HTTPS协议进行通信，或者使用VPN等其他加密通道。虽然加密无法阻止中间人拦截通信，但是由于他们无法解密通信内容，因此除非他们能获取到解密的密钥，否则他们无法理解也无法修改通信内容。\nss缺陷 密码学安全：ss支持各种加密方法，从最初的AES-256-CFB到现在的更为现代的AEAD密码，例如ChaCha20-Poly1305或AES-128-GCM。然而，这依赖于用户选用比较安全的加密算法并使用足够复杂的密码来保证加密的强度。如果用户选择了较弱的加密算法，或者使用了简单的密码，那么他们的通信内容可能被破解。\n时间戳泄漏：ss在OTA（One-Time Authentication）模式中使用时间戳来作为一个部分的key。固不能清楚地泄露通信内容，但可能泄露一些信息，例如通过观察时间戳的变化，攻击者可能能猜测出通信的模式或行为。\n对重放攻击的防护不完全：尽管ss在设计时考虑了防重放攻击，并在后来加入了OTA来进一步加强防护，但有研究指出，在某些特定情况下，ss仍可能受到重放攻击。这需要用户在使用时做好其它安全措施，例如使用最新版本的软件，及时更新和打补丁。\n流量特征识别：虽然ss采用了流加密和混淆技术来尽可能减少流量特征，使得内容被审查的可能性降低，但目前的深度包检查（DPI）技术和机器学习技术在某些场景下仍有可能识别出ss的流量特征。\n流量特征识别（Traffic Pattern Recognition）是网络安全和网络管理领域的一个重要研究课题。在这一领域中，研究者试图通过分析网络流量的各种属性和行为，来识别或预测这些流量的源、目的、协议等信息。这也被广泛应用在流量分类、应用识别、恶意流量检测等多个方面。\n流量特征识别主要关注以下几个方面的信息：\n数据包的大小和时间间隔：这可以帮助分析者了解网络连接的一些基本特性，例如其传输速率、周期性行为等。\n传输层协议和端口：例如TCP、UDP和它们使用的端口号，这些信息可以帮助分析者推测数据流可能的应用类型。\n数据包头部中的特定字段值：例如TCP的标志位、IP的TTL值、窗口大小等，这些信息可以反映网络连接的状态和行为。\n链路层和网络层的信息：例如MAC地址、IP地址等，这些信息可以暴露网络连接的实际位置和路由路径。\n数据流的统计信息：例如数据包的平均大小、方差、峰度、偏度等统计量，这些信息可以帮助分析者理解网络流量的整体特性。\n这些信息通常会被收集并处理成特征向量，然后输入到一些机器学习模型（例如SVM、决策树、深度学习等）进行训练和预测。这样可以自动化地识别出各种特定的流量类型或行为模式。\nDeep Packet Inspection (DPI) for *** Encrypted Traffic with Deep Learning Models\nAutomated Identification of *** Traffic with machine learning techniques\nhttps 因为http是明文的数据 所以还是存在被防火墙探测的风险 想要将http流量进行加密的话又需要引入tls 让它成为https流量\nHTTPS使用的是一种叫做“混合加密”系统的方法，它结合了对称加密和非对称加密的优势。具体来说：\n非对称加密: 在握手阶段，服务器会给客户端提供它的公钥，并且可能会请求客户端的公钥。客户端选择一个会话密钥，然后使用服务器的公钥对其进行加密，并发送给服务器。服务器使用它的私钥解密会话密钥。\n这种加密方式主要用于在网络上安全地发送会话密钥，因为只有服务器才能解密通过其公钥加密的密钥。然而，非对称加密在计算上相当昂贵，因此不适合用于所有数据传输。\n对称加密: 一旦服务器和客户端都有了会话密钥，他们就可以使用对称加密来加密和解密数据。对称加密要比非对称加密快得多，而且对资源的要求也更低，这使其成为加密大量数据的理想选择。\n设想你是一个发信人，你要给朋友（接收人）寄一个需要保密的信件。因此，你在把信寄出去之前，选择把它放在一个有锁的箱子中，然后用一把钥匙把箱子锁起来。这个箱子就相当于HTTPS的加密数据，而那把钥匙就相当于是你的公钥。\n连接请求：你（客户端/浏览器）首先向你的朋友（服务器）发送一个连接请求，就像你告诉朋友你打算寄给他一封信。\n服务器响应与公钥分享：你的朋友（服务器）向你提供了一个公钥，就像他给你一把打开他的箱子的钥匙。\n创建会话密钥：然后，你使用这把钥匙（公钥）来创建一个独特的会话密钥，这相当于你把锁上的箱子放在另一个箱子里，然后使用你自己的锁将其锁起来。然后，你将这个箱子（包含公钥加密后的会话密钥）寄给你的朋友。\n解密会话密钥：你的朋友收到你的信件（加密的箱子），使用他的另一把钥匙（私钥）打开外面的箱子，得到里面的会话密钥。\n数据交换：你和你的朋友都有了相同的会话密钥，你们可以用这个密钥来加密和解密信息。你把你的信（数据）放在箱子里，用这个会话密钥锁起来，然后寄给你的朋友。你的朋友收到后，用同样的会话密钥打开箱子，读取你的信。反之亦然。\n连接结束：当你们的对话结束，你们会丢掉这个会话密钥，如果下次还要交谈，就重新生成一个新的会话密钥。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 首先 当一个网站需要开启https访问的时候 他就必须要先有一个网站的证书 网站的证书的话又需要交到CA机构那里去申请 但是在申请证书之前我们又需要有一对公私钥 这对公私钥 就是我们在进行tls加密的时候必须要用到的 所以说我们又先要来了解一下这对公私钥 这对公私钥的作用主要就是用来进行非对称加密的 假设我现在对 网站证书 这几个字进行加密 我加密的时候 如果使用的是私钥对它进行加密的话 加面之后的内容完全看不见 我要解密的话 必须使用公钥去给它解密 它才能解密成原来的数据内容 你如果还是使用私钥去给它解密的话 它是解不出来的 这就是非对称加密算法的一种特性 反过来 如果我用公钥对这一串数据进行加密的话 我要解的话 也必须用私钥对他进行解密才能解读出来 你如果用公钥去给他解密的话 你是解不出来的 这就是非对称加密算法 有了这个基本概念之后 我们就可以来探索一下我们申请证书的一个流程 首先申请证书的话 我们需要到这个CA机构这里去申请 首先的话 我们的这个服务器生成了这么一个请求 我要申请证书 我要给这个baidu.com的域名申请证书 然后我的公钥是123 这就是申请证书里面必须要包含的内容 你发到这个是A机构这里去 所以这个拿到你这个申请表一看 你申请一个证书 他说好 但是你要证明这个域名是你的 我才能给你下发证书 证明的方式有很多种 假设 他说 在你的网站目录下放个文件 等一下 我过来看一下 如果你确实能把这个文件放在你这个网站的话 那说明你确实能控制这个域名 也就是说 这个域名确实是你的 然后我才能给你颁发证书 给了你一个证书 证书拿好 里面包含了是哪个机构给你颁发的 这个是A机构 颁发给这个域名的 然后他有一个有效期 同时也有你的公钥 这就是你申请的这个网站证书里面的一些基本信息 有了这个证书之后 我们才能建立https的连接 ok 现在我们这个网站证书也有了 网站也已经搭建了 现在又要开始进行https的访问了 在http阶段 我们生成了这么一个数据包 然后由于我们开起来https然后他就会把这个数据往tls里面扔 刚扔进来之后 他不会立刻对数据进行加密 因为我们现在根本就不知道加密的密钥 所以说tls我们要先进行握手连接 也就是要先跟这个网站建立一个tls的连接 首先的话 tls是跟这个网站说我要用https访问你 然后我这边的话也开起来https的访问 这台服务器的话就会同意这个连接请求 给他回一个 那你连接连呗 同时的话会把他的这个网站证书发送给你 这是我的证书 你看一下行不行 这个时候 你拿到这个证书 你就会对他进行一些验证 比如说 他是颁发给谷歌的 然后你比对一下我确实是访问谷歌 这里对上了 然后他的颁发机构是a的话 我的电脑里面有一个授信任的根证书颁发机构 里面有a 也就是说我信任 这个机构颁发的证书 同时它的有效期是多少 现在的话还没过期 也就是说条件都满足了 这个证书就是有效的 我就可以和他进行连接 如果说有其中一个条件不满足的话 浏览器就会报警 提示我们连接不安全 所以连接不安全的情况下有非常多的可能 我们这边收到服务器给我们的证书之后 发现证书是有效的 于是乎 我们就可以用它的这个公钥对我们的数据内容进行加密 这里要申明一下 我们的tls并不是直接使用公钥来对这一串数据内容进行加密的 他们之间的握手最终会协商出一个对称加密算法的密钥 来对我们的数据进行加密 因为对称加密算法的效率很高 如果所有的数据都用非对称加密上法进行加密的话 那效率是非常低的 他的具体的握手协商流程的话 如果你感兴趣的话 可以自行去搜索 我们这里的话仅仅是简单的讲解一下它的执行流程 为了能够让大家简单的理解 我们这里就不那么严谨了 这个经过加密后的数据的话就会从我们的电脑发送到网络上面去 防火墙这边拿的数据一看这边是经过了加密的 所以它是看不到里面的内容 也修改不了里面的数据 如果他想强行的对这一串数据进行中间人攻击 进行拦截或者篡改的话 网站的证书的话就会不匹配 所以我们的浏览器就会报警 如果你在浏览器看到了警告的信息的话 建议检查一下是否存在中间人攻击的这么一种情况 有的朋友可能会有这样的疑问 为什么不把我们的网址也进行加密 不加密的原因是可能这个服务器它不只运行了一个网站 除了谷歌之外 他可能youtube也在这个服务器上面 但是他只有一个ip 我们如果把这一串也进行加密了的话 他把这个数据发到这个服务器上面去 那服务器就不知道用哪个证书的私钥去进行解密 因为每个网站它都有自己的证书 而每个证书所用的公钥和私钥它是不一样的 所以说这一串的话必须是明文的 他有一个名称叫做sni 这个后面的话我们也会经常用到 当然 我这里说的是tls1.2的情况 1.3的话 他可以对这个sni进行加密 也就是esni 在传输的过程中 这个网址他也可以给你进行加密 也就他只露出了一个ip地址 但是这个的话目前还没有普及 而且据说防火墙会直接丢弃esni加密后的这个数据包流量 因为他看不到网址 他不知道你访问的是哪个网站 ","date":"2023-07-15T12:44:47+08:00","permalink":"https://blog.importzhh.me/p/%E4%B8%80%E4%BA%9B%E5%85%B6%E4%BB%96%E6%A6%82%E5%BF%B5%E4%B8%8D%E4%B8%A5%E8%B0%A8%E7%9A%84%E8%A1%A5%E5%85%85/","title":"一些其他概念不严谨的补充"},{"content":"分支保护 GitHub 的分支保护选项允许存储库所有者对规定的分支设置一些特定的访问和修改规则，以增加代码健壮性和项目管理的效率。\n照你描述的需求，你可以本地创建新的开发分支，完成开发后推送到 GitHub。然后你可以发起一个 pull request，请求合并你的开发分支到主分支。开发团队其他成员（或你自己）可以审查这个 pull request，在确认没有问题后将其合并到主分支。\n以下是对分支保护选项的设置建议:\nProtect matching branches: 开启。你可能想要保护你的主分支，防止直接推送代码。\nRequire a pull request before merging: 开启。此设置强制你必须通过 pull request 才能将更改合并到受保护的分支。\nRequire approvals: 根据你的团队规模和审查需求来决定。如果你希望有人审核代码，那就开启这个选项。\nDismiss stale pull request approvals when new commits are pushed: 开启。这确保每次新提交都得到审查。\nRequire review from Code Owners: 可以按需开启。如果你的团队有指定代码所有者，那么他们的审查就变得非常重要。\nRequire approval of the most recent reviewable push: 可以按需开启。这确保最新的提交已被审查。\nRequire status checks to pass before merging: 如果你有 CI/CD 流程，那么你应该开启。这确保所有测试都通过后才能合并。\nRequire branches to be up to date before merging: 应开启，以确保在合并之前所有的代码都是最新的。\nRequire conversation resolution before merging: 可以按需开启。这确保所有的代码讨论都已经解决。\nRequire signed commits: 可以按需开启。如果你需要确保提交的代码来源的身份验证和完整性，可以开启这个选项。\nRequire linear history: 可以按需开启。如果你希望保持你的提交历史直线型，可以选择此项。\nRequire deployments to succeed before merging: 如果你的项目有自动部署流程，那你可以开启这个选项。\nLock branch: 除非你有特别的需求，否则一般不需要开启。\nDo not allow bypassing the above settings: 为确保所有设置都被执行，开启该选项。\nAllow force pushes/Allow deletions: 这两个选项的开启取决于你是否希望允许有权限访问的用户执行强制推送和删除分支。这些操作会改变历史记录，一般较少使用，如非必要，可以不选择开启。\n值得注意的是，由于分支保护规则可能会阻止某些类型的拉取请求，所以对于希望构建和测试拉取请求的人来说，这可能会造成问题。你可以设置允许特定的人或团队推送到受保护的分支，或者允许那些提出拉取请求的人合并他们的改动。\n配置多个账户 要在同一台机器上配置多个 GitHub 账户，你需要为每个账户创建一个新的 SSH 密钥，并在 ~/.ssh/config 文件中为每个密钥创建一个新的 SSH 别名。以下是详细步骤：\n生成新的 SSH 密钥：对于每个 GitHub 账户，你需要创建一个新的 SSH 密钥。你可以使用 ssh-keygen 命令来生成密钥：\n1 ssh-keygen -t ed25519 -C \u0026#34;your-email@example.com\u0026#34; 当提示你输入文件以保存密钥时，输入一个新的文件名，例如 ~/.ssh/id_ed25519_github_username。这将在 ~/.ssh 目录下创建两个文件，一个是私钥（id_ed25519_github_username），另一个是公钥（id_ed25519_github_username.pub）。\n将新的公钥添加到 GitHub 账户：登录到你的 GitHub 账户，然后打开 \u0026ldquo;Settings\u0026rdquo; -\u0026gt; \u0026ldquo;SSH and GPG keys\u0026rdquo; -\u0026gt; \u0026ldquo;New SSH key\u0026rdquo;。然后，打开你刚刚创建的公钥文件，将内容复制粘贴到 \u0026ldquo;Key\u0026rdquo; 字段，给密钥起个名字，点击 \u0026ldquo;Add SSH key\u0026rdquo;。\n在 ~/.ssh/config 文件中创建新的 SSH 别名：在 ~/.ssh/config 文件中（如果文件不存在，你需要创建一个），为每个 SSH 密钥创建一个新的 Host。你需要为每个 Host 指定 HostName（总是 github.com），User（总是 git），以及你的私钥文件的位置。例如：\n1 2 3 4 5 6 7 8 9 10 11 # Account 1 Host github.com-username1 HostName github.com User git IdentityFile ~/.ssh/id_ed25519_github_username1 # Account 2 Host github.com-username2 HostName github.com User git IdentityFile ~/.ssh/id_ed25519_github_username2 在 Git 仓库中切换账户：在你的 Git 仓库中，你可以通过更改仓库的 remote URL 来切换账户。例如，如果你想要将仓库的 owner 切换为 username2，你可以这样做：\n1 git remote set-url origin git@github.com-username2:username2/repo.git 这样，当你 push 到 origin 时，Git 会使用 github.com-username2 的 SSH 密钥，这个密钥是与你的 username2 GitHub 账户关联的。\n请注意，这种方法只能用于切换整个仓库的 owner，而不能在单个 commit 中切换账户。如果你需要在不同的 commits 中使用不同的账户，你可能需要考虑在每次 commit 之前更改你的 Git 配置（使用 git config user.name 和 git config user.email），或者使用不同的工作目录或 Git 工作树。\ngithub pages 原文链接： https://siriusq.top/github-pages-启用-cloudflare-加速及-https.html 登录 Cloudflare， 然后点击顶栏的Add site\n输入自己的域名，然后点击Add site按钮，我这胡乱加了一个没人用的做示范\n选择方案，一般选Free就够用了 然后Cloudflare会自动获取域名的DNS记录，这里跳过，拉到页面最底部，点击Continue按钮，待会回来再配置 复制Cloudflare给出的域名服务器，回到域名注册商那里替换掉原有的 回到Cloudflare，点击Done, check nameservers 然后慢慢等待生效，生效后会有邮件提醒，等待生效的时候可以先设置SSL/TLS\n在Overview里，将Encryption Mode先设置为Full 然后到Edge Certificates中，开启Always Use HTTPS、Opportunistic Encryption等 到Origin Server中，开启Authenticated Origin Pulls 生效后到DNS设置里添加四条A记录与一条CNAME记录 CNAME记录的NAME设置为www，Content设置为GitHub Pages默认的域名，用户名.github.io那个 A记录的Name都使用@或者自己的域名，Content依次设置为 确认一下各个记录的Proxy status都是Proxied状态，即橙色的云朵 GitHub Pages 配置 打开自己的网站的仓库设置Settings-Pages 在Custom domain中填入自己的域名，点击Save后会自动生成一个CNAME文件 开启Encryption Full (strict)模式 这里没有测试成功 Full (strict)模式与Full模式的区别在于，Full (strict)模式使用的是由可信 CA 或 Cloudflare Origin CA 签名的有效证书并对每个请求验证证书，而非Full模式使用的无需验证的自签名证书。GitHub Pages可以通过开启Enforce HTTPS来获取免费的可信证书，满足开启Full (strict)模式的条件。下面就是我今天新发现的问题，我的Pages设置中不能开启Enforce HTTPS，勾选框一直是灰色的，折腾半天发现是Cloudflare的代理状态的造成的，解决方案如下\n在Cloudflare的DNS设置中把Proxy status全部设置为DNS Only状态，即灰色的云朵 回到Pages设置，刷新一下就可以勾选Enforce HTTPS了 GitHub会自动申请SSL证书，有了这个证书才能够在Cloudflare开启Full (strict)模式 等待Pages的HTTPS生效后，回到Cloudflare，把刚才修改的Proxy status全部恢复为Proxied状态，即橙色的云朵 到SSL/TLS设置中，将Encryption Mode设置为Full (strict) DNS记录类型科普 部分内容引用自 维基百科\nA记录 A记录(Address record)是用来指定主机名（或域名）对应的IP地址记录。用户可以将该域名下的网站服务器指向到自己的网页服务器(web server)上。同时也可以设置域名的子域名。它会传回一个32位元的IPv4地址。\nAAAA记录 AAAA记录(AAAA record)是用来将域名解析到IPv6地址的DNS记录。用户可以将一个域名解析到IPv6地址上，也可以将子域名解析到IPv6地址上。它会传回一个128位元的IPv6地址。\nCNAME记录 CNAME记录(Canonical Name Record)，用于将一个域名（同名）映射到另一个域名（真实名称），域名解析服务器遇到CNAME记录会以映射到的目标重新开始查询。CNAME 记录可用于 CDN 加速，通过 CDN 加速别名解析网站域名。\nNS记录 NS(Name Server)记录是域名服务器记录，用来指定该域名由哪个DNS服务器来进行解析。如果需要把子域名交给其他DNS服务商解析，就需要添加NS记录。\nMX记录 MX(mail exchanger)记录用于指定负责处理发往收件人域名的邮件服务器。\nTXT记录 TXT记录一般指为某个主机名或域名设置的说明。\nTTL TTL(Time To Live)表示解析记录在DNS服务器中的缓存时间，长度为秒。当本地DNS服务器收到某一域名的解析请求时，需要向该域名指定的权威DNS服务器发送解析请求获取解析记录。获得的解析记录会在本地DNS服务器中保存一段时间。在这段时间内，如果本地DNS服务器再次收到该域名的解析请求，将不再向权威DNS服务器发送解析请求，而是直接返回保存在本地DNS服务器中的解析记录。\n对比原来博文差距还是很大 有待进一步美化 没有自定义颜色 自定义背景 每一项分割线 字体都有待优化\n","date":"2023-07-14T01:09:09+08:00","permalink":"https://blog.importzhh.me/p/github%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","title":"github基本操作"},{"content":"Welcome to Hugo theme Stack. This is your first post. Edit or delete it, then start writing!\nFor more information about this theme, check the documentation: https://docs.stack.jimmycai.com/\nWant a site like this? Check out hugo-theme-stack-stater\nPhoto by Pawel Czerwinski on Unsplash\n","date":"2022-03-06T00:00:00Z","image":"https://blog.importzhh.me/p/hello-world/cover_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.importzhh.me/p/hello-world/","title":"Hello World"}]